{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/zss/estimation_new/baseline_new/')\n",
    "import Utils\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import easygraph as eg\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    seed = int(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRUModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int,\n",
    "            hidden_size2: int,\n",
    "            out_features: int\n",
    "    ):\n",
    "        super(BiGRUModel, self).__init__()\n",
    "        # 定义双向GRU层\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(2*hidden_size, 2*hidden_size, batch_first=True, bidirectional=True)\n",
    "        # self.gru3 = nn.GRU(4*hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        # 由于是双向GRU，隐藏层维度变为 hidden_size * 2\n",
    "        self.l1 = nn.Linear(hidden_size * 2, hidden_size2)\n",
    "        self.l2 = nn.Linear(hidden_size2, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  \n",
    "        # Bi-GRU的输出\n",
    "        gru_out, _ = self.gru(x)\n",
    "        # gru_out2, _ = self.gru2(gru_out)\n",
    "        # gru_out3, _ = self.gru3(gru_out2)\n",
    "        gru_out_last_step = gru_out[:, -1, :]\n",
    "        gru_out_last_step = self.dropout(gru_out_last_step)\n",
    "\n",
    "        # 通过全连接层\n",
    "        l1_out = self.l1(gru_out_last_step)\n",
    "        output = F.relu(self.l2(l1_out))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(hg, final_features, label):\n",
    "\n",
    "# There is no default feature vector for this dataset. Users can generate their own features.\n",
    "# Here we use random initialisation to generate 100-dimensional node feature vectors\n",
    "\n",
    "  input_feature_dim = final_features.shape[1]\n",
    "  hidden_dim = 128\n",
    "  hidden_dim2 = 32\n",
    "  out_dim = 1\n",
    "\n",
    "  # 初始划分：90%的数据用于训练和验证，10%的数据用于测试\n",
    "  train_val_nodes, test_nodes = train_test_split(hg.v, test_size=0.1, random_state=42)\n",
    "# 在训练和验证数据集里再划分：80%的数据用于训练，20%的数据用于验证\n",
    "  train_nodes, val_nodes = train_test_split(train_val_nodes, test_size=0.2, random_state=42)\n",
    "  train_mask = train_nodes\n",
    "  val_mask = val_nodes\n",
    "  test_mask = test_nodes\n",
    "\n",
    "  # X = np.array([node_features[node] for node in range(len(node_labels))])\n",
    "  # X = torch.from_numpy(X).float()\n",
    "  X = final_features.float()\n",
    "\n",
    "  # y = np.array([node_labels[node] for node in range(len(node_labels))])\n",
    "  # y = torch.from_numpy(y)\n",
    "  y = label.float()\n",
    "\n",
    "  dataset = {}\n",
    "  # dataset[\"structure\"] = eg.Hypergraph(num_v=label.shape[0], e_list=data)\n",
    "  dataset[\"features\"] = X\n",
    "  dataset[\"labels\"] = y\n",
    "  dataset[\"train_mask\"] = train_mask\n",
    "  dataset[\"val_mask\"] = val_mask\n",
    "  dataset[\"test_mask\"] = test_mask\n",
    "  # dataset[\"num_classes\"] = num_classes\n",
    "\n",
    "  model = BiGRUModel(input_size = input_feature_dim, hidden_size = hidden_dim, hidden_size2 = hidden_dim2, out_features = out_dim)\n",
    "\n",
    "  return dataset, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    data: dict,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,):\n",
    "\n",
    "    features = data[\"features\"]\n",
    "    train_mask, labels = data[\"train_mask\"], data[\"labels\"]\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features)\n",
    "    loss = criterion(torch.log(outputs[train_mask].squeeze()+1), torch.log(labels[train_mask]+1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "torch.no_grad()\n",
    "def valid(model, data):\n",
    "    features, labels = data[\"features\"], data[\"labels\"]\n",
    "    val_mask = data[\"val_mask\"]\n",
    "    # weights = weights\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "        mse = nn.functional.mse_loss(torch.log(outputs[val_mask].squeeze()+1), torch.log(labels[val_mask]+1)).item()\n",
    "        # criterion = WeightedMSELoss(weights[data[\"val_mask\"]])\n",
    "        # mse = criterion(outputs[val_mask].squeeze(), labels[val_mask])\n",
    "    return mse\n",
    "\n",
    "\n",
    "torch.no_grad()\n",
    "def test(model, data):\n",
    "    features = data[\"features\"]\n",
    "    test_mask, labels = data[\"test_mask\"], data[\"labels\"]\n",
    "    # weights = weights\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "        # criterion = WeightedMSELoss(weights[data[\"test_mask\"]])\n",
    "        mse = nn.functional.mse_loss(torch.log(outputs[test_mask].squeeze()+1), torch.log(labels[test_mask]+1)).item()\n",
    "        # mse = criterion(outputs[test_mask].squeeze(), labels[test_mask])\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss_curve(loss1 ,save_path = \"loss_pic.png\"):\n",
    "    plt.clf()\n",
    "    epochs = range(1, len(loss1) + 1)\n",
    "    plt.plot(epochs, loss1, 'b', label='EG Training loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(matrix):\n",
    "    # 初始化两个字典\n",
    "    nd_he = {}\n",
    "    he_nd = {}\n",
    "\n",
    "    rows, cols = matrix.shape\n",
    "    for row_idx in range(rows):\n",
    "        for col_idx in range(cols):\n",
    "            if matrix[row_idx, col_idx] == 1:\n",
    "                # 更新 row_to_cols 字典\n",
    "                if row_idx not in nd_he:\n",
    "                    nd_he[row_idx] = []\n",
    "                nd_he[row_idx].append(col_idx)\n",
    "\n",
    "                # 更新 col_to_rows 字典\n",
    "                if col_idx not in he_nd:\n",
    "                    he_nd[col_idx] = []\n",
    "                he_nd[col_idx].append(row_idx)\n",
    "    return nd_he, he_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkname = [\n",
    "    'Algebra', \n",
    "    'Bars-Rev', \n",
    "    'Geometry', \n",
    "    'iAF1260b', \n",
    "    'iJO1366', \n",
    "    'Music-Rev', \n",
    "    'Restaurants-Rev', \n",
    "    'senate-committees', \n",
    "    'fb-tvshow', \n",
    "    'HighSchool_2013_hour', \n",
    "    'Highschool_2012_hour', \n",
    "    'ht09_contact_hour', \n",
    "    'soc-hamsterster', \n",
    "    'house-committees'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algebra\n",
      "Algebra\n",
      "Algebra kendall:0.918051\n",
      "loss: 0.038519859313964844\n",
      "Bars-Rev\n",
      "Bars-Rev\n",
      "Bars-Rev kendall:0.919785\n",
      "loss: 0.0068529462441802025\n",
      "Geometry\n",
      "Geometry\n",
      "Geometry kendall:0.898033\n",
      "loss: 0.08746229112148285\n",
      "iAF1260b\n",
      "iAF1260b\n",
      "iAF1260b kendall:0.854759\n",
      "loss: 0.001076948712579906\n",
      "iJO1366\n",
      "iJO1366\n",
      "iJO1366 kendall:nan\n",
      "loss: 0.7929626107215881\n",
      "Music-Rev\n",
      "Music-Rev\n",
      "Music-Rev kendall:0.940675\n",
      "loss: 0.007641966454684734\n",
      "Restaurants-Rev\n",
      "Restaurants-Rev\n",
      "Restaurants-Rev kendall:0.940771\n",
      "loss: 0.003949963953346014\n",
      "senate-committees\n",
      "senate-committees\n",
      "senate-committees kendall:0.935961\n",
      "loss: 0.019843418151140213\n",
      "fb-tvshow\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[1;32m     79\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 80\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     loss_lst\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     83\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(torch\u001b[38;5;241m.\u001b[39mlog(outputs[train_mask]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mlog(labels[train_mask]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 13\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/estimation/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/estimation/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/estimation/lib/python3.8/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/estimation/lib/python3.8/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/estimation/lib/python3.8/site-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/estimation/lib/python3.8/site-packages/torch/optim/adam.py:379\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    376\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "all_y={}\n",
    "all_x={}\n",
    "all_test_list = []\n",
    "all_ken=[]\n",
    "seed_torch(42)\n",
    "for net in range(len(networkname)):\n",
    "    seed_torch(42)\n",
    "    print(networkname[net])\n",
    "    matrix = np.load(f'/home/zss/estimation_new/datamatrix/{networkname[net]}_incmatrix.npy')\n",
    "    N, M = matrix.shape\n",
    "    nd_he, he_nd = generate_dict(matrix)\n",
    "    # with open('/home/zss/estimation_new/N_E_dict/fb-pages-sport_nd_he.pkl', 'rb') as f:\n",
    "    #     nd_he = pickle.load(f)\n",
    "\n",
    "    # with open('/home/zss/estimation_new/N_E_dict/fb-pages-sport_he_nd.pkl', 'rb') as f:\n",
    "    #     he_nd = pickle.load(f)\n",
    "\n",
    "    # 使用字典推导式来创建一个新的字典，过滤掉值只有一个元素的键值对\n",
    "    filtered_dict = {k: v for k, v in he_nd.items() if len(v) > 1}\n",
    "\n",
    "    data = [] \n",
    "    for key in filtered_dict.keys():\n",
    "        data.append(filtered_dict[key])\n",
    "\n",
    "    #  label\n",
    "    npy_array = np.load(f'/home/zss/estimation_new/spread_RP_threshold/{networkname[net]}_ICRP_threshold_iter2000_time50.npy')\n",
    "    label = torch.from_numpy(npy_array)\n",
    "    # npy_array_normalized = (npy_array - np.min(npy_array)) / (np.max(npy_array) - np.min(npy_array))\n",
    "    # label = torch.from_numpy(npy_array_normalized)\n",
    "\n",
    "    # hypergraph\n",
    "    hg = eg.Hypergraph(num_v = label.shape[0], e_list = data,  merge_op=\"sum\")\n",
    "\n",
    "    # feature，中心性特征\n",
    "    # center_feature= future_generate(matrix,networkname[net])\n",
    "    # final_features =  center_feature\n",
    "\n",
    "    # np.save(f'../center_feature/{networkname[net]}_center.npy', final_features)\n",
    "    #距离特征结合中心性特征\n",
    "    center_feature=np.load(f'/home/zss/estimation_new/center_feature/{networkname[net]}_center.npy')\n",
    "    center_feature=torch.from_numpy(center_feature).to(torch.float32)\n",
    "\n",
    "    #距离特征\n",
    "    hydis = np.zeros((matrix.shape[0], matrix.shape[0]))\n",
    "    order=1\n",
    "    for s in range(1,order+1):\n",
    "        temp=np.load(f'/home/zss/estimation_new/hyper_distance/{networkname[net]}_distance_{s}.npy')\n",
    "        temp=1/temp\n",
    "        hydis+=temp*((0.1)**(order-s))\n",
    "        \n",
    "    np.fill_diagonal(hydis,0)\n",
    "    hydis=torch.from_numpy(hydis).to(torch.float32)\n",
    "\n",
    "    final_features = torch.cat((hydis,center_feature), dim=1).to(torch.float32)\n",
    "\n",
    "    # hydis = np.zeros((label.shape[0], label.shape[0]))\n",
    "    # num = order-1\n",
    "    # for _o in range(order):\n",
    "    #     _o += 1\n",
    "    #     temp = np.load(f'/home/zss/estimation_new/hyper_distance/{name}_distance_{_o}.npy')\n",
    "    #     temp = 1/temp\n",
    "    #     hydis += temp*((0.1)**num)\n",
    "    #     num -= 1\n",
    "\n",
    "    dataset, model = preprocess(hg, final_features, label)\n",
    "    loss_lst = []\n",
    "    acc = []\n",
    "    test_ls = []\n",
    "    epoch = 6000\n",
    "    lr = 0.01\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    \n",
    "    best_val_loss = float('inf')  # 初始化最小验证集损失为正无穷\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        loss = train(data = dataset, model = model, optimizer=optimizer, criterion=loss_fn)\n",
    "        loss_lst.append(loss.detach().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_acc = valid(model = model, data = dataset)\n",
    "\n",
    "        # acc.append(val_acc)\n",
    "        # if val_acc < best_val_loss:\n",
    "        #     best_val_loss = val_acc\n",
    "        #     torch.save(model.state_dict(), f'/home/zss/estimation_new/baseline_new/Bi-GRU/BI-GRU_best_model_threshold/best_model_{networkname[net]}_1.pth')  # 保存当前模型参数 \n",
    "    torch.save(model.state_dict(), f'/home/zss/estimation_new/baseline_new/Bi-GRU/BI-GRU_best_model_threshold/best_model_{networkname[net]}_1.pth')\n",
    "\n",
    "    # draw_loss_curve(loss_lst)\n",
    "    # draw_loss_curve(acc)\n",
    "\n",
    "    # dataset, model = preprocess()\n",
    "    features, labels = dataset[\"features\"], dataset[\"labels\"]\n",
    "    test_mask = dataset[\"test_mask\"]\n",
    "    train_mask = dataset[\"train_mask\"]\n",
    "    # features = features.unsqueeze(1)\n",
    "\n",
    "    dataset, model_T = preprocess(hg, final_features, label)\n",
    "    model_T.load_state_dict(torch.load(f'/home/zss/estimation_new/baseline_new/Bi-GRU/BI-GRU_best_model_threshold/best_model_{networkname[net]}_1.pth'))\n",
    "    model_T.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_T(features)\n",
    "    \n",
    "    test_acc = test(model = model_T, data=dataset)\n",
    "    # print(\"test mse:\", round(test_acc,4))\n",
    "    all_test_list.append(round(test_acc,6))\n",
    "\n",
    "    # min_value = min(npy_array)\n",
    "    # max_value = max(npy_array) \n",
    "\n",
    "    # y = labels[test_mask].squeeze()* (max_value - min_value) + min_value\n",
    "    # x = outputs[test_mask].squeeze()* (max_value - min_value) + min_value\n",
    "\n",
    "    y = labels[test_mask].squeeze()\n",
    "    x = outputs[test_mask].squeeze()\n",
    "    all_y[net] = y\n",
    "    all_x[net] = x\n",
    "    # print(y)\n",
    "    ken=round(kendalltau(x, y)[0],6)\n",
    "    all_ken.append(ken)\n",
    "    # print(x)\n",
    "    print(networkname[net])\n",
    "    print(f'{networkname[net]} kendall:{round(ken,6)}')\n",
    "    print('loss:',test_acc)\n",
    "\n",
    "print(all_ken)\n",
    "print(all_test_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algebra\n",
      "Bars-Rev\n",
      "Geometry\n",
      "iAF1260b\n",
      "iJO1366\n",
      "Music-Rev\n",
      "Restaurants-Rev\n",
      "senate-committees\n",
      "fb-tvshow\n",
      "HighSchool_2013_hour\n",
      "Highschool_2012_hour\n",
      "ht09_contact_hour\n",
      "soc-hamsterster\n",
      "house-committees\n"
     ]
    }
   ],
   "source": [
    "##### 获取测试结果，直接调用保存的模型\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "\n",
    "for net in range(len(networkname)):\n",
    "    seed_torch(42)\n",
    "    print(networkname[net])\n",
    "    matrix = np.load(f'/home/zss/estimation_new/datamatrix/{networkname[net]}_incmatrix.npy')\n",
    "    N, M = matrix.shape\n",
    "    nd_he, he_nd = generate_dict(matrix)\n",
    "\n",
    "    # 使用字典推导式来创建一个新的字典，过滤掉值只有一个元素的键值对\n",
    "    filtered_dict = {k: v for k, v in he_nd.items() if len(v) > 1}\n",
    "\n",
    "    data = [] \n",
    "    for key in filtered_dict.keys():\n",
    "        data.append(filtered_dict[key])\n",
    "\n",
    "    #  label\n",
    "    npy_array = np.load(f'/home/zss/estimation_new/spread_RP_threshold/{networkname[net]}_ICRP_threshold_iter2000_time50.npy')\n",
    "    label = torch.from_numpy(npy_array)\n",
    "\n",
    "\n",
    "    # hypergraph\n",
    "    hg = eg.Hypergraph(num_v = label.shape[0], e_list = data,  merge_op=\"sum\")\n",
    "\n",
    "    #距离特征结合中心性特征\n",
    "    center_feature=np.load(f'/home/zss/estimation_new/center_feature/{networkname[net]}_center.npy')\n",
    "    center_feature=torch.from_numpy(center_feature).to(torch.float32)\n",
    "\n",
    "    #距离特征\n",
    "    hydis = np.zeros((matrix.shape[0], matrix.shape[0]))\n",
    "    order=1\n",
    "    for s in range(1,order+1):\n",
    "        temp=np.load(f'/home/zss/estimation_new/hyper_distance/{networkname[net]}_distance_{s}.npy')\n",
    "        temp=1/temp\n",
    "        hydis+=temp*((0.1)**(order-s))\n",
    "        \n",
    "    np.fill_diagonal(hydis,0)\n",
    "    hydis=torch.from_numpy(hydis).to(torch.float32)\n",
    "\n",
    "    final_features = torch.cat((hydis,center_feature), dim=1).to(torch.float32)\n",
    "\n",
    "\n",
    "    dataset, model_T = preprocess(hg, final_features, label)\n",
    "    features, labels = dataset[\"features\"], dataset[\"labels\"]\n",
    "    test_mask = dataset[\"test_mask\"]\n",
    "    train_mask = dataset[\"train_mask\"]\n",
    "\n",
    "    \n",
    "    model_T.load_state_dict(torch.load(f'/home/zss/estimation_new/baseline_new/Bi-GRU/BI-GRU_best_model_threshold/best_model_{networkname[net]}.pth'))\n",
    "    model_T.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_T(features)\n",
    "    \n",
    "\n",
    "    # y = labels[range(len(matrix))].squeeze()\n",
    "    x = outputs[range(len(matrix))].squeeze()\n",
    "\n",
    "    np.save(f'/home/zss/estimation_new/baseline_new/Bi-GRU/predict_result/{networkname[net]}_Bi-GRU_predict_threshold.npy', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estimation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
