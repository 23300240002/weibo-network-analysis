import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
from scipy.stats import kendalltau
import random
import os
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimSun', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 10

def seed_everything(seed=42):
    """å›ºå®šéšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"âœ… éšæœºç§å­å·²å›ºå®šä¸º {seed}")

class BiGRUModel(nn.Module):
    """å­¦é•¿çš„Bi-GRUæ¨¡å‹ï¼ˆé€‚é…ç‰ˆï¼‰"""
    def __init__(self, input_size, hidden_size=128, hidden_size2=32, out_features=1):
        super(BiGRUModel, self).__init__()
        # åŒå‘GRUå±‚
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(p=0.5)
        # ç”±äºæ˜¯åŒå‘GRUï¼Œéšè—å±‚ç»´åº¦å˜ä¸º hidden_size * 2
        self.l1 = nn.Linear(hidden_size * 2, hidden_size2)
        self.l2 = nn.Linear(hidden_size2, out_features)

    def forward(self, x):
        x = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
        # Bi-GRUçš„è¾“å‡º
        gru_out, _ = self.gru(x)
        gru_out_last_step = gru_out[:, -1, :]
        gru_out_last_step = self.dropout(gru_out_last_step)
        
        # é€šè¿‡å…¨è¿æ¥å±‚
        l1_out = self.l1(gru_out_last_step)
        output = torch.relu(self.l2(l1_out))
        return output

class GLSTMModel(nn.Module):
    """å­¦é•¿çš„GLSTMæ¨¡å‹ï¼ˆé€‚é…ç‰ˆï¼‰"""
    def __init__(self, input_size, hidden_size=128, hidden_size2=32, out_features=1):
        super(GLSTMModel, self).__init__()
        # LSTMå±‚
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.dropout = nn.Dropout(p=0.5)
        # å…¨è¿æ¥å±‚
        self.l1 = nn.Linear(hidden_size, hidden_size2)
        self.l2 = nn.Linear(hidden_size2, out_features)

    def forward(self, x):
        x = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
        # LSTMå±‚çš„è¾“å‡º
        lstm_out, _ = self.lstm(x)
        # å–LSTMè¾“å‡ºçš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        lstm_out_last_step = lstm_out[:, -1, :]
        lstm_out_last_step = self.dropout(lstm_out_last_step)
        
        # é€šè¿‡å…¨è¿æ¥å±‚
        l1_out = self.l1(lstm_out_last_step)
        output = torch.relu(self.l2(l1_out))
        return output

class MLPBaseline(nn.Module):
    """ç®€å•MLPä½œä¸ºåŸºçº¿"""
    def __init__(self, input_size, hidden_size=64, out_features=1):
        super(MLPBaseline, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size//2, out_features),
            nn.ReLU()
        )

    def forward(self, x):
        return self.model(x)

def load_and_preprocess_data(data_path='C:/Tengfei/data/results/test2.csv'):
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print(f"ğŸ“ åŠ è½½æ•°æ®: {data_path}")
    
    try:
        df = pd.read_csv(data_path)
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(df)} ä¸ªç”¨æˆ·")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return None, None, None, None
    
    # ğŸ¯ é€‰æ‹©8ä¸ªç›¸å…³æ€§è¾ƒå¤§çš„ç‰¹å¾ï¼ˆåŸºäºä¹‹å‰çš„åˆ†æï¼‰
    feature_columns = [
        'density',
        'clustering_coefficient', 
        'average_nearest_neighbor_degree',
        'spectral_radius',
        'modularity',
        'global_in_degree',
        'global_out_degree',
        'node_count'
    ]
    
    target_column = 'avg_popularity_of_all'
    
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    missing_features = [col for col in feature_columns if col not in df.columns]
    if missing_features:
        print(f"âŒ ç¼ºå°‘ç‰¹å¾åˆ—: {missing_features}")
        print(f"ğŸ“‹ å¯ç”¨åˆ—: {list(df.columns)}")
        return None, None, None, None
    
    if target_column not in df.columns:
        print(f"âŒ ç¼ºå°‘ç›®æ ‡åˆ—: {target_column}")
        return None, None, None, None
    
    # æå–ç‰¹å¾å’Œç›®æ ‡
    X = df[feature_columns].copy()
    y = df[target_column].copy()
    
    print(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ:")
    print(f"   ğŸ“Š ç‰¹å¾æ•°é‡: {len(feature_columns)}")
    print(f"   ğŸ“Š é€‰æ‹©çš„ç‰¹å¾: {feature_columns}")
    
    # æ•°æ®è´¨é‡åˆ†æ
    print(f"\nğŸ“Š æ•°æ®è´¨é‡åˆ†æ:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(y)}")
    zero_count = (y == 0).sum()
    print(f"   å½±å“åŠ›ä¸º0çš„ç”¨æˆ·: {zero_count} ({zero_count/len(y)*100:.1f}%)")
    print(f"   æœ‰å½±å“åŠ›çš„ç”¨æˆ·: {len(y)-zero_count} ({(len(y)-zero_count)/len(y)*100:.1f}%)")
    print(f"   å½±å“åŠ›ç»Ÿè®¡: å‡å€¼={y.mean():.2f}, æœ€å¤§å€¼={y.max():.2f}, æ ‡å‡†å·®={y.std():.2f}")
    
    # å¤„ç†ç¼ºå¤±å€¼
    if X.isnull().any().any():
        print(f"âš ï¸ å‘ç°ç¼ºå¤±å€¼ï¼Œç”¨å‡å€¼å¡«å……")
        X = X.fillna(X.mean())
    
    return X, y, feature_columns, target_column

def create_data_loaders(X, y, test_size=0.2, val_size=0.2, batch_size=32, use_normalization=True):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print(f"ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # åˆ’åˆ†æ•°æ®é›†
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size/(1-test_size), random_state=42
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬") 
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # ç‰¹å¾å½’ä¸€åŒ–
    if use_normalization:
        print(f"   ğŸ”§ åº”ç”¨ç‰¹å¾å½’ä¸€åŒ–...")
        feature_scaler = StandardScaler()
        X_train_scaled = feature_scaler.fit_transform(X_train)
        X_val_scaled = feature_scaler.transform(X_val)
        X_test_scaled = feature_scaler.transform(X_test)
    else:
        X_train_scaled = X_train.values
        X_val_scaled = X_val.values
        X_test_scaled = X_test.values
        feature_scaler = None
    
    # ç›®æ ‡å˜é‡å˜æ¢ï¼ˆå­¦é•¿ä½¿ç”¨logå˜æ¢ï¼‰
    print(f"   ğŸ”§ åº”ç”¨ç›®æ ‡å˜é‡logå˜æ¢...")
    y_train_log = np.log(y_train + 1)
    y_val_log = np.log(y_val + 1)
    y_test_log = np.log(y_test + 1)
    
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train_scaled)
    X_val_tensor = torch.FloatTensor(X_val_scaled)
    X_test_tensor = torch.FloatTensor(X_test_scaled)
    
    y_train_tensor = torch.FloatTensor(y_train_log.values).reshape(-1, 1)
    y_val_tensor = torch.FloatTensor(y_val_log.values).reshape(-1, 1)
    y_test_tensor = torch.FloatTensor(y_test_log.values).reshape(-1, 1)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    return {
        'train_loader': train_loader,
        'val_loader': val_loader, 
        'test_loader': test_loader,
        'feature_scaler': feature_scaler,
        'original_data': {
            'y_train': y_train,
            'y_val': y_val,
            'y_test': y_test
        }
    }

def train_model(model, train_loader, val_loader, num_epochs=200, lr=0.001, device='cpu'):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹: {model.__class__.__name__}")
    
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model_state = None
    patience = 20
    no_improve_count = 0
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_epoch_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_epoch_loss += loss.item()
        
        avg_train_loss = train_epoch_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_epoch_loss = 0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_epoch_loss += loss.item()
        
        avg_val_loss = val_epoch_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_state = model.state_dict().copy()
            no_improve_count = 0
        else:
            no_improve_count += 1
        
        if (epoch + 1) % 50 == 0:
            print(f"   Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")
        
        # æ—©åœ
        if no_improve_count >= patience:
            print(f"   â¹ æ—©åœäºç¬¬ {epoch+1} è½®")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_model_state)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    
    return model, train_losses, val_losses

def evaluate_model(model, data_loaders, device='cpu'):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    results = {}
    
    with torch.no_grad():
        for split_name, loader in [('train', data_loaders['train_loader']), 
                                  ('val', data_loaders['val_loader']),
                                  ('test', data_loaders['test_loader'])]:
            
            all_preds = []
            all_targets = []
            
            for batch_x, batch_y in loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                
                all_preds.extend(outputs.cpu().numpy())
                all_targets.extend(batch_y.cpu().numpy())
            
            # è½¬æ¢å›åŸå§‹å°ºåº¦
            preds_original = np.expm1(np.array(all_preds).flatten())
            targets_original = np.expm1(np.array(all_targets).flatten())
            
            # ç¡®ä¿éè´Ÿ
            preds_original = np.maximum(preds_original, 0)
            
            # è®¡ç®—æŒ‡æ ‡
            mse = mean_squared_error(targets_original, preds_original)
            r2 = r2_score(targets_original, preds_original)
            mae = mean_absolute_error(targets_original, preds_original)
            
            # Kendall tau
            tau, p_value = kendalltau(targets_original, preds_original)
            
            results[split_name] = {
                'mse': mse,
                'r2': r2, 
                'mae': mae,
                'kendall_tau': tau,
                'kendall_p': p_value,
                'predictions': preds_original,
                'targets': targets_original
            }
    
    return results

def plot_results(results, model_name, output_dir):
    """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # è®­ç»ƒé›†é¢„æµ‹æ•£ç‚¹å›¾
    train_preds = results['train']['predictions']
    train_targets = results['train']['targets']
    train_r2 = results['train']['r2']
    train_tau = results['train']['kendall_tau']
    
    axes[0, 0].scatter(train_targets, train_preds, alpha=0.6, s=20)
    axes[0, 0].plot([train_targets.min(), train_targets.max()], 
                    [train_targets.min(), train_targets.max()], 'r--', lw=2)
    axes[0, 0].set_xlabel('çœŸå®å½±å“åŠ›')
    axes[0, 0].set_ylabel('é¢„æµ‹å½±å“åŠ›')
    axes[0, 0].set_title(f'{model_name} - è®­ç»ƒé›†\nRÂ² = {train_r2:.4f}, Kendall Ï„ = {train_tau:.4f}')
    axes[0, 0].grid(True, alpha=0.3)
    
    # æµ‹è¯•é›†é¢„æµ‹æ•£ç‚¹å›¾
    test_preds = results['test']['predictions']
    test_targets = results['test']['targets']
    test_r2 = results['test']['r2']
    test_tau = results['test']['kendall_tau']
    
    axes[0, 1].scatter(test_targets, test_preds, alpha=0.6, s=20, color='orange')
    axes[0, 1].plot([test_targets.min(), test_targets.max()], 
                    [test_targets.min(), test_targets.max()], 'r--', lw=2)
    axes[0, 1].set_xlabel('çœŸå®å½±å“åŠ›')
    axes[0, 1].set_ylabel('é¢„æµ‹å½±å“åŠ›')
    axes[0, 1].set_title(f'{model_name} - æµ‹è¯•é›†\nRÂ² = {test_r2:.4f}, Kendall Ï„ = {test_tau:.4f}')
    axes[0, 1].grid(True, alpha=0.3)
    
    # è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
    test_errors = test_preds - test_targets
    axes[1, 0].hist(test_errors, bins=50, alpha=0.7, color='green')
    axes[1, 0].set_xlabel('é¢„æµ‹è¯¯å·®')
    axes[1, 0].set_ylabel('é¢‘æ¬¡')
    axes[1, 0].set_title(f'{model_name} - æµ‹è¯•é›†è¯¯å·®åˆ†å¸ƒ')
    axes[1, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
    axes[1, 0].grid(True, alpha=0.3)
    
    # é¢„æµ‹å€¼åˆ†å¸ƒå¯¹æ¯”
    axes[1, 1].hist(test_targets, bins=30, alpha=0.5, label='çœŸå®å€¼', color='blue')
    axes[1, 1].hist(test_preds, bins=30, alpha=0.5, label='é¢„æµ‹å€¼', color='red')
    axes[1, 1].set_xlabel('å½±å“åŠ›å€¼')
    axes[1, 1].set_ylabel('é¢‘æ¬¡')
    axes[1, 1].set_title(f'{model_name} - æµ‹è¯•é›†åˆ†å¸ƒå¯¹æ¯”')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(output_dir, f'{model_name}_results.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ç»“æœå›¾è¡¨å·²ä¿å­˜: {plot_path}")
    plt.close()

def save_detailed_results(all_results, output_dir):
    """ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV"""
    summary_data = []
    
    for model_name, results in all_results.items():
        for split in ['train', 'test']:
            summary_data.append({
                'model': model_name,
                'split': split,
                'mse': results[split]['mse'],
                'r2': results[split]['r2'],
                'mae': results[split]['mae'],
                'kendall_tau': results[split]['kendall_tau'],
                'kendall_p': results[split]['kendall_p']
            })
    
    summary_df = pd.DataFrame(summary_data)
    summary_path = os.path.join(output_dir, 'model_comparison_summary.csv')
    summary_df.to_csv(summary_path, index=False)
    print(f"ğŸ“‹ æ¨¡å‹å¯¹æ¯”ç»“æœå·²ä¿å­˜: {summary_path}")
    
    return summary_df

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å­¦é•¿ç®—æ³•å¯¹æ¯”æµ‹è¯•å™¨")
    print("=" * 60)
    print("ğŸ“Š ç›®æ ‡ï¼šå¯¹æ¯”Bi-GRUã€GLSTMå’ŒMLPåœ¨ä½ çš„æ•°æ®ä¸Šçš„è¡¨ç°")
    print("ğŸ¯ æ•°æ®ï¼š8ä¸ªç½‘ç»œæŒ‡æ ‡ â†’ å½±å“åŠ›é¢„æµ‹")
    print("ğŸ”„ å¤„ç†ï¼šç‰¹å¾å½’ä¸€åŒ– + ç›®æ ‡logå˜æ¢")
    print("=" * 60)
    
    seed_everything(42)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = 'C:/Tengfei/data/results/others_comparison'
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾å¤‡é€‰æ‹©
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    X, y, feature_columns, target_column = load_and_preprocess_data()
    if X is None:
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_loaders = create_data_loaders(X, y, use_normalization=True)
    
    # å®šä¹‰æ¨¡å‹
    input_size = len(feature_columns)
    models = {
        'Bi-GRU': BiGRUModel(input_size=input_size),
        'GLSTM': GLSTMModel(input_size=input_size), 
        'MLP-Baseline': MLPBaseline(input_size=input_size)
    }
    
    print(f"\nğŸ¤– å°†è®­ç»ƒå’Œæµ‹è¯• {len(models)} ä¸ªæ¨¡å‹:")
    for name in models.keys():
        print(f"   - {name}")
    
    # è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰æ¨¡å‹
    all_results = {}
    
    for model_name, model in models.items():
        print(f"\n{'='*20} {model_name} {'='*20}")
        
        # è®­ç»ƒæ¨¡å‹
        trained_model, train_losses, val_losses = train_model(
            model, data_loaders['train_loader'], data_loaders['val_loader'], 
            num_epochs=300, lr=0.001, device=device
        )
        
        # è¯„ä¼°æ¨¡å‹
        results = evaluate_model(trained_model, data_loaders, device=device)
        all_results[model_name] = results
        
        # æ˜¾ç¤ºç»“æœ
        print(f"ğŸ“Š {model_name} æ€§èƒ½:")
        print(f"   è®­ç»ƒé›† - RÂ²: {results['train']['r2']:.4f}, MAE: {results['train']['mae']:.2f}, Kendall Ï„: {results['train']['kendall_tau']:.4f}")
        print(f"   æµ‹è¯•é›† - RÂ²: {results['test']['r2']:.4f}, MAE: {results['test']['mae']:.2f}, Kendall Ï„: {results['test']['kendall_tau']:.4f}")
        
        # è¿‡æ‹Ÿåˆæ£€æŸ¥
        r2_gap = results['train']['r2'] - results['test']['r2']
        overfitting_level = "ä¸¥é‡" if r2_gap > 0.2 else "è½»å¾®" if r2_gap > 0.1 else "æ­£å¸¸"
        print(f"   è¿‡æ‹Ÿåˆæ£€æŸ¥: RÂ²å·®å¼‚ = {r2_gap:.4f} ({overfitting_level})")
        
        # ç”Ÿæˆå¯è§†åŒ–
        plot_results(results, model_name, output_dir)
    
    # ä¿å­˜è¯¦ç»†å¯¹æ¯”ç»“æœ
    summary_df = save_detailed_results(all_results, output_dir)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print(f"\n" + "="*60)
    print("ğŸ† æ¨¡å‹æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("="*60)
    
    # æŒ‰æµ‹è¯•é›†RÂ²æ’åº
    test_r2_ranking = [(name, results['test']['r2']) for name, results in all_results.items()]
    test_r2_ranking.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ“Š æµ‹è¯•é›†RÂ²æ’å:")
    for i, (name, r2) in enumerate(test_r2_ranking, 1):
        tau = all_results[name]['test']['kendall_tau']
        mae = all_results[name]['test']['mae']
        print(f"   {i}. {name}: RÂ² = {r2:.4f}, Kendall Ï„ = {tau:.4f}, MAE = {mae:.2f}")
    
    # æŒ‰Kendall tauæ’åº
    kendall_ranking = [(name, results['test']['kendall_tau']) for name, results in all_results.items()]
    kendall_ranking.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ“Š æµ‹è¯•é›†Kendall Ï„æ’å:")
    for i, (name, tau) in enumerate(kendall_ranking, 1):
        r2 = all_results[name]['test']['r2']
        mae = all_results[name]['test']['mae']
        print(f"   {i}. {name}: Kendall Ï„ = {tau:.4f}, RÂ² = {r2:.4f}, MAE = {mae:.2f}")
    
    # æœ€ä½³æ¨¡å‹åˆ†æ
    best_r2_model = test_r2_ranking[0][0]
    best_tau_model = kendall_ranking[0][0]
    
    print(f"\nğŸ¯ ç»“è®ºåˆ†æ:")
    print(f"   ğŸ“ˆ RÂ²æœ€ä½³æ¨¡å‹: {best_r2_model}")
    print(f"   ğŸ“ˆ Kendall Ï„æœ€ä½³æ¨¡å‹: {best_tau_model}")
    
    if best_r2_model == best_tau_model:
        print(f"   âœ… {best_r2_model} åœ¨ä¸¤é¡¹æŒ‡æ ‡ä¸Šéƒ½è¡¨ç°æœ€ä½³ï¼")
    else:
        print(f"   âš ï¸ ä¸åŒæŒ‡æ ‡æ˜¾ç¤ºä¸åŒçš„æœ€ä½³æ¨¡å‹ï¼Œéœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©")
    
    # ä¸æ•°æ®è´¨é‡çš„å…³ç³»åˆ†æ
    zero_ratio = (y == 0).sum() / len(y) * 100
    print(f"\nğŸ’¡ æ•°æ®è´¨é‡å½±å“åˆ†æ:")
    print(f"   ğŸ“Š å½±å“åŠ›ä¸º0çš„ç”¨æˆ·æ¯”ä¾‹: {zero_ratio:.1f}%")
    
    best_model_r2 = all_results[best_r2_model]['test']['r2']
    if best_model_r2 < 0.1:
        print(f"   ğŸš¨ æ‰€æœ‰æ¨¡å‹RÂ²éƒ½å¾ˆä½ï¼Œè¯å®äº†æ•°æ®è´¨é‡é—®é¢˜çš„å½±å“")
        print(f"   ğŸ’¡ å»ºè®®ï¼šè€ƒè™‘ä»ä¼ æ’­èƒ½åŠ›æ›´å¼ºçš„ç”¨æˆ·å¼€å§‹çˆ¬å–æ•°æ®")
    elif best_model_r2 > 0.3:
        print(f"   âœ… æ¨¡å‹æ€§èƒ½ç›¸å¯¹è¾ƒå¥½ï¼Œè¯´æ˜ç½‘ç»œæŒ‡æ ‡ç¡®å®æœ‰é¢„æµ‹ä»·å€¼")
    else:
        print(f"   âš ï¸ æ¨¡å‹æ€§èƒ½ä¸­ç­‰ï¼Œæ•°æ®è´¨é‡ä»æœ‰æ”¹å–„ç©ºé—´")
    
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print(f"   - æ¨¡å‹å¯¹æ¯”å›¾è¡¨: *_results.png")
    print(f"   - å¯¹æ¯”æ•°æ®è¡¨: model_comparison_summary.csv")
    
    print(f"\nğŸ¯ å­¦é•¿ç®—æ³•å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()