{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/zss/estimation_new/baseline_new/')\n",
    "import Utils\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import easygraph as eg\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "from easygraph.nn import HGNNConv\n",
    "from easygraph.classes import Graph\n",
    "from easygraph.nn import HyperGCNConv\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import kendalltau\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    seed = int(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "def generate_dict(matrix):\n",
    "    # 初始化两个字典\n",
    "    nd_he = {}\n",
    "    he_nd = {}\n",
    "\n",
    "    rows, cols = matrix.shape\n",
    "    for row_idx in range(rows):\n",
    "        for col_idx in range(cols):\n",
    "            if matrix[row_idx, col_idx] == 1:\n",
    "                # 更新 row_to_cols 字典\n",
    "                if row_idx not in nd_he:\n",
    "                    nd_he[row_idx] = []\n",
    "                nd_he[row_idx].append(col_idx)\n",
    "\n",
    "                # 更新 col_to_rows 字典\n",
    "                if col_idx not in he_nd:\n",
    "                    he_nd[col_idx] = []\n",
    "                he_nd[col_idx].append(row_idx)\n",
    "    return nd_he, he_nd\n",
    "\n",
    "def get_neigbors(g, node, depth):\n",
    "    output = {}\n",
    "    layers = dict(nx.bfs_successors(g, source=node, depth_limit=depth))\n",
    "    nodes = [node]\n",
    "    for i in range(1, depth + 1):\n",
    "        output[i] = []\n",
    "        for x in nodes:\n",
    "            output[i].extend(layers.get(x, []))\n",
    "        nodes = output[i]\n",
    "    return output\n",
    "\n",
    "\n",
    "def H_index(g, node_set=-1):\n",
    "    if node_set == -1:\n",
    "        nodes = list(g.nodes())\n",
    "        d = dict()\n",
    "        H = dict()  # H-index\n",
    "        for node in nodes:\n",
    "            d[node] = g.degree(node)\n",
    "        for node in nodes:\n",
    "            neighbors = list(g.neighbors(node))\n",
    "            neighbors_d = [d[x] for x in neighbors]\n",
    "            for y in range(len(neighbors_d)):\n",
    "                if y > len([x for x in neighbors_d if x >= y]):\n",
    "                    break\n",
    "            H[node] = y - 1\n",
    "\n",
    "    if node_set in list(g.nodes()):  # 计算节点node的H-index\n",
    "        neighbors = list(g.neighbors(node))\n",
    "        neighbors_d = [d[x] for x in neighbors]\n",
    "        for y in range(len(neighbors_d)):\n",
    "            if y > len([x for x in neighbors_d if x >= y]):\n",
    "                break\n",
    "        H = y - 1\n",
    "    return H\n",
    "\n",
    "\n",
    "def k_shell(G, data_memory,train_nodes=[]):\n",
    "    for x in data_memory: x[0] = int(x[0])\n",
    "    simu_I = data_memory.copy()\n",
    "    simu_I.sort(key=lambda x: x[1], reverse=True)\n",
    "    simu_sort = [x[0] for x in simu_I]\n",
    "\n",
    "\n",
    "    K = nx.core_number(G)\n",
    "    k = [[x, K[x]] for x in K]\n",
    "    k.sort(key=lambda x: x[1], reverse=True)\n",
    "    k_sort = [x[0] for x in k if x[0] in simu_sort]\n",
    "\n",
    "    for node in train_nodes: # test set Ken\n",
    "        simu_sort.remove(node)\n",
    "        k_sort.remove(node)\n",
    "\n",
    "    node_rank_simu = list(range(1, len(simu_sort) + 1))\n",
    "    node_rank_K = [k_sort.index(x) if x in k_sort else len(k_sort) for x in\n",
    "                   simu_sort]\n",
    "    ken_K = kendalltau(node_rank_simu, node_rank_K)\n",
    "    K = list(K.values())\n",
    "    # #计算肯德尔系数\n",
    "    value_ = sorted(K, reverse=True)\n",
    "    rank = [1]\n",
    "    for i in range(1, len(value_)):\n",
    "        if value_[i] < value_[i - 1]:\n",
    "            rank.append(i + 1)\n",
    "        elif value_[i] == value_[i - 1]:\n",
    "            rank.append(rank[-1])\n",
    "    # print(ken_K[0])\n",
    "    return ken_K[0], rank,k_sort,node_rank_K\n",
    "\n",
    "\n",
    "def future_generate(im,im_name):\n",
    "\n",
    "    D = np.load(f'/home/zss/estimation_new/hyper_distance/{im_name}_distance_1.npy')\n",
    "\n",
    "    A = np.dot(im, im.T)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    adj_matrix = (A >= 1).astype(int)\n",
    "    # print('complete adj_matrix')\n",
    "    # 将邻接矩阵转换为边对列表\n",
    "    edge_list = [(i, j) for i in range(len(adj_matrix)) for j in range(len(adj_matrix[i])) if adj_matrix[i][j] == 1]\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    # 保证所有节点都在图中，孤立节点也会被添加\n",
    "    for i in range(len(adj_matrix)):\n",
    "        if i not in G:\n",
    "            G.add_node(i)\n",
    "\n",
    "    input = torch.ones(im.shape[0], 3)\n",
    "\n",
    "    deg = Utils.degree_centrality(im)\n",
    "    print('finish:dcc',len(deg))\n",
    "    h_index = list(H_index(G).values())\n",
    "    print('finish:h-index',len(h_index))\n",
    "    k_shell = list(nx.core_number(G))\n",
    "    print('finish:k-shell',len(k_shell))\n",
    "\n",
    "    input[:, 0] = torch.tensor(deg)\n",
    "    input[:, 1] = torch.tensor(h_index)\n",
    "    input[:, 2] = torch.tensor(k_shell)\n",
    "\n",
    "    return input\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int,\n",
    "            hidden_size2: int,\n",
    "            out_features: int\n",
    "                 ):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # 定义LSTM层，输入特征为3，隐藏层单元数为128\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)#LSTM\n",
    "\n",
    "        # self.lstm2 = nn.LSTM(hidden_size, hidden_size)#LSTM\n",
    "        # self.lstm = nn.GRU(input_size, hidden_size, batch_first=True)#GRU\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        # 定义全连接层，隐藏层到32个神经元\n",
    "        self.l1 = nn.Linear(hidden_size, hidden_size2)\n",
    "        # 定义输出层，将32个神经元连接到输出1个值\n",
    "        self.l2 = nn.Linear(hidden_size2, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        # LSTM层的输出\n",
    "        lstm_out, _ = self.lstm(x)  # 忽略隐状态输出\n",
    "        # 取LSTM输出的最后一个时间步的输出，维度是(batch_size, 128)\n",
    "        lstm_out_last_step = lstm_out[:, -1, :]\n",
    "        lstm_out_last_step = self.dropout(lstm_out_last_step)\n",
    "\n",
    "        # lstm_out2, _ =self.lstm2(lstm_out_last_step.unsqueeze(1))\n",
    "        # lstm_out_last_step2 = lstm_out2[:, -1, :]\n",
    "        # lstm_out_last_step2 = self.dropout(lstm_out_last_step2)\n",
    "        # 通过第一个全连接层\n",
    "        l1_out = self.l1(lstm_out_last_step)\n",
    "        # 通过第二个全连接层得到最终输出\n",
    "        # l1_out = self.dropout(l1_out)\n",
    "        output = F.relu(self.l2(l1_out))\n",
    "        return output\n",
    "\n",
    "\n",
    "def preprocess(hg, final_features, label, data):\n",
    "\n",
    "# There is no default feature vector for this dataset. Users can generate their own features.\n",
    "# Here we use random initialisation to generate 100-dimensional node feature vectors\n",
    "\n",
    "  input_feature_dim = final_features.shape[1]\n",
    "  hidden_dim = 128\n",
    "  hidden_dim_2 = 32\n",
    "  out_dim = 1\n",
    "  \n",
    "  '''\n",
    "  Since there is no default split for this dataset, here we split the test set, validation set, and test set in a 50:25:25 fashion\n",
    "  '''\n",
    "  # 初始划分：90%的数据用于训练和验证，10%的数据用于测试\n",
    "  train_val_nodes, test_nodes = train_test_split(hg.v, test_size=0.1, random_state=42)\n",
    "# 在训练和验证数据集里再划分：80%的数据用于训练，20%的数据用于验证\n",
    "  train_nodes, val_nodes = train_test_split(train_val_nodes, test_size=0.2, random_state=42)\n",
    "  train_mask = train_nodes\n",
    "  val_mask = val_nodes\n",
    "  test_mask = test_nodes\n",
    "\n",
    "  X = final_features.float()\n",
    "\n",
    "  y = label.float()\n",
    "\n",
    "  dataset = {}\n",
    "  dataset[\"structure\"] = eg.Hypergraph(num_v=label.shape[0], e_list=data)\n",
    "  dataset[\"features\"] = X\n",
    "  dataset[\"labels\"] = y\n",
    "  dataset[\"train_mask\"] = train_mask\n",
    "  dataset[\"val_mask\"] = val_mask\n",
    "  dataset[\"test_mask\"] = test_mask\n",
    "  # dataset[\"num_classes\"] = num_classes\n",
    "\n",
    "  model = LSTMModel(input_size = input_feature_dim, hidden_size = hidden_dim, hidden_size2 = hidden_dim_2, out_features = out_dim)\n",
    "\n",
    "  return dataset, model\n",
    "\n",
    "def train(data: dict, model: nn.Module, optimizer: torch.optim.Optimizer, criterion: nn.Module):\n",
    "    features, structure = data[\"features\"], data[\"structure\"]\n",
    "    train_mask, labels = data[\"train_mask\"], data[\"labels\"]\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features)\n",
    "    loss = criterion(torch.log(outputs[train_mask]+1), torch.log(labels[train_mask]+1))\n",
    "    # loss = criterion(outputs[train_mask], labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "torch.no_grad()\n",
    "def valid(model, data, weights):\n",
    "    features, labels, structure = data[\"features\"], data[\"labels\"], data[\"structure\"]\n",
    "    val_mask = data[\"val_mask\"]\n",
    "    weights = weights\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "        log_outputs = torch.log(outputs[val_mask] + 1)\n",
    "        log_labels = torch.log(labels[val_mask] + 1)\n",
    "        mse = nn.functional.mse_loss(log_outputs.squeeze(), log_labels).item()\n",
    "    return mse\n",
    "\n",
    "torch.no_grad()\n",
    "def test(model, data, weights):\n",
    "    features, structure = data[\"features\"], data[\"structure\"]\n",
    "    test_mask, labels = data[\"test_mask\"], data[\"labels\"]\n",
    "    weights = weights\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "        # criterion = WeightedMSELoss(weights[data[\"test_mask\"]])\n",
    "        log_outputs = torch.log(outputs[test_mask] + 1)\n",
    "        log_labels = torch.log(labels[test_mask] + 1)\n",
    "        mse = nn.functional.mse_loss(log_outputs.squeeze(), log_labels).item()\n",
    "        # mse = criterion(outputs[test_mask].squeeze(), labels[test_mask])\n",
    "    return mse\n",
    "\n",
    "\n",
    "def draw_loss_curve(loss1 ,save_path = \"loss_pic.png\"):\n",
    "    plt.clf()\n",
    "    epochs = range(1, len(loss1) + 1)\n",
    "    plt.plot(epochs, loss1, 'b', label='EG Training loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "file_name=[\n",
    "    'Algebra', \n",
    "    'Bars-Rev', \n",
    "    'Geometry', \n",
    "    'iAF1260b', \n",
    "    'iJO1366', \n",
    "    'Music-Rev', \n",
    "    # 'NDC-classes', \n",
    "    'Restaurants-Rev', \n",
    "    'senate-committees', \n",
    "    'fb-tvshow', \n",
    "    'HighSchool_2013_hour', \n",
    "    'Highschool_2012_hour', \n",
    "    'ht09_contact_hour', \n",
    "    'soc-hamsterster', \n",
    "    'house-committees'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    kendalltau_value = [] \n",
    "    test_mse = []\n",
    "    seed_torch(42)\n",
    "    for name in file_name:\n",
    "        matrix = np.load(f'/home/zss/estimation_new/datamatrix/{name}_incmatrix.npy')\n",
    "        N, M = matrix.shape\n",
    "        nd_he, he_nd = generate_dict(matrix)\n",
    "\n",
    "        # 使用字典推导式来创建一个新的字典，过滤掉值只有一个元素的键值对\n",
    "        filtered_dict = {k: v for k, v in he_nd.items() if len(v) > 1}\n",
    "\n",
    "        data = [] \n",
    "        for key in filtered_dict.keys():\n",
    "            data.append(filtered_dict[key])\n",
    "\n",
    "        npy_array = np.load(f'/home/zss/estimation_new/spread_RP/{name}_ICRP_iter2000_time50.npy')\n",
    "        label = torch.from_numpy(npy_array)\n",
    "\n",
    "        hg = eg.Hypergraph(num_v = label.shape[0], e_list = data,  merge_op=\"sum\")\n",
    "\n",
    "        # final_features =  future_generate(matrix, name)\n",
    "        #距离特征结合中心性特征\n",
    "        center_feature=np.load(f'/home/zss/estimation_new/center_feature/{name}_center.npy')\n",
    "        center_feature=torch.from_numpy(center_feature).to(torch.float32)\n",
    "        \n",
    "\n",
    "        #距离特征\n",
    "        hydis = np.zeros((matrix.shape[0], matrix.shape[0]))\n",
    "        order = 1\n",
    "        for s in range(1,order+1):\n",
    "            temp=np.load(f'/home/zss/estimation_new/hyper_distance/{name}_distance_{s}.npy')\n",
    "            temp=1/temp\n",
    "            hydis+=temp*((0.1)**(order-s))\n",
    "            \n",
    "        np.fill_diagonal(hydis,0)\n",
    "        hydis=torch.from_numpy(hydis).to(torch.float32)\n",
    "\n",
    "        final_features = torch.cat((hydis,center_feature), dim=1).to(torch.float32)\n",
    "\n",
    "        \n",
    "        dataset, model = preprocess(hg, final_features, label, data)\n",
    "        loss_lst = []\n",
    "        acc = []\n",
    "        test_ls = []\n",
    "        epoch = 500\n",
    "        lr = 0.001\n",
    "        # 示例：为较大的真实值样本赋予更高的权重\n",
    "        weights = torch.sqrt(label)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "        best_val_loss = float('inf')  # 初始化最小验证集损失为正无穷\n",
    "        for i in range(epoch):\n",
    "            model.train()\n",
    "            loss = train(data = dataset, model = model, optimizer=optimizer, criterion=loss_fn)\n",
    "            loss_lst.append(loss.detach().numpy())\n",
    "            print(f\"epoch: {i}, loss : {loss}\")\n",
    "        \n",
    "            model.eval()\n",
    "            val_acc = valid(model = model, data = dataset, weights=weights)\n",
    "            acc.append(val_acc)\n",
    "            if val_acc < best_val_loss:\n",
    "                best_val_loss = val_acc\n",
    "                torch.save(model.state_dict(), f'/home/zss/estimation_new/baseline_new/GLSTM/GLSTM_best_model/best_model_{name}.pth')  # 保存当前模型参数\n",
    "            else:\n",
    "                print(f'epoch {epoch + 1}, 验证集损失: {val_acc}, 未达到最佳')    \n",
    "\n",
    "            \n",
    "\n",
    "        print(\"Training finish!\")\n",
    "        \n",
    "        # draw_loss_curve(loss_lst)\n",
    "        # draw_loss_curve(acc)\n",
    "        # draw_loss_curve(test_ls)\n",
    "\n",
    "        features, labels, structure = dataset[\"features\"], dataset[\"labels\"], dataset[\"structure\"]\n",
    "        test_mask = dataset[\"test_mask\"]\n",
    "        train_mask = dataset[\"train_mask\"]\n",
    "\n",
    "        \n",
    "        dataset, model_T = preprocess(hg, final_features, label, data)\n",
    "        model_T.load_state_dict(torch.load(f'/home/zss/estimation_new/baseline_new/GLSTM/GLSTM_best_model/best_model_{name}.pth'))\n",
    "        model_T.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model_T(features)\n",
    "\n",
    "        test_acc = test(model = model_T, data=dataset, weights=weights)\n",
    "        test_mse.append(round(test_acc,6))\n",
    "        \n",
    "        \n",
    "        y = labels[test_mask].squeeze()\n",
    "        x = outputs[test_mask]\n",
    "        # plt.xlabel('predict')\n",
    "        # plt.ylabel('true')\n",
    "        # plt.scatter(x.tolist(),y.tolist())\n",
    "        # plt.title(f'{name} kendall:{round(kendalltau(x, y)[0],6)}')\n",
    "        kendalltau_value.append(round(kendalltau(x, y)[0],6))\n",
    "        # plt.savefig(f\"/home/zss/estimation_new/centrality_version/figures_cp/{name}.jpg\",dpi=600)\n",
    "        # plt.clf()  # 或者 plt.close()\n",
    "\n",
    "    np_data_1 = np.array(kendalltau_value)\n",
    "\n",
    "    np_data_2 = np.array(test_mse)\n",
    "\n",
    "    np.save('/home/zss/estimation_new/baseline_new/GLSTM/kendall_glstm.npy', np_data_1)\n",
    "\n",
    "    np.save('/home/zss/estimation_new/baseline_new/GLSTM/MSE_glstm.npy', np_data_2)\n",
    "\n",
    "    print(kendalltau_value)\n",
    "    print(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 获取测试结果，直接调用保存的模型\n",
    "\n",
    "for name in file_name:\n",
    "    seed_torch(42)\n",
    "    matrix = np.load(f'/home/zss/estimation_new/datamatrix/{name}_incmatrix.npy')\n",
    "    N, M = matrix.shape\n",
    "    nd_he, he_nd = generate_dict(matrix)\n",
    "\n",
    "    # 使用字典推导式来创建一个新的字典，过滤掉值只有一个元素的键值对\n",
    "    filtered_dict = {k: v for k, v in he_nd.items() if len(v) > 1}\n",
    "\n",
    "    data = [] \n",
    "    for key in filtered_dict.keys():\n",
    "        data.append(filtered_dict[key])\n",
    "\n",
    "    npy_array = np.load(f'/home/zss/estimation_new/spread_RP_threshold/{name}_ICRP_threshold_iter2000_time50.npy')\n",
    "    label = torch.from_numpy(npy_array)\n",
    "\n",
    "    hg = eg.Hypergraph(num_v = label.shape[0], e_list = data,  merge_op=\"sum\")\n",
    "\n",
    "    center_feature=np.load(f'/home/zss/estimation_new/center_feature/{name}_center.npy')\n",
    "    center_feature=torch.from_numpy(center_feature).to(torch.float32)\n",
    "\n",
    "    #距离特征\n",
    "    hydis = np.zeros((matrix.shape[0], matrix.shape[0]))\n",
    "    order = 1\n",
    "    for s in range(1,order+1):\n",
    "        temp=np.load(f'/home/zss/estimation_new/hyper_distance/{name}_distance_{s}.npy')\n",
    "        temp=1/temp\n",
    "        hydis+=temp*((0.1)**(order-s))\n",
    "        \n",
    "    np.fill_diagonal(hydis,0)\n",
    "    hydis=torch.from_numpy(hydis).to(torch.float32)\n",
    "\n",
    "    final_features = torch.cat((hydis,center_feature), dim=1).to(torch.float32)\n",
    "\n",
    "    dataset, model_T = preprocess(hg, final_features, label, data)\n",
    "\n",
    "    features, labels, structure = dataset[\"features\"], dataset[\"labels\"], dataset[\"structure\"]\n",
    "    test_mask = dataset[\"test_mask\"]\n",
    "    train_mask = dataset[\"train_mask\"]\n",
    "\n",
    "    model_T.load_state_dict(torch.load(f'/home/zss/estimation_new/baseline_new/GLSTM/GLSTM_best_model_threshold/best_model_{name}.pth'))\n",
    "    model_T.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_T(features)\n",
    "\n",
    "    x = outputs[range(len(matrix))].squeeze()\n",
    "\n",
    "    np.save(f'/home/zss/estimation_new/baseline_new/GLSTM/predict_result/{name}_GLSTM_predict_threshold.npy', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estimation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
