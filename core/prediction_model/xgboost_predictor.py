import os
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import joblib
import json

def normalize_id(id_value):
    """è§„èŒƒåŒ–ç”¨æˆ·ID"""
    try:
        id_str = str(id_value).strip()
        if id_str == '-2147483648':
            return id_str
        return str(int(float(id_str)))
    except:
        return str(id_value).strip()

def detect_available_abnormal_methods():
    """ğŸ”¥ æ–°å¢ï¼šè‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„å¼‚å¸¸ç”¨æˆ·æ’é™¤æ–¹æ³•"""
    base_dir = 'results/pick_out_abnormal_users'
    
    if not os.path.exists(base_dir):
        print(f"âŒ å¼‚å¸¸ç”¨æˆ·ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return []
    
    available_methods = []
    for item in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, item)
        if os.path.isdir(folder_path):
            csv_file = os.path.join(folder_path, 'abnormal_users.csv')
            if os.path.exists(csv_file):
                available_methods.append(item)
    
    # æ’åºï¼šåŸå§‹ç½‘ç»œåœ¨å‰ï¼Œç„¶åæŒ‰æ•°å­—æ’åº
    available_methods.sort(key=lambda x: (0 if 'original' in x.lower() else 1, x))
    
    return available_methods

def parse_exclude_percentage(method_name):
    """ğŸ”¥ æ–°å¢ï¼šä»æ–¹æ³•åç§°ä¸­è§£ææ’é™¤æ¯”ä¾‹"""
    if 'original' in method_name.lower():
        return 0.0
    
    import re
    match = re.search(r'(\d+(?:\.\d+)?)pct', method_name)
    if match:
        return float(match.group(1))
    
    return -1  # æ— æ³•è§£æ

def load_abnormal_users(abnormal_method):
    """åŠ è½½æŒ‡å®šæ–¹æ³•çš„å¼‚å¸¸ç”¨æˆ·åˆ—è¡¨"""
    if abnormal_method is None:
        return set()
    
    abnormal_file = f'results/pick_out_abnormal_users/{abnormal_method}/abnormal_users.csv'
    
    if not os.path.exists(abnormal_file):
        print(f"âš ï¸ æœªæ‰¾åˆ°å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶: {abnormal_file}")
        return set()
    
    try:
        abnormal_df = pd.read_csv(abnormal_file)
        abnormal_users = set(abnormal_df['user_id'].apply(normalize_id))
        return abnormal_users
    except Exception as e:
        print(f"âŒ åŠ è½½å¼‚å¸¸ç”¨æˆ·å¤±è´¥: {e}")
        return set()

def prepare_features_and_target(data_path, abnormal_users, target_column='avg_popularity_of_all'):
    """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
    # åŠ è½½æ•°æ®
    df = pd.read_csv(data_path)
    df['user_id'] = df['user_id'].apply(normalize_id)
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} ä¸ªç”¨æˆ·")
    
    # æ’é™¤å¼‚å¸¸ç”¨æˆ·
    normal_df = df[~df['user_id'].isin(abnormal_users)].copy()
    print(f"ğŸ“Š æ’é™¤å¼‚å¸¸ç”¨æˆ·å: {len(normal_df)} ä¸ªç”¨æˆ·")
    
    # æ£€æŸ¥ç›®æ ‡åˆ—
    if target_column not in normal_df.columns:
        print(f"âŒ æœªæ‰¾åˆ°ç›®æ ‡åˆ—: {target_column}")
        return None, None, None
    
    # å®šä¹‰ç‰¹å¾åˆ—ï¼ˆ11ä¸ªç½‘ç»œæŒ‡æ ‡ï¼‰
    feature_columns = [
        'density', 'clustering_coefficient', 'average_nearest_neighbor_degree',
        'betweenness_centrality', 'spectral_radius', 'modularity',
        'global_out_degree', 'global_in_degree', 'global_total_degree',
        'node_count', 'edge_count'
    ]
    
    # æ£€æŸ¥ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
    available_features = [col for col in feature_columns if col in normal_df.columns]
    missing_features = [col for col in feature_columns if col not in normal_df.columns]
    
    if missing_features:
        print(f"âš ï¸ ç¼ºå°‘ç‰¹å¾åˆ—: {missing_features}")
    
    print(f"âœ… å¯ç”¨ç‰¹å¾: {len(available_features)} ä¸ª")
    
    # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
    X = normal_df[available_features].copy()
    y = normal_df[target_column].copy()
    user_ids = normal_df['user_id'].copy()
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    print(f"ğŸ“Š ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"ğŸ“Š ç›®æ ‡å˜é‡ç»Ÿè®¡: å‡å€¼={y.mean():.2f}, æœ€å¤§å€¼={y.max():.2f}, éé›¶æ•°={(y>0).sum()}")
    
    # å¤„ç†ç¼ºå¤±å€¼
    if X.isnull().any().any():
        print(f"âš ï¸ å‘ç°ç¼ºå¤±å€¼ï¼Œå°†ç”¨å‡å€¼å¡«å……")
        X = X.fillna(X.mean())
    
    return X, y, user_ids

def train_xgboost_model(X, y, test_size=0.3, random_state=42):
    """ğŸ”¥ å½»åº•ä¿®å¤ç‰ˆï¼šç‰¹å¾é€‰æ‹© + æ•°æ®æ¸…æ´— + å¼ºæ­£åˆ™åŒ–"""
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆå½»åº•ä¿®å¤ç‰ˆï¼‰...")
    
    # ğŸ”¥ æ­¥éª¤1ï¼šæ•°æ®æ¸…æ´— - ç§»é™¤æç«¯å¼‚å¸¸å€¼
    print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"   å‡å€¼: {y.mean():.2f}, æ ‡å‡†å·®: {y.std():.2f}")
    print(f"   æœ€å°å€¼: {y.min():.2f}, æœ€å¤§å€¼: {y.max():.2f}")
    
    # ä½¿ç”¨99.5åˆ†ä½æ•°ä½œä¸ºä¸Šç•Œï¼Œç§»é™¤æç«¯å¼‚å¸¸å€¼
    upper_bound = y.quantile(0.995)  # ç§»é™¤å‰0.5%çš„æå€¼
    lower_bound = 0  # å½±å“åŠ›ä¸èƒ½ä¸ºè´Ÿ
    
    # è¿‡æ»¤å¼‚å¸¸å€¼
    valid_mask = (y >= lower_bound) & (y <= upper_bound)
    X_clean = X[valid_mask].copy()
    y_clean = y[valid_mask].copy()
    
    removed_count = len(y) - len(y_clean)
    print(f"ğŸ“Š æ•°æ®æ¸…æ´—ç»“æœ:")
    print(f"   ç§»é™¤æç«¯å¼‚å¸¸å€¼: {removed_count} ä¸ª ({removed_count/len(y)*100:.1f}%)")
    print(f"   æ¸…æ´—åå‡å€¼: {y_clean.mean():.2f}, æ ‡å‡†å·®: {y_clean.std():.2f}")
    print(f"   æ¸…æ´—åèŒƒå›´: {y_clean.min():.2f} ~ {y_clean.max():.2f}")
    
    # ğŸ”¥ æ­¥éª¤2ï¼šç›®æ ‡å˜é‡å˜æ¢ - ä¿®å¤ç‰ˆï¼šç›´æ¥ä½¿ç”¨å¯¹æ•°å˜æ¢
    print(f"ğŸ“Š åŸå§‹ç›®æ ‡å˜é‡ç»Ÿè®¡: å‡å€¼={y_clean.mean():.2f}, æ ‡å‡†å·®={y_clean.std():.2f}")
    
    # ğŸ”¥ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨å¯¹æ•°å˜æ¢ï¼Œé¿å…Box-Cox APIé—®é¢˜
    y_transformed = np.log1p(y_clean)  # log(1+x)
    lambda_param = None  # æ ‡è®°ä¸ºå¯¹æ•°å˜æ¢
    print(f"ğŸ“Š ä½¿ç”¨å¯¹æ•°å˜æ¢")
    
    print(f"   å˜æ¢åå‡å€¼: {y_transformed.mean():.2f}, æ ‡å‡†å·®: {y_transformed.std():.2f}")
    
    # ğŸ”¥ æ­¥éª¤3ï¼šç‰¹å¾é€‰æ‹© - ç§»é™¤ä½ç›¸å…³æ€§å’Œé«˜å…±çº¿æ€§ç‰¹å¾
    from sklearn.feature_selection import SelectKBest, f_regression
    from sklearn.preprocessing import StandardScaler
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_clean)
    
    # è®¡ç®—ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
    correlations = []
    feature_names = X_clean.columns.tolist()
    
    for i, feature in enumerate(feature_names):
        corr = np.corrcoef(X_scaled[:, i], y_transformed)[0, 1]
        correlations.append(abs(corr))
    
    # é€‰æ‹©ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾
    n_features = min(8, len(feature_names))  # æœ€å¤šä¿ç•™8ä¸ªç‰¹å¾
    top_indices = np.argsort(correlations)[-n_features:]
    
    X_selected = X_scaled[:, top_indices]
    selected_features = [feature_names[i] for i in top_indices]
    
    print(f"ğŸ“Š ç‰¹å¾é€‰æ‹©ç»“æœ:")
    print(f"   ä¿ç•™ç‰¹å¾æ•°: {len(selected_features)}")
    print(f"   é€‰æ‹©çš„ç‰¹å¾: {selected_features}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y_transformed, test_size=test_size, random_state=random_state, stratify=None
    )
    
    print(f"ğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(X_train)} ä¸ªæ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(X_test)} ä¸ªæ ·æœ¬")
    
    # ğŸ”¥ æ­¥éª¤4ï¼šXGBoostæ¨¡å‹ - å¼ºæ­£åˆ™åŒ–é˜²è¿‡æ‹Ÿåˆ
    model = xgb.XGBRegressor(
        n_estimators=30,           # å¤§å¹…å‡å°‘æ ‘æ•°é‡
        max_depth=2,               # ä¸¥æ ¼é™åˆ¶æ ‘æ·±åº¦
        learning_rate=0.01,        # æä½å­¦ä¹ ç‡
        subsample=0.6,             # å¼ºçƒˆæ¬ é‡‡æ ·
        colsample_bytree=0.6,      # å¼ºçƒˆç‰¹å¾é‡‡æ ·
        reg_alpha=10.0,            # å¼ºL1æ­£åˆ™åŒ–
        reg_lambda=50.0,           # å¼ºL2æ­£åˆ™åŒ–
        min_child_weight=10,       # å¢åŠ æœ€å°å¶èŠ‚ç‚¹æƒé‡
        gamma=1.0,                 # å¢åŠ åˆ†è£‚æœ€å°å¢ç›Š
        early_stopping_rounds=5,   # ğŸ”¥ ä¿®å¤ï¼šç§»åˆ°è¿™é‡Œ
        random_state=random_state,
        n_jobs=-1,
        objective='reg:squarederror',
        eval_metric='rmse'
    )

    print(f"â³ è®­ç»ƒä¸­ï¼ˆä½¿ç”¨å¼ºæ­£åˆ™åŒ–å‚æ•°ï¼‰...")
    # ğŸ”¥ ä¿®å¤ï¼šç§»é™¤early_stopping_roundså‚æ•°
    model.fit(X_train, y_train, 
            eval_set=[(X_train, y_train), (X_test, y_test)],
            verbose=False)
    print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    # ğŸ”¥ æ­¥éª¤5ï¼šé¢„æµ‹å¹¶é€†å˜æ¢ - ä¿®å¤ç‰ˆ
    y_train_pred_transformed = model.predict(X_train)
    y_test_pred_transformed = model.predict(X_test)
    
    # ğŸ”¥ ä¿®å¤ï¼šåªä½¿ç”¨å¯¹æ•°é€†å˜æ¢
    y_train_pred = np.expm1(y_train_pred_transformed)
    y_test_pred = np.expm1(y_test_pred_transformed)
    
    # ç¡®ä¿é¢„æµ‹å€¼éè´Ÿä¸”åˆç†
    y_train_pred = np.clip(y_train_pred, 0, upper_bound)
    y_test_pred = np.clip(y_test_pred, 0, upper_bound)
    
    # ğŸ”¥ æ­¥éª¤6ï¼šåœ¨åŸå§‹ç©ºé—´è¯„ä¼° - ä¿®å¤ç‰ˆ
    y_train_original = np.expm1(y_train)
    y_test_original = np.expm1(y_test)
    
    # ç¡®ä¿åŸå§‹å€¼ä¹Ÿåœ¨åˆç†èŒƒå›´å†…
    y_train_original = np.clip(y_train_original, 0, upper_bound)
    y_test_original = np.clip(y_test_original, 0, upper_bound)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    train_mse = mean_squared_error(y_train_original, y_train_pred)
    test_mse = mean_squared_error(y_test_original, y_test_pred)
    train_r2 = r2_score(y_train_original, y_train_pred)
    test_r2 = r2_score(y_test_original, y_test_pred)
    train_mae = mean_absolute_error(y_train_original, y_train_pred)
    test_mae = mean_absolute_error(y_test_original, y_test_pred)
    
    # ğŸ”¥ æ­¥éª¤7ï¼šè¯Šæ–­åˆ†æ
    print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½è¯Šæ–­:")
    print(f"   è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
    print(f"   æµ‹è¯•é›† RÂ²: {test_r2:.4f}")
    print(f"   è®­ç»ƒé›† MAE: {train_mae:.2f}")
    print(f"   æµ‹è¯•é›† MAE: {test_mae:.2f}")
    print(f"   é¢„æµ‹å€¼èŒƒå›´: {y_test_pred.min():.2f} ~ {y_test_pred.max():.2f}")
    print(f"   è¿‡æ‹Ÿåˆæ£€æŸ¥: {abs(train_r2 - test_r2):.4f} ({'è½»å¾®' if abs(train_r2 - test_r2) < 0.1 else 'ä¸¥é‡'})")
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = model.feature_importances_
    importance_df = pd.DataFrame({
        'feature': selected_features,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print(f"ğŸ“ˆ ç‰¹å¾é‡è¦æ€§TOP5:")
    for idx, row in importance_df.head().iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")
    
    # ğŸ”¥ æ„å»ºç»“æœ - æ‰©å±•åŸå§‹Xå’Œyåˆ°æ¸…æ´—åçš„æ•°æ®
    results = {
        'model': model,
        'scaler': scaler,
        'selected_features': selected_features,  # ğŸ”¥ ç¡®ä¿è¿™æ˜¯ç‰¹å¾ååˆ—è¡¨
        'lambda_param': lambda_param,
        'upper_bound': upper_bound,
        'X_train': X_train,  # ğŸ”¥ æ³¨æ„ï¼šè¿™æ˜¯numpyæ•°ç»„ï¼Œä¸æ˜¯DataFrame
        'X_test': X_test,    # ğŸ”¥ æ³¨æ„ï¼šè¿™æ˜¯numpyæ•°ç»„ï¼Œä¸æ˜¯DataFrame
        'y_train': y_train_original,
        'y_test': y_test_original,
        'y_train_pred': y_train_pred,
        'y_test_pred': y_test_pred,
        'metrics': {
            'train_mse': train_mse, 'test_mse': test_mse,
            'train_r2': train_r2, 'test_r2': test_r2,
            'train_mae': train_mae, 'test_mae': test_mae
        }
    }

    return results

def analyze_feature_importance(model, feature_names):
    """ğŸ”¥ ä¿®å¤ç‰ˆï¼šåˆ†æç‰¹å¾é‡è¦æ€§ï¼Œç¡®ä¿é•¿åº¦åŒ¹é…"""
    try:
        importance = model.feature_importances_
        
        # ğŸ”¥ ç¡®ä¿ç‰¹å¾åå’Œé‡è¦æ€§æ•°ç»„é•¿åº¦ä¸€è‡´
        if len(feature_names) != len(importance):
            print(f"âš ï¸ ç‰¹å¾åæ•°é‡({len(feature_names)})ä¸é‡è¦æ€§æ•°é‡({len(importance)})ä¸åŒ¹é…")
            # æˆªæ–­åˆ°è¾ƒçŸ­çš„é•¿åº¦
            min_length = min(len(feature_names), len(importance))
            feature_names = feature_names[:min_length]
            importance = importance[:min_length]
        
        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        print(f"ğŸ“ˆ ç‰¹å¾é‡è¦æ€§æ’åº:")
        for idx, row in feature_importance_df.iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")
        
        return feature_importance_df
        
    except Exception as e:
        print(f"âŒ åˆ†æç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")
        # è¿”å›ç©ºçš„DataFrame
        return pd.DataFrame({'feature': [], 'importance': []})

def save_method_results(results, feature_importance_df, method_info, output_dir):
    """ğŸ”¥ ä¿®å¤ç‰ˆï¼šä¿å­˜å•ä¸ªæ–¹æ³•çš„ç»“æœï¼Œå¤„ç†æ•°æ®é•¿åº¦ä¸åŒ¹é…é—®é¢˜"""
    method_dir = os.path.join(output_dir, f"exclude_{method_info['exclude_pct']}pct")
    os.makedirs(method_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹å’Œscaler
    model_file = os.path.join(method_dir, 'xgboost_model.joblib')
    joblib.dump({
        'model': results['model'],
        'scaler': results['scaler'],
        'feature_names': results['selected_features'],  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾å
        'lambda_param': results.get('lambda_param'),
        'upper_bound': results.get('upper_bound')
    }, model_file)
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§
    importance_file = os.path.join(method_dir, 'feature_importance.csv')
    feature_importance_df.to_csv(importance_file, index=False)
    
    # ğŸ”¥ ä¿®å¤ï¼šåˆ›å»ºç»“æœæ‘˜è¦ï¼Œé¿å…DataFrameé•¿åº¦ä¸åŒ¹é…
    results_summary = {
        'method_info': method_info,
        'training_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_info': {
            'train_samples': len(results['y_train']),
            'test_samples': len(results['y_test']),
            'feature_count': len(results['selected_features']),
            'selected_features': results['selected_features']
        },
        'performance_metrics': results['metrics'],
        'data_processing': {
            'upper_bound': results.get('upper_bound', 'Unknown'),
            'transform_method': 'log1p',
            'feature_selection': 'correlation_based'
        },
        'top_features': feature_importance_df.head(5).to_dict('records')
    }
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON
    results_file = os.path.join(method_dir, 'model_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)
    
    # ğŸ”¥ ä¿®å¤ï¼šä¿å­˜é¢„æµ‹ç»“æœåˆ°CSVï¼Œç¡®ä¿é•¿åº¦ä¸€è‡´
    try:
        predictions_data = {
            'y_true': results['y_test'],
            'y_pred': results['y_test_pred']
        }
        predictions_df = pd.DataFrame(predictions_data)
        predictions_file = os.path.join(method_dir, 'predictions.csv')
        predictions_df.to_csv(predictions_file, index=False)
        
        print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: {len(predictions_df)} è¡Œ")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜é¢„æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    try:
        generate_visualization(results, method_info, method_dir)
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–æ—¶å‡ºé”™: {e}")
    
    print(f"âœ… æ–¹æ³•ç»“æœå·²ä¿å­˜åˆ°: {method_dir}")

def generate_visualization(results, method_info, output_dir):
    """ç”Ÿæˆé¢„æµ‹ç»“æœå¯è§†åŒ–"""
    y_train, y_test = results['y_train'], results['y_test']
    y_train_pred, y_test_pred = results['y_train_pred'], results['y_test_pred']
    
    # çœŸå®å€¼ vs é¢„æµ‹å€¼æ•£ç‚¹å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # è®­ç»ƒé›†
    ax1.scatter(y_train, y_train_pred, alpha=0.5, s=20)
    ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)
    ax1.set_xlabel('çœŸå®å½±å“åŠ›')
    ax1.set_ylabel('é¢„æµ‹å½±å“åŠ›')
    ax1.set_title(f'è®­ç»ƒé›†é¢„æµ‹ç»“æœ (æ’é™¤{method_info["exclude_pct"]}%)\nRÂ² = {results["metrics"]["train_r2"]:.4f}')
    ax1.grid(True, alpha=0.3)
    
    # æµ‹è¯•é›†
    ax2.scatter(y_test, y_test_pred, alpha=0.5, s=20, color='orange')
    ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    ax2.set_xlabel('çœŸå®å½±å“åŠ›')
    ax2.set_ylabel('é¢„æµ‹å½±å“åŠ›')
    ax2.set_title(f'æµ‹è¯•é›†é¢„æµ‹ç»“æœ (æ’é™¤{method_info["exclude_pct"]}%)\nRÂ² = {results["metrics"]["test_r2"]:.4f}')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    scatter_plot = os.path.join(output_dir, 'prediction_scatter.png')
    plt.savefig(scatter_plot, dpi=300, bbox_inches='tight')
    plt.close()

def generate_comparison_report(all_results, output_dir):
    """ğŸ”¥ æ–°å¢ï¼šç”Ÿæˆä¸åŒæ’é™¤æ¯”ä¾‹çš„å¯¹æ¯”æŠ¥å‘Š"""
    print(f"ğŸ“Š ç”Ÿæˆæ’é™¤æ¯”ä¾‹å¯¹æ¯”æŠ¥å‘Š...")
    
    # æ”¶é›†æ‰€æœ‰ç»“æœæ•°æ®
    comparison_data = []
    for method_name, result_data in all_results.items():
        exclude_pct = result_data['method_info']['exclude_pct']
        metrics = result_data['results']['metrics']
        feature_importance = result_data['feature_importance']
        
        # è·å–å‰3ä¸ªæœ€é‡è¦ç‰¹å¾
        top_3_features = feature_importance.head(3)['feature'].tolist()
        
        comparison_data.append({
            'exclude_percentage': exclude_pct,
            'train_r2': metrics['train_r2'],
            'test_r2': metrics['test_r2'],
            'train_mae': metrics['train_mae'],
            'test_mae': metrics['test_mae'],
            'train_samples': result_data['results']['X_train'].shape[0],
            'test_samples': result_data['results']['X_test'].shape[0],
            'top_3_features': ', '.join(top_3_features)
        })
    
    # è½¬æ¢ä¸ºDataFrame
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df = comparison_df.sort_values('exclude_percentage')
    
    # ä¿å­˜å¯¹æ¯”æ•°æ®
    comparison_csv = os.path.join(output_dir, 'exclude_percentage_comparison.csv')
    comparison_df.to_csv(comparison_csv, index=False)
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_file = os.path.join(output_dir, 'comparison_report.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("XGBoostå½±å“åŠ›é¢„æµ‹æ¨¡å‹ - å¼‚å¸¸ç”¨æˆ·æ’é™¤æ¯”ä¾‹å¯¹æ¯”æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç›®æ ‡ç”¨æˆ·: user_3855570307 (ç›¸å…³æ€§æœ€ä¼˜)\n")
        f.write(f"ç›®æ ‡å˜é‡: avg_popularity_of_all (æ€»ä½“è½¬èµè¯„å¹³å‡å€¼)\n")
        f.write(f"ç‰¹å¾æ•°é‡: 11ä¸ªç½‘ç»œæŒ‡æ ‡\n\n")
        
        f.write("=" * 60 + "\n")
        f.write("å„æ’é™¤æ¯”ä¾‹æ€§èƒ½å¯¹æ¯”\n")
        f.write("=" * 60 + "\n")
        f.write(f"{'æ’é™¤æ¯”ä¾‹':<10} {'æµ‹è¯•RÂ²':<10} {'æµ‹è¯•MAE':<10} {'è®­ç»ƒæ ·æœ¬':<10} {'æµ‹è¯•æ ·æœ¬':<10} {'å‰3é‡è¦ç‰¹å¾'}\n")
        f.write("-" * 100 + "\n")
        
        for _, row in comparison_df.iterrows():
            f.write(f"{row['exclude_percentage']:<10.0f}% {row['test_r2']:<10.4f} {row['test_mae']:<10.2f} "
                   f"{row['train_samples']:<10} {row['test_samples']:<10} {row['top_3_features']}\n")
        
        # æ‰¾å‡ºæœ€ä½³æ€§èƒ½
        best_r2_idx = comparison_df['test_r2'].idxmax()
        best_mae_idx = comparison_df['test_mae'].idxmin()
        
        f.write(f"\n" + "=" * 60 + "\n")
        f.write("æ€§èƒ½æ€»ç»“\n")
        f.write("=" * 60 + "\n")
        f.write(f"æœ€é«˜æµ‹è¯•RÂ²: {comparison_df.loc[best_r2_idx, 'exclude_percentage']:.0f}% "
                f"(RÂ² = {comparison_df.loc[best_r2_idx, 'test_r2']:.4f})\n")
        f.write(f"æœ€ä½æµ‹è¯•MAE: {comparison_df.loc[best_mae_idx, 'exclude_percentage']:.0f}% "
                f"(MAE = {comparison_df.loc[best_mae_idx, 'test_mae']:.2f})\n")
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        f.write(f"\næ€§èƒ½è¶‹åŠ¿åˆ†æ:\n")
        f.write(f"- RÂ²èŒƒå›´: {comparison_df['test_r2'].min():.4f} ~ {comparison_df['test_r2'].max():.4f}\n")
        f.write(f"- MAEèŒƒå›´: {comparison_df['test_mae'].min():.2f} ~ {comparison_df['test_mae'].max():.2f}\n")
        
        # æ ·æœ¬æ•°å˜åŒ–
        f.write(f"\næ ·æœ¬æ•°å˜åŒ–:\n")
        f.write(f"- è®­ç»ƒæ ·æœ¬: {comparison_df['train_samples'].max()} â†’ {comparison_df['train_samples'].min()}\n")
        f.write(f"- æµ‹è¯•æ ·æœ¬: {comparison_df['test_samples'].max()} â†’ {comparison_df['test_samples'].min()}\n")
    
    # ç”Ÿæˆæ€§èƒ½è¶‹åŠ¿å›¾
    plt.figure(figsize=(15, 10))
    
    # RÂ²è¶‹åŠ¿
    plt.subplot(2, 2, 1)
    plt.plot(comparison_df['exclude_percentage'], comparison_df['test_r2'], 'bo-', linewidth=2, markersize=8)
    plt.xlabel('æ’é™¤æ¯”ä¾‹ (%)')
    plt.ylabel('æµ‹è¯•é›† RÂ²')
    plt.title('æµ‹è¯•é›†RÂ²éšæ’é™¤æ¯”ä¾‹å˜åŒ–')
    plt.grid(True, alpha=0.3)
    
    # MAEè¶‹åŠ¿
    plt.subplot(2, 2, 2)
    plt.plot(comparison_df['exclude_percentage'], comparison_df['test_mae'], 'ro-', linewidth=2, markersize=8)
    plt.xlabel('æ’é™¤æ¯”ä¾‹ (%)')
    plt.ylabel('æµ‹è¯•é›† MAE')
    plt.title('æµ‹è¯•é›†MAEéšæ’é™¤æ¯”ä¾‹å˜åŒ–')
    plt.grid(True, alpha=0.3)
    
    # è®­ç»ƒæ ·æœ¬æ•°è¶‹åŠ¿
    plt.subplot(2, 2, 3)
    plt.plot(comparison_df['exclude_percentage'], comparison_df['train_samples'], 'go-', linewidth=2, markersize=8)
    plt.xlabel('æ’é™¤æ¯”ä¾‹ (%)')
    plt.ylabel('è®­ç»ƒæ ·æœ¬æ•°')
    plt.title('è®­ç»ƒæ ·æœ¬æ•°éšæ’é™¤æ¯”ä¾‹å˜åŒ–')
    plt.grid(True, alpha=0.3)
    
    # RÂ²å¯¹æ¯”ï¼ˆè®­ç»ƒvsæµ‹è¯•ï¼‰
    plt.subplot(2, 2, 4)
    plt.plot(comparison_df['exclude_percentage'], comparison_df['train_r2'], 'b-', label='è®­ç»ƒé›†RÂ²', linewidth=2)
    plt.plot(comparison_df['exclude_percentage'], comparison_df['test_r2'], 'r-', label='æµ‹è¯•é›†RÂ²', linewidth=2)
    plt.xlabel('æ’é™¤æ¯”ä¾‹ (%)')
    plt.ylabel('RÂ²')
    plt.title('è®­ç»ƒé›†vsæµ‹è¯•é›†RÂ²å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    trend_plot = os.path.join(output_dir, 'exclude_percentage_trends.png')
    plt.savefig(trend_plot, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print(f"âœ… å¯¹æ¯”æ•°æ®å·²ä¿å­˜: {comparison_csv}")
    print(f"âœ… è¶‹åŠ¿å›¾å·²ä¿å­˜: {trend_plot}")
    
    return comparison_df

def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    print(f"XGBoostå½±å“åŠ›é¢„æµ‹æ¨¡å‹ - å¤šæ¯”ä¾‹æµ‹è¯•ç‰ˆ")
    print(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print("ğŸ¯ ç›®æ ‡ï¼šæµ‹è¯•ä¸åŒå¼‚å¸¸ç”¨æˆ·æ’é™¤æ¯”ä¾‹å¯¹é¢„æµ‹æ€§èƒ½çš„å½±å“")
    print("ğŸ“Š è¾“å…¥ï¼š11ä¸ªç½‘ç»œæŒ‡æ ‡ï¼ˆå¯†åº¦ã€èšç±»ç³»æ•°ç­‰ï¼‰")
    print("ğŸ¯ è¾“å‡ºï¼šavg_popularity_of_allï¼ˆæ€»ä½“è½¬èµè¯„å¹³å‡å€¼ï¼‰")
    print("ğŸ”§ æ¨¡å‹ï¼šXGBoostå›å½’å™¨")
    print("ğŸ‘¤ ç›®æ ‡ç”¨æˆ·ï¼š3855570307ï¼ˆç›¸å…³æ€§æœ€ä¼˜ï¼‰")
    print("ğŸ“ˆ æµ‹è¯•èŒƒå›´ï¼š0% ~ 40% å¼‚å¸¸ç”¨æˆ·æ’é™¤æ¯”ä¾‹")
    print("=" * 80)
    
    # ğŸ”¥ ä¿®æ”¹ï¼šä½¿ç”¨ç›¸å…³æ€§æœ€å¥½çš„ç”¨æˆ·3855570307
    data_path = 'C:/Tengfei/data/results/user_3855570307_metrics/merged_metrics_popularity.csv'
    output_dir = 'C:/Tengfei/data/results/prediction_results/user_3855570307_multi_exclude'
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(data_path):
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
        print("è¯·å…ˆç¡®ä¿ç”¨æˆ·3855570307çš„æ•°æ®å·²å‡†å¤‡å®Œæ¯•")
        return
    
    # ğŸ”¥ æ–°å¢ï¼šè‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„å¼‚å¸¸ç”¨æˆ·æ’é™¤æ–¹æ³•
    print(f"\nğŸ” è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„å¼‚å¸¸ç”¨æˆ·æ’é™¤æ–¹æ³•...")
    available_methods = detect_available_abnormal_methods()
    
    if not available_methods:
        print(f"âŒ æœªæ‰¾åˆ°ä»»ä½•å¼‚å¸¸ç”¨æˆ·æ’é™¤æ–¹æ³•")
        print("è¯·å…ˆè¿è¡Œ pick_out_abnormal_users.py ç”Ÿæˆå¼‚å¸¸ç”¨æˆ·æ•°æ®")
        return
    
    # ğŸ”¥ æ–°å¢ï¼šç­›é€‰0%-40%èŒƒå›´å†…çš„æ–¹æ³•
    print(f"\nğŸ“‹ ç­›é€‰0%-40%èŒƒå›´å†…çš„æ’é™¤æ–¹æ³•...")
    valid_methods = []
    
    for method in available_methods:
        exclude_pct = parse_exclude_percentage(method)
        if 0 <= exclude_pct <= 40:
            valid_methods.append({
                'name': method,
                'exclude_pct': exclude_pct,
                'description': f'æ’é™¤{exclude_pct}%å¼‚å¸¸ç”¨æˆ·' if exclude_pct > 0 else 'åŸå§‹ç½‘ç»œ'
            })
    
    # æŒ‰æ’é™¤æ¯”ä¾‹æ’åº
    valid_methods.sort(key=lambda x: x['exclude_pct'])
    
    if not valid_methods:
        print(f"âŒ åœ¨0%-40%èŒƒå›´å†…æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ’é™¤æ–¹æ³•")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(valid_methods)} ä¸ªæœ‰æ•ˆçš„æ’é™¤æ–¹æ³•:")
    for method in valid_methods:
        print(f"   - {method['exclude_pct']:5.1f}%: {method['description']}")
    
    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    print(f"\nâš ï¸ å°†æµ‹è¯• {len(valid_methods)} ç§ä¸åŒçš„æ’é™¤æ¯”ä¾‹")
    print(f"âš ï¸ æ¯ç§æ–¹æ³•é¢„è®¡éœ€è¦1-3åˆ†é’Ÿï¼Œæ€»è®¡çº¦ {len(valid_methods) * 2} åˆ†é’Ÿ")
    
    confirm = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
    if confirm != 'y':
        print("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return
    
    # ğŸ”¥ æ–°å¢ï¼šæ‰¹é‡æµ‹è¯•æ‰€æœ‰æ–¹æ³•
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•...")
    all_results = {}
    
    for i, method_info in enumerate(valid_methods, 1):
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•æ–¹æ³• [{i}/{len(valid_methods)}]: {method_info['description']}")
        print(f"æ’é™¤æ¯”ä¾‹: {method_info['exclude_pct']}%")
        print(f"{'='*80}")
        
        try:
            # åŠ è½½å¼‚å¸¸ç”¨æˆ·
            abnormal_users = load_abnormal_users(method_info['name'])
            print(f"âœ… åŠ è½½äº† {len(abnormal_users)} ä¸ªå¼‚å¸¸ç”¨æˆ·")
            
            # å‡†å¤‡æ•°æ®
            X, y, user_ids = prepare_features_and_target(data_path, abnormal_users, 'avg_popularity_of_all')
            
            if X is None:
                print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œè·³è¿‡æ­¤æ–¹æ³•")
                continue
            
            # è®­ç»ƒæ¨¡å‹
            results = train_xgboost_model(X, y)
            
            # ç‰¹å¾é‡è¦æ€§åˆ†æ
            feature_importance_df = analyze_feature_importance(results['model'], X.columns.tolist())
            
            # ä¿å­˜ç»“æœ
            save_method_results(results, feature_importance_df, method_info, output_dir)
            
            # å­˜å‚¨åˆ°æ€»ç»“æœä¸­
            all_results[method_info['name']] = {
                'method_info': method_info,
                'results': results,
                'feature_importance': feature_importance_df
            }
            
            print(f"âœ… æ–¹æ³• {method_info['description']} å®Œæˆ")
            print(f"   æµ‹è¯•é›†RÂ²: {results['metrics']['test_r2']:.4f}")
            print(f"   æµ‹è¯•é›†MAE: {results['metrics']['test_mae']:.2f}")
            
        except Exception as e:
            print(f"âŒ æ–¹æ³• {method_info['description']} å¤±è´¥: {e}")
            continue
    
    # ğŸ”¥ æ–°å¢ï¼šç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š
    if len(all_results) > 1:
        print(f"\nğŸ“Š ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š...")
        comparison_df = generate_comparison_report(all_results, output_dir)
        
        # æ˜¾ç¤ºæœ€ä½³ç»“æœ
        best_r2_row = comparison_df.loc[comparison_df['test_r2'].idxmax()]
        best_mae_row = comparison_df.loc[comparison_df['test_mae'].idxmin()]
        
        print(f"\nğŸ† æœ€ä½³æ€§èƒ½æ€»ç»“:")
        print(f"   ğŸ¯ æœ€é«˜RÂ²: æ’é™¤{best_r2_row['exclude_percentage']:.0f}% (RÂ² = {best_r2_row['test_r2']:.4f})")
        print(f"   ğŸ¯ æœ€ä½MAE: æ’é™¤{best_mae_row['exclude_percentage']:.0f}% (MAE = {best_mae_row['test_mae']:.2f})")
        
        # ä¸ä½ æåˆ°çš„35%è¿›è¡Œå¯¹æ¯”
        if 35 in comparison_df['exclude_percentage'].values:
            pct_35_row = comparison_df[comparison_df['exclude_percentage'] == 35].iloc[0]
            print(f"\nğŸ“Š 35%æ’é™¤æ¯”ä¾‹æ€§èƒ½ (ä½ æåˆ°çš„æœ€ä½³ç›¸å…³æ€§):")
            print(f"   RÂ²: {pct_35_row['test_r2']:.4f}")
            print(f"   MAE: {pct_35_row['test_mae']:.2f}")
    
    # æ€»ç»“
    end_time = datetime.now()
    duration = end_time - start_time
    
    print(f"\n" + "="*80)
    print(f"ğŸ‰ å¤šæ¯”ä¾‹XGBoosté¢„æµ‹æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶: {duration}")
    print(f"ğŸ“Š æµ‹è¯•æ–¹æ³•æ•°: {len(all_results)}")
    print(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: {output_dir}")
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   ğŸ“Š å„æ¯”ä¾‹ç‹¬ç«‹ç»“æœ: exclude_X%pct/ æ–‡ä»¶å¤¹")
    print(f"   ğŸ“ˆ ç»¼åˆå¯¹æ¯”æŠ¥å‘Š: comparison_report.txt")
    print(f"   ğŸ“‹ å¯¹æ¯”æ•°æ®è¡¨: exclude_percentage_comparison.csv")
    print(f"   ğŸ“‰ è¶‹åŠ¿åˆ†æå›¾: exclude_percentage_trends.png")
    print("="*80)

if __name__ == "__main__":
    main()