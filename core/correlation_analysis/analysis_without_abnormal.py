import os
import pandas as pd
import numpy as np
from scipy import stats
from datetime import datetime
import re

def ensure_dir(directory):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    if not os.path.exists(directory):
        os.makedirs(directory)

def normalize_id(id_value):
    """è§„èŒƒåŒ–ç”¨æˆ·IDï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´"""
    try:
        id_str = str(id_value).strip()
        if id_str == '-2147483648':
            return id_str
        return str(int(float(id_str)))
    except:
        return str(id_value).strip()

def detect_abnormal_user_folders():
    """è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹"""
    base_dir = 'results/pick_out_abnormal_users'
    
    if not os.path.exists(base_dir):
        print(f"é”™è¯¯: å¼‚å¸¸ç”¨æˆ·ç›®å½•ä¸å­˜åœ¨ {base_dir}")
        return []
    
    folders = []
    for item in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, item)
        if os.path.isdir(folder_path):
            csv_file = os.path.join(folder_path, 'abnormal_users.csv')
            if os.path.exists(csv_file):
                folders.append(item)
    
    # æŒ‰æ–‡ä»¶å¤¹åç§°æ’åºï¼Œç¡®ä¿åŸå§‹ç½‘ç»œæ’åœ¨å‰é¢
    folders.sort(key=lambda x: (0 if 'original' in x.lower() else 1, x))
    
    print(f"æ£€æµ‹åˆ° {len(folders)} ä¸ªå¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹:")
    for folder in folders:
        print(f"  - {folder}")
    
    return folders

def parse_folder_info(folder_name):
    """è§£ææ–‡ä»¶å¤¹åç§°ï¼Œæå–æ–¹æ³•å’Œæ’é™¤æ¯”ä¾‹ä¿¡æ¯"""
    if 'original_network' in folder_name.lower():
        return {
            'exclude_pct': 0.0,
            'methods': ['original'],
            'description': 'åŸå§‹ç½‘ç»œ',
            'short_name': 'Original',
            'folder_name': folder_name
        }
    
    # è§£æadvanced_method1_2_3_10.0pctæ ¼å¼
    match = re.search(r'advanced_(.+?)_(\d+(?:\.\d+)?)pct', folder_name)
    if match:
        methods_part = match.group(1)
        exclude_pct = float(match.group(2))
        
        # æå–æ–¹æ³•ç¼–å·
        methods = re.findall(r'method(\d+)', methods_part)
        method_names = [f"method{m}" for m in methods]
        
        # åˆ›å»ºç®€çŸ­æ¸…æ™°çš„åç§°
        short_name = f"æ’é™¤{exclude_pct}%"
        
        return {
            'exclude_pct': exclude_pct,
            'methods': method_names,
            'description': f'æ’é™¤å‰{exclude_pct}%å¼‚å¸¸ç”¨æˆ·ï¼ˆæ–¹æ³•: {", ".join(method_names)}ï¼‰',
            'short_name': short_name,
            'folder_name': folder_name
        }
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
    return {
        'exclude_pct': -1,
        'methods': ['unknown'],
        'description': f'æœªçŸ¥é…ç½®ï¼ˆ{folder_name}ï¼‰',
        'short_name': folder_name[:10],
        'folder_name': folder_name
    }

def load_abnormal_users_from_folder(folder_name):
    """ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½å¼‚å¸¸ç”¨æˆ·åˆ—è¡¨"""
    abnormal_file = f'results/pick_out_abnormal_users/{folder_name}/abnormal_users.csv'
    
    if not os.path.exists(abnormal_file):
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ–‡ä»¶ {abnormal_file}")
        return set()
    
    try:
        abnormal_df = pd.read_csv(abnormal_file)
        if len(abnormal_df) == 0:
            print(f"  - åŠ è½½äº† 0 ä¸ªå¼‚å¸¸ç”¨æˆ·ï¼ˆ{folder_name}ï¼‰")
            return set()
        
        abnormal_users = set(abnormal_df['user_id'].apply(normalize_id))
        print(f"  - åŠ è½½äº† {len(abnormal_users)} ä¸ªå¼‚å¸¸ç”¨æˆ·ï¼ˆ{folder_name}ï¼‰")
        return abnormal_users
    except Exception as e:
        print(f"åŠ è½½å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å‡ºé”™ {abnormal_file}: {e}")
        return set()

def detect_network_features(merged_df):
    """ğŸ”¥ ä¿®å¤ç‰ˆï¼šè‡ªåŠ¨æ£€æµ‹æ•°æ®ä¸­çš„ç½‘ç»œç‰¹å¾ï¼Œæ­£ç¡®æ’é™¤å› å˜é‡"""
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šéœ€è¦æ’é™¤çš„å­—æ®µï¼ˆæ‰€æœ‰éè‡ªå˜é‡å­—æ®µï¼‰
    excluded_columns = {
        'user_id',              # ç”¨æˆ·ID
        'center_node',          # ä¸­å¿ƒèŠ‚ç‚¹ï¼ˆä¸user_idé‡å¤ï¼‰
        'avg_popularity',       # ğŸ”¥ Y1ï¼šæœ€æ–°10æ¡å¾®åšå½±å“åŠ›ï¼ˆå› å˜é‡ï¼‰
        'avg_popularity_of_all', # ğŸ”¥ Y2ï¼šæ€»ä½“å¾®åšå½±å“åŠ›ï¼ˆå› å˜é‡ï¼‰
        'is_celebrity',         # æ˜æ˜Ÿç”¨æˆ·æ ‡è¯†ï¼ˆéç½‘ç»œæŒ‡æ ‡ï¼‰
        'user_category'         # ğŸ”¥ æ–°å¢ï¼šç”¨æˆ·ç±»åˆ«ï¼ˆéç½‘ç»œæŒ‡æ ‡ï¼‰
    }
    
    # ğŸ”¥ è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯åˆ†æçš„ç½‘ç»œç‰¹å¾ï¼ˆè‡ªå˜é‡ï¼‰
    network_features = []
    for col in merged_df.columns:
        if col not in excluded_columns:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼å‹
            if pd.api.types.is_numeric_dtype(merged_df[col]):
                network_features.append(col)
    
    # æŒ‰ç…§é‡è¦æ€§æ’åºï¼ˆä¼˜å…ˆæ˜¾ç¤ºä¼ ç»Ÿçš„6å¤§ç½‘ç»œæŒ‡æ ‡ï¼‰
    priority_order = [
        'density', 'clustering_coefficient', 'average_nearest_neighbor_degree',
        'betweenness_centrality', 'spectral_radius', 'modularity',
        'global_out_degree', 'global_in_degree', 'global_total_degree',
        'node_count', 'edge_count'
    ]
    
    # é‡æ–°æ’åºï¼šä¼˜å…ˆçº§ç‰¹å¾åœ¨å‰ï¼Œå…¶ä»–ç‰¹å¾åœ¨å
    ordered_features = []
    for feature in priority_order:
        if feature in network_features:
            ordered_features.append(feature)
            
    # æ·»åŠ å…¶ä»–æœªåœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­çš„ç‰¹å¾
    for feature in network_features:
        if feature not in ordered_features:
            ordered_features.append(feature)
    
    print(f"\nğŸ” è‡ªåŠ¨æ£€æµ‹åˆ° {len(ordered_features)} ä¸ªç½‘ç»œç‰¹å¾ï¼ˆè‡ªå˜é‡ï¼‰:")
    
    # ğŸ”¥ ä¿®å¤ï¼šæŒ‰ç±»åˆ«æ˜¾ç¤ºç‰¹å¾
    traditional_features = [f for f in ordered_features if f in priority_order[:6]]
    degree_features = [f for f in ordered_features if f in priority_order[6:9]]
    network_size_features = [f for f in ordered_features if f in priority_order[9:11]]
    other_features = [f for f in ordered_features if f not in priority_order]
    
    if traditional_features:
        print(f"  ğŸ“Š ä¼ ç»Ÿç½‘ç»œæŒ‡æ ‡ ({len(traditional_features)}ä¸ª): {', '.join(traditional_features)}")
    if degree_features:
        print(f"  ğŸ”— åº¦æ•°æŒ‡æ ‡ ({len(degree_features)}ä¸ª): {', '.join(degree_features)}")
    if network_size_features:
        print(f"  ğŸ“ ç½‘ç»œè§„æ¨¡æŒ‡æ ‡ ({len(network_size_features)}ä¸ª): {', '.join(network_size_features)}")
    if other_features:
        print(f"  â• å…¶ä»–æŒ‡æ ‡ ({len(other_features)}ä¸ª): {', '.join(other_features)}")
    
    return ordered_features

# ğŸ”¥ æ–°å¢ï¼šå½±å“åŠ›æŒ‡æ ‡é€‰æ‹©å‡½æ•°
def choose_popularity_metric(merged_df):
    """è®©ç”¨æˆ·é€‰æ‹©è¦åˆ†æçš„å½±å“åŠ›æŒ‡æ ‡ï¼ˆå› å˜é‡ï¼‰"""
    available_metrics = []
    
    # æ£€æŸ¥å¯ç”¨çš„å½±å“åŠ›æŒ‡æ ‡
    if 'avg_popularity' in merged_df.columns:
        available_metrics.append(('avg_popularity', 'Y1: æœ€æ–°10æ¡å¾®åšè½¬èµè¯„å¹³å‡å€¼'))
    
    if 'avg_popularity_of_all' in merged_df.columns:
        available_metrics.append(('avg_popularity_of_all', 'Y2: æ€»ä½“å¾®åšè½¬èµè¯„å¹³å‡å€¼'))
    
    if len(available_metrics) == 0:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å½±å“åŠ›æŒ‡æ ‡åˆ—")
        return None
    
    if len(available_metrics) == 1:
        metric_name, metric_desc = available_metrics[0]
        print(f"âœ… åªæ£€æµ‹åˆ°ä¸€ç§å½±å“åŠ›æŒ‡æ ‡: {metric_desc}")
        return metric_name
    
    # æœ‰å¤šä¸ªæŒ‡æ ‡ï¼Œè®©ç”¨æˆ·é€‰æ‹©
    print(f"\nğŸ¯ æ£€æµ‹åˆ°å¤šç§å½±å“åŠ›æŒ‡æ ‡ï¼Œè¯·é€‰æ‹©è¦åˆ†æçš„ç›®æ ‡å˜é‡ï¼ˆå› å˜é‡ï¼‰:")
    print("=" * 60)
    for i, (metric_name, metric_desc) in enumerate(available_metrics, 1):
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        non_zero_count = (merged_df[metric_name] > 0).sum()
        total_count = len(merged_df)
        mean_value = merged_df[metric_name].mean()
        max_value = merged_df[metric_name].max()
        
        print(f"{i}. {metric_desc}")
        print(f"   ğŸ“Š éé›¶ç”¨æˆ·: {non_zero_count}/{total_count} ({non_zero_count/total_count*100:.1f}%)")
        print(f"   ğŸ“Š å¹³å‡å€¼: {mean_value:.2f}, æœ€å¤§å€¼: {max_value:.2f}")
        print()
    
    print("3. åŒæ—¶åˆ†æä¸¤ç§æŒ‡æ ‡ï¼ˆç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šï¼‰")
    print("=" * 60)
    
    while True:
        try:
            choice = input("è¯·é€‰æ‹© (1/2/3): ").strip()
            if choice == '1':
                return available_metrics[0][0]
            elif choice == '2':
                return available_metrics[1][0]
            elif choice == '3':
                return 'both'  # ç‰¹æ®Šæ ‡è¯†ï¼Œè¡¨ç¤ºåˆ†æä¸¤ç§æŒ‡æ ‡
            else:
                print("è¯·è¾“å…¥æœ‰æ•ˆé€‰é¡¹ (1/2/3)")
        except KeyboardInterrupt:
            print("\nâŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return None

def calculate_correlations_without_abnormal(merged_df, abnormal_users, folder_info, popularity_metric):
    """ğŸ”¥ ä¿®æ”¹ç‰ˆï¼šæ”¯æŒé€‰æ‹©ä¸åŒçš„å½±å“åŠ›æŒ‡æ ‡è¿›è¡Œç›¸å…³æ€§è®¡ç®—ï¼Œå¢åŠ å¸¸æ•°æ£€æµ‹"""
    merged_df['user_id'] = merged_df['user_id'].apply(normalize_id)
    filtered_df = merged_df[~merged_df['user_id'].isin(abnormal_users)].copy()
    
    print(f"  - åŸå§‹ç”¨æˆ·æ•°: {len(merged_df)}")
    print(f"  - æ’é™¤å¼‚å¸¸ç”¨æˆ·æ•°: {len(abnormal_users)}")
    print(f"  - å‰©ä½™æ­£å¸¸ç”¨æˆ·æ•°: {len(filtered_df)}")
    
    if len(filtered_df) < 10:
        print(f"  - è­¦å‘Š: å‰©ä½™ç”¨æˆ·æ•°è¿‡å°‘ ({len(filtered_df)})ï¼Œå¯èƒ½å½±å“ç›¸å…³æ€§åˆ†æçš„å¯é æ€§")
    
    # è‡ªåŠ¨æ£€æµ‹ç½‘ç»œç‰¹å¾ï¼ˆè‡ªå˜é‡ï¼‰
    network_features = detect_network_features(filtered_df)
    
    if not network_features:
        print(f"  - é”™è¯¯: æœªæ£€æµ‹åˆ°ä»»ä½•å¯åˆ†æçš„ç½‘ç»œç‰¹å¾")
        return {}, len(merged_df), len(abnormal_users), len(filtered_df)
    
    # éªŒè¯é€‰æ‹©çš„å½±å“åŠ›æŒ‡æ ‡
    if popularity_metric not in filtered_df.columns:
        print(f"  - é”™è¯¯: é€‰æ‹©çš„å½±å“åŠ›æŒ‡æ ‡ {popularity_metric} ä¸åœ¨æ•°æ®ä¸­")
        return {}, len(merged_df), len(abnormal_users), len(filtered_df)
    
    print(f"  - ä½¿ç”¨å½±å“åŠ›æŒ‡æ ‡: {popularity_metric}")
    
    # ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥å› å˜é‡çš„å˜å¼‚æ€§
    valid_popularity = filtered_df[popularity_metric].dropna()
    if len(valid_popularity.unique()) <= 1:
        print(f"  - âš ï¸ è­¦å‘Š: å½±å“åŠ›æŒ‡æ ‡ {popularity_metric} åœ¨å‰©ä½™ç”¨æˆ·ä¸­ç¼ºä¹å˜å¼‚æ€§")
        print(f"  - å”¯ä¸€å€¼æ•°é‡: {len(valid_popularity.unique())}")
        print(f"  - æ‰€æœ‰ç”¨æˆ·å°†è¿”å›NaNç›¸å…³ç³»æ•°")
    
    # è®¡ç®—ç›¸å…³æ€§
    correlations = {}
    constant_features = []  # è®°å½•å¸¸æ•°ç‰¹å¾
    valid_correlations = 0  # è®°å½•æœ‰æ•ˆç›¸å…³æ€§æ•°é‡
    
    for feature in network_features:
        if feature not in filtered_df.columns:
            print(f"  - è­¦å‘Š: ç‰¹å¾ {feature} ä¸åœ¨æ•°æ®ä¸­")
            correlations[feature] = {
                'spearman_corr': np.nan,
                'spearman_p': np.nan,
                'kendall_corr': np.nan,
                'kendall_p': np.nan
            }
            continue
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            valid_mask = (~pd.isna(filtered_df[feature])) & (~pd.isna(filtered_df[popularity_metric]))
            valid_feature = filtered_df.loc[valid_mask, feature]
            valid_popularity = filtered_df.loc[valid_mask, popularity_metric]
            
            if len(valid_feature) < 3:
                print(f"  - è­¦å‘Š: ç‰¹å¾ {feature} æœ‰æ•ˆæ•°æ®ç‚¹è¿‡å°‘ ({len(valid_feature)})")
                correlations[feature] = {
                    'spearman_corr': np.nan,
                    'spearman_p': np.nan,
                    'kendall_corr': np.nan,
                    'kendall_p': np.nan
                }
                continue
            
            # ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥ç‰¹å¾çš„å˜å¼‚æ€§
            unique_feature_values = len(valid_feature.unique())
            unique_popularity_values = len(valid_popularity.unique())
            
            if unique_feature_values <= 1:
                print(f"  - âš ï¸ {feature}: ç‰¹å¾å€¼æ— å˜å¼‚æ€§ (å”¯ä¸€å€¼æ•°={unique_feature_values})")
                constant_features.append(feature)
                correlations[feature] = {
                    'spearman_corr': np.nan,
                    'spearman_p': np.nan,
                    'kendall_corr': np.nan,
                    'kendall_p': np.nan
                }
                continue
            
            if unique_popularity_values <= 1:
                print(f"  - âš ï¸ {feature}: å½±å“åŠ›æŒ‡æ ‡æ— å˜å¼‚æ€§ (å”¯ä¸€å€¼æ•°={unique_popularity_values})")
                correlations[feature] = {
                    'spearman_corr': np.nan,
                    'spearman_p': np.nan,
                    'kendall_corr': np.nan,
                    'kendall_p': np.nan
                }
                continue
            
            # ğŸ”¥ æ–°å¢ï¼šé¢å¤–çš„å˜å¼‚æ€§æ£€æŸ¥
            feature_std = valid_feature.std()
            popularity_std = valid_popularity.std()
            
            if feature_std == 0:
                print(f"  - âš ï¸ {feature}: ç‰¹å¾æ ‡å‡†å·®ä¸º0ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
                constant_features.append(feature)
                correlations[feature] = {
                    'spearman_corr': np.nan,
                    'spearman_p': np.nan,
                    'kendall_corr': np.nan,
                    'kendall_p': np.nan
                }
                continue
            
            if popularity_std == 0:
                print(f"  - âš ï¸ {feature}: å½±å“åŠ›æŒ‡æ ‡æ ‡å‡†å·®ä¸º0ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")
                correlations[feature] = {
                    'spearman_corr': np.nan,
                    'spearman_p': np.nan,
                    'kendall_corr': np.nan,
                    'kendall_p': np.nan
                }
                continue
            
            # è®¡ç®—Spearmanç›¸å…³ç³»æ•°
            with np.errstate(all='ignore'):  # æŠ‘åˆ¶numpyè­¦å‘Š
                spearman_corr, spearman_p = stats.spearmanr(valid_feature, valid_popularity)
            
            # è®¡ç®—Kendallç›¸å…³ç³»æ•°
            with np.errstate(all='ignore'):  # æŠ‘åˆ¶numpyè­¦å‘Š
                kendall_corr, kendall_p = stats.kendalltau(valid_feature, valid_popularity)
            
            # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if np.isnan(spearman_corr) and np.isnan(kendall_corr):
                print(f"  - âš ï¸ {feature}: ç›¸å…³ç³»æ•°è®¡ç®—ç»“æœä¸ºNaN")
            else:
                valid_correlations += 1
            
            correlations[feature] = {
                'spearman_corr': spearman_corr if not np.isnan(spearman_corr) else np.nan,
                'spearman_p': spearman_p if not np.isnan(spearman_p) else np.nan,
                'kendall_corr': kendall_corr if not np.isnan(kendall_corr) else np.nan,
                'kendall_p': kendall_p if not np.isnan(kendall_p) else np.nan
            }
            
            if not np.isnan(spearman_corr) and not np.isnan(kendall_corr):
                print(f"  - {feature}: Spearman={spearman_corr:.4f}(p={spearman_p:.4f}), Kendall={kendall_corr:.4f}(p={kendall_p:.4f})")
            else:
                print(f"  - {feature}: Spearman={spearman_corr:.4f}(p={spearman_p:.4f}), Kendall={kendall_corr:.4f}(p={kendall_p:.4f}) [è­¦å‘Š: åŒ…å«NaN]")
            
        except Exception as e:
            print(f"  - è®¡ç®— {feature} ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
            correlations[feature] = {
                'spearman_corr': np.nan,
                'spearman_p': np.nan,
                'kendall_corr': np.nan,
                'kendall_p': np.nan
            }
    
    # ğŸ”¥ æ–°å¢ï¼šæ€»ç»“åˆ†æç»“æœ
    print(f"\n  ğŸ“Š ç›¸å…³æ€§åˆ†ææ€»ç»“:")
    print(f"     æ€»ç‰¹å¾æ•°: {len(network_features)}")
    print(f"     å¸¸æ•°ç‰¹å¾æ•°: {len(constant_features)}")
    print(f"     æœ‰æ•ˆç›¸å…³æ€§æ•°: {valid_correlations}")
    print(f"     å¤±æ•ˆæ¯”ä¾‹: {(len(network_features)-valid_correlations)/len(network_features)*100:.1f}%")
    
    if constant_features:
        print(f"  âš ï¸ å¸¸æ•°ç‰¹å¾åˆ—è¡¨: {', '.join(constant_features)}")
        print(f"  ğŸ’¡ åŸå› ï¼šæ’é™¤å¼‚å¸¸ç”¨æˆ·åï¼Œå‰©ä½™ç”¨æˆ·åœ¨è¿™äº›ç‰¹å¾ä¸Šé«˜åº¦åŒè´¨åŒ–")
    
    if valid_correlations == 0:
        print(f"  ğŸš¨ è­¦å‘Šï¼šæ‰€æœ‰ç‰¹å¾çš„ç›¸å…³æ€§è®¡ç®—éƒ½å¤±è´¥")
        print(f"  ğŸ’¡ å»ºè®®ï¼šé™ä½æ’é™¤æ¯”ä¾‹æˆ–æ£€æŸ¥æ•°æ®è´¨é‡")
    elif valid_correlations < len(network_features) * 0.5:
        print(f"  âš ï¸ è­¦å‘Šï¼šè¶…è¿‡50%çš„ç‰¹å¾æ— æ³•è®¡ç®—æœ‰æ•ˆç›¸å…³æ€§")
        print(f"  ğŸ’¡ å»ºè®®ï¼šè€ƒè™‘é™ä½æ’é™¤æ¯”ä¾‹")
    
    return correlations, len(merged_df), len(abnormal_users), len(filtered_df)

def save_results(correlations, original_count, excluded_count, remaining_count, 
                folder_info, output_dir, popularity_metric):
    """ğŸ”¥ ä¿®æ”¹ç‰ˆï¼šä¿å­˜åˆ†æç»“æœï¼Œæ”¯æŒä¸åŒå½±å“åŠ›æŒ‡æ ‡"""
    result_dir = os.path.join(output_dir, folder_info['folder_name'])
    ensure_dir(result_dir)
    
    # ğŸ”¥ æ–°å¢ï¼šæ ¹æ®å½±å“åŠ›æŒ‡æ ‡è°ƒæ•´æ–‡ä»¶å
    metric_suffix = ""
    if popularity_metric == 'avg_popularity':
        metric_suffix = "_y1_recent10"
    elif popularity_metric == 'avg_popularity_of_all':
        metric_suffix = "_y2_total"
    
    # ä¿å­˜è¯¦ç»†çš„ç›¸å…³æ€§ç»“æœåˆ°CSV
    results_data = []
    for feature, corr_data in correlations.items():
        results_data.append({
            'feature': feature,
            'spearman_correlation': corr_data['spearman_corr'],
            'spearman_p_value': corr_data['spearman_p'],
            'kendall_correlation': corr_data['kendall_corr'],
            'kendall_p_value': corr_data['kendall_p']
        })
    
    results_df = pd.DataFrame(results_data)
    csv_path = os.path.join(result_dir, f'correlation_results{metric_suffix}.csv')
    results_df.to_csv(csv_path, index=False)
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Šåˆ°TXT
    txt_path = os.path.join(result_dir, f'analysis_summary{metric_suffix}.txt')
    
    # ğŸ”¥ æ–°å¢ï¼šå½±å“åŠ›æŒ‡æ ‡æè¿°
    metric_descriptions = {
        'avg_popularity': 'Y1: æœ€æ–°10æ¡å¾®åšè½¬èµè¯„å¹³å‡å€¼',
        'avg_popularity_of_all': 'Y2: æ€»ä½“å¾®åšè½¬èµè¯„å¹³å‡å€¼'
    }
    metric_desc = metric_descriptions.get(popularity_metric, popularity_metric)
    
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(f"====== {folder_info['description']} ç›¸å…³æ€§åˆ†æç»“æœ ======\n")
        f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ£€æµ‹æ–¹æ³•: {', '.join(folder_info['methods'])}\n")
        f.write(f"æ’é™¤æ¯”ä¾‹: {folder_info['exclude_pct']}%\n")
        f.write(f"å½±å“åŠ›æŒ‡æ ‡: {metric_desc}\n\n")  # ğŸ”¥ æ–°å¢
        
        f.write(f"=== æ•°æ®ç»Ÿè®¡ ===\n")
        f.write(f"åŸå§‹ç”¨æˆ·æ€»æ•°: {original_count}\n")
        f.write(f"æ’é™¤å¼‚å¸¸ç”¨æˆ·æ•°: {excluded_count}\n")
        f.write(f"å‰©ä½™æ­£å¸¸ç”¨æˆ·æ•°: {remaining_count}\n")
        f.write(f"æ’é™¤æ¯”ä¾‹: {excluded_count/original_count*100:.2f}%\n")
        f.write(f"ä¿ç•™æ¯”ä¾‹: {remaining_count/original_count*100:.2f}%\n\n")
        
        # ğŸ”¥ ä¿®æ”¹ï¼šåŠ¨æ€æ ‡é¢˜ï¼Œæ”¯æŒä»»æ„æ•°é‡çš„ç‰¹å¾
        f.write(f"=== ç½‘ç»œç‰¹å¾ä¸{metric_desc}ç›¸å…³æ€§åˆ†æ (å…±{len(correlations)}ä¸ªç‰¹å¾) ===\n")
        f.write(f"{'ç‰¹å¾åç§°':<35} {'Spearmanç›¸å…³ç³»æ•°':<18} {'Spearman På€¼':<15} {'Kendallç›¸å…³ç³»æ•°':<17} {'Kendall På€¼':<15} {'æ˜¾è‘—æ€§'}\n")
        f.write("-" * 120 + "\n")
        
        for feature, corr_data in correlations.items():
            spearman_sig = "æ˜¾è‘—" if not pd.isna(corr_data['spearman_p']) and corr_data['spearman_p'] < 0.05 else "ä¸æ˜¾è‘—"
            kendall_sig = "æ˜¾è‘—" if not pd.isna(corr_data['kendall_p']) and corr_data['kendall_p'] < 0.05 else "ä¸æ˜¾è‘—"
            
            # ç»¼åˆæ˜¾è‘—æ€§åˆ¤æ–­
            overall_sig = "æ˜¾è‘—" if (spearman_sig == "æ˜¾è‘—" or kendall_sig == "æ˜¾è‘—") else "ä¸æ˜¾è‘—"
            
            f.write(f"{feature:<35} "
                   f"{corr_data['spearman_corr']:<18.4f} "
                   f"{corr_data['spearman_p']:<15.4f} "
                   f"{corr_data['kendall_corr']:<17.4f} "
                   f"{corr_data['kendall_p']:<15.4f} "
                   f"{overall_sig}\n")
        
        f.write(f"\n=== åˆ†æè¯´æ˜ ===\n")
        f.write(f"1. ä½¿ç”¨æ£€æµ‹æ–¹æ³•: {', '.join(folder_info['methods'])}\n")
        f.write(f"2. æ’é™¤æ¯”ä¾‹: {folder_info['exclude_pct']}%\n")
        f.write(f"3. å½±å“åŠ›æŒ‡æ ‡: {metric_desc}\n")
        f.write(f"4. è‡ªåŠ¨æ£€æµ‹åˆ° {len(correlations)} ä¸ªç½‘ç»œç‰¹å¾è¿›è¡Œåˆ†æ\n")
        f.write(f"5. Spearmanç›¸å…³ç³»æ•°è¡¡é‡å•è°ƒå…³ç³»ï¼ŒKendallç›¸å…³ç³»æ•°è¡¡é‡åºåˆ—ä¸€è‡´æ€§\n")
        f.write(f"6. På€¼<0.05è®¤ä¸ºç›¸å…³æ€§æ˜¾è‘—\n")
        f.write(f"7. ç›¸å…³ç³»æ•°ç»å¯¹å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºç›¸å…³æ€§è¶Šå¼º\n")
        f.write(f"8. å·²è‡ªåŠ¨æ’é™¤éåˆ†æå­—æ®µ: user_id, center_node, avg_popularity, avg_popularity_of_all, is_celebrity, user_category\n")
    
    print(f"  - ç»“æœå·²ä¿å­˜åˆ°: {result_dir}")
    return csv_path, txt_path

# ğŸ”¥ æ–°å¢ï¼šåŒé‡åˆ†æåŠŸèƒ½
def analyze_both_metrics(merged_df, abnormal_folders, output_dir):
    """åŒæ—¶åˆ†æä¸¤ç§å½±å“åŠ›æŒ‡æ ‡å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    
    if 'avg_popularity' not in merged_df.columns or 'avg_popularity_of_all' not in merged_df.columns:
        print("âŒ æ•°æ®ä¸­ç¼ºå°‘å®Œæ•´çš„åŒé‡å½±å“åŠ›æŒ‡æ ‡ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”åˆ†æ")
        return
    
    print(f"\nğŸ”„ å¼€å§‹åŒé‡å½±å“åŠ›æŒ‡æ ‡å¯¹æ¯”åˆ†æ...")
    
    all_results_y1 = {}
    all_results_y2 = {}
    
    # åˆ†åˆ«åˆ†æä¸¤ç§æŒ‡æ ‡
    for folder_name in abnormal_folders:
        print(f"\n{'='*60}")
        folder_info = parse_folder_info(folder_name)
        print(f"åˆ†æé…ç½®: {folder_info['description']}")
        
        # åŠ è½½å¼‚å¸¸ç”¨æˆ·åˆ—è¡¨
        abnormal_users = load_abnormal_users_from_folder(folder_name)
        
        # åˆ†æY1ï¼ˆæœ€æ–°10æ¡ï¼‰
        print(f"  ğŸ“Š åˆ†æY1: æœ€æ–°10æ¡å¾®åšå½±å“åŠ›...")
        correlations_y1, original_count, excluded_count, remaining_count = calculate_correlations_without_abnormal(
            merged_df, abnormal_users, folder_info, 'avg_popularity')
        
        # åˆ†æY2ï¼ˆæ€»ä½“ï¼‰
        print(f"  ğŸ“Š åˆ†æY2: æ€»ä½“å¾®åšå½±å“åŠ›...")
        correlations_y2, _, _, _ = calculate_correlations_without_abnormal(
            merged_df, abnormal_users, folder_info, 'avg_popularity_of_all')
        
        # ä¿å­˜ç»“æœ
        save_results(correlations_y1, original_count, excluded_count, remaining_count, 
                    folder_info, output_dir, 'avg_popularity')
        save_results(correlations_y2, original_count, excluded_count, remaining_count, 
                    folder_info, output_dir, 'avg_popularity_of_all')
        
        # å­˜å‚¨ç»“æœç”¨äºå¯¹æ¯”
        all_results_y1[folder_name] = {
            'folder_info': folder_info,
            'correlations': correlations_y1,
            'counts': (original_count, excluded_count, remaining_count)
        }
        all_results_y2[folder_name] = {
            'folder_info': folder_info,
            'correlations': correlations_y2,
            'counts': (original_count, excluded_count, remaining_count)
        }
    
    # ğŸ”¥ ç”ŸæˆåŒé‡æŒ‡æ ‡å¯¹æ¯”æ±‡æ€»æŠ¥å‘Š
    print(f"\nğŸ”„ ç”ŸæˆåŒé‡å½±å“åŠ›æŒ‡æ ‡å¯¹æ¯”æ±‡æ€»æŠ¥å‘Š...")
    
    comparison_path = os.path.join(output_dir, 'dual_metrics_comparison_summary.txt')
    with open(comparison_path, 'w', encoding='utf-8') as f:
        f.write("====== åŒé‡å½±å“åŠ›æŒ‡æ ‡ç›¸å…³æ€§åˆ†æå¯¹æ¯”æ±‡æ€» ======\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("=== å½±å“åŠ›æŒ‡æ ‡è¯´æ˜ ===\n")
        f.write("Y1 (avg_popularity): æœ€æ–°10æ¡å¾®åšè½¬èµè¯„å¹³å‡å€¼\n")
        f.write("Y2 (avg_popularity_of_all): æ€»ä½“å¾®åšè½¬èµè¯„å¹³å‡å€¼\n\n")
        
        # é…ç½®æ¦‚è§ˆ
        f.write("=== åˆ†æé…ç½®æ¦‚è§ˆ ===\n")
        for i, folder_name in enumerate(abnormal_folders, 1):
            folder_info = all_results_y1[folder_name]['folder_info']
            f.write(f"{i}. {folder_info['short_name']}: {folder_info['description']}\n")
        f.write("\n")
        
        # è·å–æ‰€æœ‰ç‰¹å¾
        all_features = set()
        for folder_name in abnormal_folders:
            all_features.update(all_results_y1[folder_name]['correlations'].keys())
        features = sorted(list(all_features))
        
        # Y1 vs Y2 Spearmanç›¸å…³ç³»æ•°å¯¹æ¯”
        f.write(f"=== Y1 vs Y2 Spearmanç›¸å…³ç³»æ•°å¯¹æ¯” (å…±{len(features)}ä¸ªç‰¹å¾) ===\n")
        
        # Y1æ•°æ®
        f.write(f"\n--- Y1 (æœ€æ–°10æ¡) Spearmanç›¸å…³ç³»æ•° ---\n")
        header = f"{'ç‰¹å¾':<35} "
        for folder_name in abnormal_folders:
            folder_info = all_results_y1[folder_name]['folder_info']
            header += f"{folder_info['short_name']:<15} "
        f.write(header + "\n")
        f.write("-" * (35 + 15 * len(abnormal_folders)) + "\n")
        
        for feature in features:
            line = f"{feature:<35} "
            for folder_name in abnormal_folders:
                if feature in all_results_y1[folder_name]['correlations']:
                    corr = all_results_y1[folder_name]['correlations'][feature]['spearman_corr']
                    if pd.isna(corr):
                        line += f"{'N/A':<15}"
                    else:
                        line += f"{corr:<15.4f}"
                else:
                    line += f"{'N/A':<15}"
            f.write(line + "\n")
        
        # Y2æ•°æ®
        f.write(f"\n--- Y2 (æ€»ä½“) Spearmanç›¸å…³ç³»æ•° ---\n")
        header = f"{'ç‰¹å¾':<35} "
        for folder_name in abnormal_folders:
            folder_info = all_results_y2[folder_name]['folder_info']
            header += f"{folder_info['short_name']:<15} "
        f.write(header + "\n")
        f.write("-" * (35 + 15 * len(abnormal_folders)) + "\n")
        
        for feature in features:
            line = f"{feature:<35} "
            for folder_name in abnormal_folders:
                if feature in all_results_y2[folder_name]['correlations']:
                    corr = all_results_y2[folder_name]['correlations'][feature]['spearman_corr']
                    if pd.isna(corr):
                        line += f"{'N/A':<15}"
                    else:
                        line += f"{corr:<15.4f}"
                else:
                    line += f"{'N/A':<15}"
            f.write(line + "\n")
        
        # Y1 vs Y2å·®å¼‚åˆ†æ
        f.write(f"\n--- Y1 vs Y2 ç›¸å…³ç³»æ•°å·®å¼‚ (Y2 - Y1) ---\n")
        header = f"{'ç‰¹å¾':<35} "
        for folder_name in abnormal_folders:
            folder_info = all_results_y1[folder_name]['folder_info']
            header += f"{folder_info['short_name']:<15} "
        f.write(header + "\n")
        f.write("-" * (35 + 15 * len(abnormal_folders)) + "\n")
        
        for feature in features:
            line = f"{feature:<35} "
            for folder_name in abnormal_folders:
                corr_y1 = all_results_y1[folder_name]['correlations'].get(feature, {}).get('spearman_corr', np.nan)
                corr_y2 = all_results_y2[folder_name]['correlations'].get(feature, {}).get('spearman_corr', np.nan)
                
                if pd.isna(corr_y1) or pd.isna(corr_y2):
                    line += f"{'N/A':<15}"
                else:
                    diff = corr_y2 - corr_y1
                    line += f"{diff:<15.4f}"
            f.write(line + "\n")
        
        f.write(f"\n=== åˆ†æè¯´æ˜ ===\n")
        f.write(f"1. Y1é€‚åˆåˆ†æè¿‘æœŸæ´»è·ƒåº¦ä¸ç½‘ç»œç»“æ„çš„å…³ç³»\n")
        f.write(f"2. Y2é€‚åˆåˆ†ææ•´ä½“å½±å“åŠ›ä¸ç½‘ç»œç»“æ„çš„å…³ç³»\n")
        f.write(f"3. æ­£å·®å¼‚è¡¨ç¤ºY2ç›¸å…³æ€§æ›´å¼ºï¼Œè´Ÿå·®å¼‚è¡¨ç¤ºY1ç›¸å…³æ€§æ›´å¼º\n")
        f.write(f"4. å»ºè®®é‡ç‚¹å…³æ³¨å·®å¼‚è¾ƒå¤§çš„ç‰¹å¾ï¼Œå¯èƒ½æ­ç¤ºä¸åŒæ—¶é—´å°ºåº¦ä¸‹çš„å½±å“æœºåˆ¶\n")
    
    print(f"âœ… åŒé‡æŒ‡æ ‡å¯¹æ¯”æ±‡æ€»æŠ¥å‘Š: {comparison_path}")

def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    print(f"å¼€å§‹åŒé‡å½±å“åŠ›æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ...")
    print(f"åˆ†ææ—¶é—´: {start_time}")
    
    # ğŸ”¥ ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„æ•°æ®è·¯å¾„
    merged_data_path = 'C:/Tengfei/data/results/topic_å­™é¢–è_metrics/merged_metrics_popularity.csv'
    
    if not os.path.exists(merged_data_path):
        print(f"é”™è¯¯: æœªæ‰¾åˆ°åˆå¹¶æ•°æ®æ–‡ä»¶ {merged_data_path}")
        print("è¯·å…ˆè¿è¡Œ create3.py ç”Ÿæˆ merged_metrics_popularity.csv")
        return
    
    print(f"æ­£åœ¨åŠ è½½åˆå¹¶æ•°æ®: {merged_data_path}")
    try:
        merged_df = pd.read_csv(merged_data_path)
        print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼ŒåŒ…å« {len(merged_df)} ä¸ªç”¨æˆ·")
    except Exception as e:
        print(f"åŠ è½½åˆå¹¶æ•°æ®å‡ºé”™: {e}")
        return
    
    # ğŸ”¥ ä¿®æ”¹ï¼šæ£€æŸ¥å¿…è¦çš„åˆ—
    required_columns = ['user_id']
    
    missing_columns = [col for col in required_columns if col not in merged_df.columns]
    if missing_columns:
        print(f"é”™è¯¯: åˆå¹¶æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
        return
    
    print(f"âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
    print(f"ğŸ“Š æ•°æ®åŒ…å«åˆ—: {list(merged_df.columns)}")
    
    # ğŸ”¥ æ–°å¢ï¼šé€‰æ‹©å½±å“åŠ›æŒ‡æ ‡
    print(f"\n{'='*60}")
    print(f"é€‰æ‹©å½±å“åŠ›æŒ‡æ ‡...")
    popularity_metric = choose_popularity_metric(merged_df)
    
    if popularity_metric is None:
        print("âŒ æœªé€‰æ‹©å½±å“åŠ›æŒ‡æ ‡ï¼Œç¨‹åºé€€å‡º")
        return
    
    # è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹
    print(f"\n{'='*60}")
    print(f"è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹...")
    abnormal_folders = detect_abnormal_user_folders()
    
    if not abnormal_folders:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹")
        print("è¯·å…ˆè¿è¡Œ pick_out_abnormal_users.py ç”Ÿæˆå¼‚å¸¸ç”¨æˆ·æ•°æ®")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'results/correlation_result'
    ensure_dir(output_dir)
    
    # ğŸ”¥ æ–°å¢ï¼šæ ¹æ®é€‰æ‹©çš„æŒ‡æ ‡æ‰§è¡Œä¸åŒçš„åˆ†æ
    if popularity_metric == 'both':
        # åŒé‡åˆ†ææ¨¡å¼
        analyze_both_metrics(merged_df, abnormal_folders, output_dir)
    else:
        # å•ä¸€æŒ‡æ ‡åˆ†ææ¨¡å¼
        print(f"\n{'='*60}")
        print(f"å¼€å§‹åˆ†æ {len(abnormal_folders)} ç§é…ç½®ä¸‹çš„ç›¸å…³æ€§...")
        print(f"ä½¿ç”¨å½±å“åŠ›æŒ‡æ ‡: {popularity_metric}")
        
        all_results = {}
        
        # åˆ†ææ¯ä¸ªé…ç½®
        for folder_name in abnormal_folders:
            print(f"\n{'='*60}")
            
            # è§£ææ–‡ä»¶å¤¹ä¿¡æ¯
            folder_info = parse_folder_info(folder_name)
            print(f"åˆ†æé…ç½®: {folder_info['description']}")
            print(f"æ–‡ä»¶å¤¹: {folder_name}")
            print(f"{'='*60}")
            
            # åŠ è½½å¼‚å¸¸ç”¨æˆ·åˆ—è¡¨
            abnormal_users = load_abnormal_users_from_folder(folder_name)
            
            # è®¡ç®—ç›¸å…³æ€§
            print(f"  - å¼€å§‹è®¡ç®—ç›¸å…³æ€§...")
            correlations, original_count, excluded_count, remaining_count = calculate_correlations_without_abnormal(
                merged_df, abnormal_users, folder_info, popularity_metric)
            
            # ä¿å­˜ç»“æœ
            csv_path, txt_path = save_results(correlations, original_count, excluded_count, 
                                            remaining_count, folder_info, output_dir, popularity_metric)
            
            # å­˜å‚¨ç»“æœç”¨äºåç»­æ¯”è¾ƒ
            all_results[folder_name] = {
                'folder_info': folder_info,
                'correlations': correlations,
                'original_count': original_count,
                'excluded_count': excluded_count,
                'remaining_count': remaining_count
            }
        
        # ğŸ”¥ ä¿®æ”¹ï¼šç”Ÿæˆå•ä¸€æŒ‡æ ‡å¯¹æ¯”æ±‡æ€»æŠ¥å‘Š
        print(f"\n{'='*60}")
        print(f"ç”Ÿæˆå¯¹æ¯”æ±‡æ€»æŠ¥å‘Š...")
        
        # æ–‡ä»¶ååŒ…å«æŒ‡æ ‡æ ‡è¯†
        metric_suffix = "_y1" if popularity_metric == 'avg_popularity' else "_y2"
        summary_path = os.path.join(output_dir, f'comprehensive_comparison_summary{metric_suffix}.txt')
        
        metric_descriptions = {
            'avg_popularity': 'Y1: æœ€æ–°10æ¡å¾®åšè½¬èµè¯„å¹³å‡å€¼',
            'avg_popularity_of_all': 'Y2: æ€»ä½“å¾®åšè½¬èµè¯„å¹³å‡å€¼'
        }
        metric_desc = metric_descriptions.get(popularity_metric, popularity_metric)
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("====== å¼‚å¸¸ç”¨æˆ·ç­›é€‰ç›¸å…³æ€§åˆ†æç»¼åˆå¯¹æ¯”æ±‡æ€» ======\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å½±å“åŠ›æŒ‡æ ‡: {metric_desc}\n\n")
            
            # å…¶ä½™å†…å®¹ä¸åŸç‰ˆç›¸åŒ...
            # é…ç½®æ¦‚è§ˆ
            f.write("=== åˆ†æé…ç½®æ¦‚è§ˆ ===\n")
            for i, folder_name in enumerate(abnormal_folders, 1):
                folder_info = all_results[folder_name]['folder_info']
                f.write(f"{i}. {folder_info['short_name']}: {folder_info['description']}\n")
            f.write("\n")
            
            # æ•°æ®é‡å¯¹æ¯”
            f.write("=== æ•°æ®é‡å¯¹æ¯” ===\n")
            f.write(f"{'é…ç½®':<15} {'åŸå§‹ç”¨æˆ·æ•°':<12} {'æ’é™¤ç”¨æˆ·æ•°':<12} {'å‰©ä½™ç”¨æˆ·æ•°':<12} {'æ’é™¤æ¯”ä¾‹':<10} {'ä¿ç•™æ¯”ä¾‹'}\n")
            f.write("-" * 75 + "\n")
            for folder_name in abnormal_folders:
                result = all_results[folder_name]
                folder_info = result['folder_info']
                exclude_pct = result['excluded_count'] / result['original_count'] * 100
                remain_pct = result['remaining_count'] / result['original_count'] * 100
                
                f.write(f"{folder_info['short_name']:<15} {result['original_count']:<12} {result['excluded_count']:<12} "
                       f"{result['remaining_count']:<12} {exclude_pct:<10.2f}% {remain_pct:.2f}%\n")
            
            # è·å–æ‰€æœ‰ç‰¹å¾è¿›è¡Œå¯¹æ¯”
            all_features = set()
            for folder_name in abnormal_folders:
                all_features.update(all_results[folder_name]['correlations'].keys())
            
            features = sorted(list(all_features))
            
            f.write(f"\n=== Spearmanç›¸å…³ç³»æ•°å¯¹æ¯” (å…±{len(features)}ä¸ªç‰¹å¾) ===\n")
            # æ„å»ºåˆ—æ ‡é¢˜
            header = f"{'ç‰¹å¾':<35} "
            for folder_name in abnormal_folders:
                folder_info = all_results[folder_name]['folder_info']
                header += f"{folder_info['short_name']:<15} "
            f.write(header + "\n")
            f.write("-" * (35 + 15 * len(abnormal_folders)) + "\n")
            
            for feature in features:
                line = f"{feature:<35} "
                for folder_name in abnormal_folders:
                    if feature in all_results[folder_name]['correlations']:
                        corr = all_results[folder_name]['correlations'][feature]['spearman_corr']
                        if pd.isna(corr):
                            line += f"{'N/A':<15}"
                        else:
                            line += f"{corr:<15.4f}"
                    else:
                        line += f"{'N/A':<15}"
                f.write(line + "\n")
            
            f.write(f"\n=== Kendallç›¸å…³ç³»æ•°å¯¹æ¯” (å…±{len(features)}ä¸ªç‰¹å¾) ===\n")
            # æ„å»ºåˆ—æ ‡é¢˜
            header = f"{'ç‰¹å¾':<35} "
            for folder_name in abnormal_folders:
                folder_info = all_results[folder_name]['folder_info']
                header += f"{folder_info['short_name']:<15} "
            f.write(header + "\n")
            f.write("-" * (35 + 15 * len(abnormal_folders)) + "\n")
            
            for feature in features:
                line = f"{feature:<35} "
                for folder_name in abnormal_folders:
                    if feature in all_results[folder_name]['correlations']:
                        corr = all_results[folder_name]['correlations'][feature]['kendall_corr']
                        if pd.isna(corr):
                            line += f"{'N/A':<15}"
                        else:
                            line += f"{corr:<15.4f}"
                    else:
                        line += f"{'N/A':<15}"
                f.write(line + "\n")
            
            f.write(f"\n=== åˆ†æè¯´æ˜ ===\n")
            f.write(f"1. åˆ†æçš„å½±å“åŠ›æŒ‡æ ‡: {metric_desc}\n")
            f.write(f"2. è‡ªåŠ¨æ£€æµ‹å¹¶åˆ†æäº† {len(features)} ä¸ªç½‘ç»œç‰¹å¾\n")
            f.write(f"3. å·²æ’é™¤éåˆ†æå­—æ®µ: user_id, center_node, avg_popularity, avg_popularity_of_all, is_celebrity, user_category\n")
            f.write(f"4. Original: åŸå§‹ç½‘ç»œï¼Œæœªæ’é™¤ä»»ä½•ç”¨æˆ·\n")
            for folder_name in abnormal_folders:
                folder_info = all_results[folder_name]['folder_info']
                if folder_info['exclude_pct'] > 0:
                    f.write(f"5. {folder_info['short_name']}: æ’é™¤å‰{folder_info['exclude_pct']}%å¼‚å¸¸ç”¨æˆ·\n")
        
        print(f"âœ… å•ä¸€æŒ‡æ ‡å¯¹æ¯”æ±‡æ€»æŠ¥å‘Š: {summary_path}")
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    print(f"\n{'='*60}")
    print(f"åˆ†æå®Œæˆï¼")
    print(f"æ€»è€—æ—¶: {duration}")
    print(f"å¤„ç†äº† {len(abnormal_folders)} ç§é…ç½®")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    # æ‰“å°ç®€è¦ç»“æœ
    print(f"\n=== åˆ†æç»“æœé¢„è§ˆ ===")
    if popularity_metric == 'both':
        print(f"å·²å®ŒæˆåŒé‡å½±å“åŠ›æŒ‡æ ‡å¯¹æ¯”åˆ†æ")
        print(f"ç”Ÿæˆäº†Y1å’ŒY2çš„ç‹¬ç«‹æŠ¥å‘Šä»¥åŠå¯¹æ¯”æ±‡æ€»æŠ¥å‘Š")
    else:
        metric_desc = "Y1(æœ€æ–°10æ¡)" if popularity_metric == 'avg_popularity' else "Y2(æ€»ä½“)"
        print(f"å·²å®Œæˆ {metric_desc} å½±å“åŠ›æŒ‡æ ‡åˆ†æ")

if __name__ == "__main__":
    main()