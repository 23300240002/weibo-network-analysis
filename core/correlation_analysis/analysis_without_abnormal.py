import os
import pandas as pd
import numpy as np
from scipy import stats
from datetime import datetime
import re

def ensure_dir(directory):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    if not os.path.exists(directory):
        os.makedirs(directory)

def normalize_id(id_value):
    """è§„èŒƒåŒ–ç”¨æˆ·IDï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´"""
    try:
        id_str = str(id_value).strip()
        if id_str == '-2147483648':
            return id_str
        return str(int(float(id_str)))
    except:
        return str(id_value).strip()

def detect_abnormal_user_folders():
    """è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹"""
    base_dir = 'results/pick_out_abnormal_users'
    
    if not os.path.exists(base_dir):
        print(f"é”™è¯¯: å¼‚å¸¸ç”¨æˆ·ç›®å½•ä¸å­˜åœ¨ {base_dir}")
        return []
    
    folders = []
    for item in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, item)
        if os.path.isdir(folder_path):
            csv_file = os.path.join(folder_path, 'abnormal_users.csv')
            if os.path.exists(csv_file):
                folders.append(item)
    
    # æŒ‰æ–‡ä»¶å¤¹åç§°æ’åºï¼Œç¡®ä¿åŸå§‹ç½‘ç»œæ’åœ¨å‰é¢
    folders.sort(key=lambda x: (0 if 'original' in x.lower() else 1, x))
    
    print(f"æ£€æµ‹åˆ° {len(folders)} ä¸ªå¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹:")
    for folder in folders:
        print(f"  - {folder}")
    
    return folders

def parse_folder_info(folder_name):
    """è§£ææ–‡ä»¶å¤¹åç§°ï¼Œæå–æ–¹æ³•å’Œæ’é™¤æ¯”ä¾‹ä¿¡æ¯"""
    if 'original_network' in folder_name.lower():
        return {
            'exclude_pct': 0.0,
            'methods': ['original'],
            'description': 'åŸå§‹ç½‘ç»œ',
            'short_name': 'Original',
            'folder_name': folder_name
        }
    
    # è§£æadvanced_method1_2_3_10.0pctæ ¼å¼
    match = re.search(r'advanced_(.+?)_(\d+(?:\.\d+)?)pct', folder_name)
    if match:
        methods_part = match.group(1)
        exclude_pct = float(match.group(2))
        
        # æå–æ–¹æ³•ç¼–å·
        methods = re.findall(r'method(\d+)', methods_part)
        method_names = [f"method{m}" for m in methods]
        
        # åˆ›å»ºç®€çŸ­æ¸…æ™°çš„åç§°
        short_name = f"æ’é™¤{exclude_pct}%"
        
        return {
            'exclude_pct': exclude_pct,
            'methods': method_names,
            'description': f'æ’é™¤å‰{exclude_pct}%å¼‚å¸¸ç”¨æˆ·ï¼ˆæ–¹æ³•: {", ".join(method_names)}ï¼‰',
            'short_name': short_name,
            'folder_name': folder_name
        }
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
    return {
        'exclude_pct': -1,
        'methods': ['unknown'],
        'description': f'æœªçŸ¥é…ç½®ï¼ˆ{folder_name}ï¼‰',
        'short_name': folder_name[:10],
        'folder_name': folder_name
    }

def load_abnormal_users_from_folder(folder_name):
    """ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½å¼‚å¸¸ç”¨æˆ·åˆ—è¡¨"""
    abnormal_file = f'results/pick_out_abnormal_users/{folder_name}/abnormal_users.csv'
    
    if not os.path.exists(abnormal_file):
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ–‡ä»¶ {abnormal_file}")
        return set()
    
    try:
        abnormal_df = pd.read_csv(abnormal_file)
        if len(abnormal_df) == 0:
            print(f"  - åŠ è½½äº† 0 ä¸ªå¼‚å¸¸ç”¨æˆ·ï¼ˆ{folder_name}ï¼‰")
            return set()
        
        abnormal_users = set(abnormal_df['user_id'].apply(normalize_id))
        print(f"  - åŠ è½½äº† {len(abnormal_users)} ä¸ªå¼‚å¸¸ç”¨æˆ·ï¼ˆ{folder_name}ï¼‰")
        return abnormal_users
    except Exception as e:
        print(f"åŠ è½½å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å‡ºé”™ {abnormal_file}: {e}")
        return set()

def detect_network_features(merged_df):
    """ğŸ”¥ æ–°å¢ï¼šè‡ªåŠ¨æ£€æµ‹æ•°æ®ä¸­çš„ç½‘ç»œç‰¹å¾ï¼Œæ’é™¤éåˆ†æå­—æ®µ"""
    # éœ€è¦æ’é™¤çš„å­—æ®µ
    excluded_columns = {
        'user_id',           # ç”¨æˆ·ID
        'center_node',       # ä¸­å¿ƒèŠ‚ç‚¹ï¼ˆä¸user_idé‡å¤ï¼‰
        'avg_popularity',    # å½±å“åŠ›ï¼ˆå› å˜é‡ï¼‰
        'is_celebrity'       # æ˜æ˜Ÿç”¨æˆ·æ ‡è¯†ï¼ˆéç½‘ç»œæŒ‡æ ‡ï¼‰
    }
    
    # ğŸ”¥ è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯åˆ†æçš„ç½‘ç»œç‰¹å¾
    network_features = []
    for col in merged_df.columns:
        if col not in excluded_columns:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼å‹
            if pd.api.types.is_numeric_dtype(merged_df[col]):
                network_features.append(col)
    
    # æŒ‰ç…§é‡è¦æ€§æ’åºï¼ˆä¼˜å…ˆæ˜¾ç¤ºä¼ ç»Ÿçš„6å¤§ç½‘ç»œæŒ‡æ ‡ï¼‰
    priority_order = [
        'density', 'clustering_coefficient', 'average_nearest_neighbor_degree',
        'betweenness_centrality', 'spectral_radius', 'modularity',
        'global_out_degree', 'global_in_degree', 'global_total_degree',
        'node_count', 'edge_count'
    ]
    
    # é‡æ–°æ’åºï¼šä¼˜å…ˆçº§ç‰¹å¾åœ¨å‰ï¼Œå…¶ä»–ç‰¹å¾åœ¨å
    ordered_features = []
    for feature in priority_order:
        if feature in network_features:
            ordered_features.append(feature)
            
    # æ·»åŠ å…¶ä»–æœªåœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­çš„ç‰¹å¾
    for feature in network_features:
        if feature not in ordered_features:
            ordered_features.append(feature)
    
    print(f"\nğŸ” è‡ªåŠ¨æ£€æµ‹åˆ° {len(ordered_features)} ä¸ªå¯åˆ†æçš„ç½‘ç»œç‰¹å¾:")
    
    # ğŸ”¥ æ–°å¢ï¼šæŒ‰ç±»åˆ«æ˜¾ç¤ºç‰¹å¾
    traditional_features = [f for f in ordered_features if f in priority_order[:6]]
    degree_features = [f for f in ordered_features if f in priority_order[6:9]]
    network_size_features = [f for f in ordered_features if f in priority_order[9:11]]
    other_features = [f for f in ordered_features if f not in priority_order]
    
    if traditional_features:
        print(f"  ğŸ“Š ä¼ ç»Ÿç½‘ç»œæŒ‡æ ‡ ({len(traditional_features)}ä¸ª): {', '.join(traditional_features)}")
    if degree_features:
        print(f"  ğŸ”— åº¦æ•°æŒ‡æ ‡ ({len(degree_features)}ä¸ª): {', '.join(degree_features)}")
    if network_size_features:
        print(f"  ğŸ“ ç½‘ç»œè§„æ¨¡æŒ‡æ ‡ ({len(network_size_features)}ä¸ª): {', '.join(network_size_features)}")
    if other_features:
        print(f"  â• å…¶ä»–æŒ‡æ ‡ ({len(other_features)}ä¸ª): {', '.join(other_features)}")
    
    return ordered_features

def calculate_correlations_without_abnormal(merged_df, abnormal_users, folder_info):
    """ğŸ”¥ ä¿®æ”¹ç‰ˆï¼šè®¡ç®—æ’é™¤å¼‚å¸¸ç”¨æˆ·åçš„ç›¸å…³æ€§ï¼Œè‡ªåŠ¨æ£€æµ‹æ‰€æœ‰ç½‘ç»œç‰¹å¾"""
    merged_df['user_id'] = merged_df['user_id'].apply(normalize_id)
    filtered_df = merged_df[~merged_df['user_id'].isin(abnormal_users)].copy()
    
    print(f"  - åŸå§‹ç”¨æˆ·æ•°: {len(merged_df)}")
    print(f"  - æ’é™¤å¼‚å¸¸ç”¨æˆ·æ•°: {len(abnormal_users)}")
    print(f"  - å‰©ä½™æ­£å¸¸ç”¨æˆ·æ•°: {len(filtered_df)}")
    
    if len(filtered_df) < 10:
        print(f"  - è­¦å‘Š: å‰©ä½™ç”¨æˆ·æ•°è¿‡å°‘ ({len(filtered_df)})ï¼Œå¯èƒ½å½±å“ç›¸å…³æ€§åˆ†æçš„å¯é æ€§")
    
    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šè‡ªåŠ¨æ£€æµ‹ç½‘ç»œç‰¹å¾
    network_features = detect_network_features(filtered_df)
    
    if not network_features:
        print(f"  - é”™è¯¯: æœªæ£€æµ‹åˆ°ä»»ä½•å¯åˆ†æçš„ç½‘ç»œç‰¹å¾")
        return {}, len(merged_df), len(abnormal_users), len(filtered_df)
    
    # è®¡ç®—ç›¸å…³æ€§
    correlations = {}
    
    for feature in network_features:
        if feature not in filtered_df.columns:
            print(f"  - è­¦å‘Š: ç‰¹å¾ {feature} ä¸åœ¨æ•°æ®ä¸­")
            correlations[feature] = {
                'spearman_corr': np.nan,
                'spearman_p': np.nan,
                'kendall_corr': np.nan,
                'kendall_p': np.nan
            }
            continue
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            valid_mask = (~pd.isna(filtered_df[feature])) & (~pd.isna(filtered_df['avg_popularity']))
            valid_feature = filtered_df.loc[valid_mask, feature]
            valid_popularity = filtered_df.loc[valid_mask, 'avg_popularity']
            
            if len(valid_feature) < 3:
                print(f"  - è­¦å‘Š: ç‰¹å¾ {feature} æœ‰æ•ˆæ•°æ®ç‚¹è¿‡å°‘ ({len(valid_feature)})")
                correlations[feature] = {
                    'spearman_corr': np.nan,
                    'spearman_p': np.nan,
                    'kendall_corr': np.nan,
                    'kendall_p': np.nan
                }
                continue
            
            # è®¡ç®—Spearmanç›¸å…³ç³»æ•°
            spearman_corr, spearman_p = stats.spearmanr(valid_feature, valid_popularity)
            
            # è®¡ç®—Kendallç›¸å…³ç³»æ•°
            kendall_corr, kendall_p = stats.kendalltau(valid_feature, valid_popularity)
            
            correlations[feature] = {
                'spearman_corr': spearman_corr,
                'spearman_p': spearman_p,
                'kendall_corr': kendall_corr,
                'kendall_p': kendall_p
            }
            
            print(f"  - {feature}: Spearman={spearman_corr:.4f}(p={spearman_p:.4f}), Kendall={kendall_corr:.4f}(p={kendall_p:.4f})")
            
        except Exception as e:
            print(f"  - è®¡ç®— {feature} ç›¸å…³æ€§æ—¶å‡ºé”™: {e}")
            correlations[feature] = {
                'spearman_corr': np.nan,
                'spearman_p': np.nan,
                'kendall_corr': np.nan,
                'kendall_p': np.nan
            }
    
    return correlations, len(merged_df), len(abnormal_users), len(filtered_df)

def save_results(correlations, original_count, excluded_count, remaining_count, 
                folder_info, output_dir):
    """ğŸ”¥ ä¿®æ”¹ç‰ˆï¼šä¿å­˜åˆ†æç»“æœï¼Œæ”¯æŒåŠ¨æ€ç‰¹å¾æ•°é‡"""
    result_dir = os.path.join(output_dir, folder_info['folder_name'])
    ensure_dir(result_dir)
    
    # ä¿å­˜è¯¦ç»†çš„ç›¸å…³æ€§ç»“æœåˆ°CSV
    results_data = []
    for feature, corr_data in correlations.items():
        results_data.append({
            'feature': feature,
            'spearman_correlation': corr_data['spearman_corr'],
            'spearman_p_value': corr_data['spearman_p'],
            'kendall_correlation': corr_data['kendall_corr'],
            'kendall_p_value': corr_data['kendall_p']
        })
    
    results_df = pd.DataFrame(results_data)
    csv_path = os.path.join(result_dir, 'correlation_results.csv')
    results_df.to_csv(csv_path, index=False)
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Šåˆ°TXT
    txt_path = os.path.join(result_dir, 'analysis_summary.txt')
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(f"====== {folder_info['description']} ç›¸å…³æ€§åˆ†æç»“æœ ======\n")
        f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ£€æµ‹æ–¹æ³•: {', '.join(folder_info['methods'])}\n")
        f.write(f"æ’é™¤æ¯”ä¾‹: {folder_info['exclude_pct']}%\n\n")
        
        f.write(f"=== æ•°æ®ç»Ÿè®¡ ===\n")
        f.write(f"åŸå§‹ç”¨æˆ·æ€»æ•°: {original_count}\n")
        f.write(f"æ’é™¤å¼‚å¸¸ç”¨æˆ·æ•°: {excluded_count}\n")
        f.write(f"å‰©ä½™æ­£å¸¸ç”¨æˆ·æ•°: {remaining_count}\n")
        f.write(f"æ’é™¤æ¯”ä¾‹: {excluded_count/original_count*100:.2f}%\n")
        f.write(f"ä¿ç•™æ¯”ä¾‹: {remaining_count/original_count*100:.2f}%\n\n")
        
        # ğŸ”¥ ä¿®æ”¹ï¼šåŠ¨æ€æ ‡é¢˜ï¼Œæ”¯æŒä»»æ„æ•°é‡çš„ç‰¹å¾
        f.write(f"=== ç½‘ç»œç‰¹å¾ä¸å½±å“åŠ›ç›¸å…³æ€§åˆ†æ (å…±{len(correlations)}ä¸ªç‰¹å¾) ===\n")
        f.write(f"{'ç‰¹å¾åç§°':<35} {'Spearmanç›¸å…³ç³»æ•°':<18} {'Spearman På€¼':<15} {'Kendallç›¸å…³ç³»æ•°':<17} {'Kendall På€¼':<15} {'æ˜¾è‘—æ€§'}\n")
        f.write("-" * 120 + "\n")
        
        for feature, corr_data in correlations.items():
            spearman_sig = "æ˜¾è‘—" if not pd.isna(corr_data['spearman_p']) and corr_data['spearman_p'] < 0.05 else "ä¸æ˜¾è‘—"
            kendall_sig = "æ˜¾è‘—" if not pd.isna(corr_data['kendall_p']) and corr_data['kendall_p'] < 0.05 else "ä¸æ˜¾è‘—"
            
            # ç»¼åˆæ˜¾è‘—æ€§åˆ¤æ–­
            overall_sig = "æ˜¾è‘—" if (spearman_sig == "æ˜¾è‘—" or kendall_sig == "æ˜¾è‘—") else "ä¸æ˜¾è‘—"
            
            f.write(f"{feature:<35} "
                   f"{corr_data['spearman_corr']:<18.4f} "
                   f"{corr_data['spearman_p']:<15.4f} "
                   f"{corr_data['kendall_corr']:<17.4f} "
                   f"{corr_data['kendall_p']:<15.4f} "
                   f"{overall_sig}\n")
        
        f.write(f"\n=== åˆ†æè¯´æ˜ ===\n")
        f.write(f"1. ä½¿ç”¨æ£€æµ‹æ–¹æ³•: {', '.join(folder_info['methods'])}\n")
        f.write(f"2. æ’é™¤æ¯”ä¾‹: {folder_info['exclude_pct']}%\n")
        f.write(f"3. è‡ªåŠ¨æ£€æµ‹åˆ° {len(correlations)} ä¸ªç½‘ç»œç‰¹å¾è¿›è¡Œåˆ†æ\n")
        f.write(f"4. Spearmanç›¸å…³ç³»æ•°è¡¡é‡å•è°ƒå…³ç³»ï¼ŒKendallç›¸å…³ç³»æ•°è¡¡é‡åºåˆ—ä¸€è‡´æ€§\n")
        f.write(f"5. På€¼<0.05è®¤ä¸ºç›¸å…³æ€§æ˜¾è‘—\n")
        f.write(f"6. ç›¸å…³ç³»æ•°ç»å¯¹å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºç›¸å…³æ€§è¶Šå¼º\n")
        f.write(f"7. å·²è‡ªåŠ¨æ’é™¤éåˆ†æå­—æ®µ: user_id, center_node, avg_popularity, is_celebrity\n")
    
    print(f"  - ç»“æœå·²ä¿å­˜åˆ°: {result_dir}")
    return csv_path, txt_path

def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    print(f"å¼€å§‹è‡ªé€‚åº”å¼‚å¸¸ç”¨æˆ·ç›¸å…³æ€§åˆ†æ...")
    print(f"åˆ†ææ—¶é—´: {start_time}")
    
    # ğŸ”¥ ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„æ•°æ®è·¯å¾„
    merged_data_path = 'C:/Tengfei/data/results/user_3855570307_metrics/merged_metrics_popularity.csv'
    
    if not os.path.exists(merged_data_path):
        print(f"é”™è¯¯: æœªæ‰¾åˆ°åˆå¹¶æ•°æ®æ–‡ä»¶ {merged_data_path}")
        print("è¯·å…ˆè¿è¡Œ create3.py ç”Ÿæˆ merged_metrics_popularity.csv")
        return
    
    print(f"æ­£åœ¨åŠ è½½åˆå¹¶æ•°æ®: {merged_data_path}")
    try:
        merged_df = pd.read_csv(merged_data_path)
        print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼ŒåŒ…å« {len(merged_df)} ä¸ªç”¨æˆ·")
    except Exception as e:
        print(f"åŠ è½½åˆå¹¶æ•°æ®å‡ºé”™: {e}")
        return
    
    # ğŸ”¥ ä¿®æ”¹ï¼šæ£€æŸ¥å¿…è¦çš„åˆ—ï¼ˆç§»é™¤ç¡¬ç¼–ç çš„ç½‘ç»œç‰¹å¾æ£€æŸ¥ï¼‰
    required_columns = ['user_id', 'avg_popularity']
    
    missing_columns = [col for col in required_columns if col not in merged_df.columns]
    if missing_columns:
        print(f"é”™è¯¯: åˆå¹¶æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
        return
    
    print(f"âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
    print(f"ğŸ“Š æ•°æ®åŒ…å«åˆ—: {list(merged_df.columns)}")
    
    # è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹
    print(f"\n{'='*60}")
    print(f"è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹...")
    abnormal_folders = detect_abnormal_user_folders()
    
    if not abnormal_folders:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•å¼‚å¸¸ç”¨æˆ·æ–‡ä»¶å¤¹")
        print("è¯·å…ˆè¿è¡Œ pick_out_abnormal_users.py ç”Ÿæˆå¼‚å¸¸ç”¨æˆ·æ•°æ®")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'results/correlation_result'
    ensure_dir(output_dir)
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹åˆ†æ {len(abnormal_folders)} ç§é…ç½®ä¸‹çš„ç›¸å…³æ€§...")
    
    all_results = {}
    
    # åˆ†ææ¯ä¸ªé…ç½®
    for folder_name in abnormal_folders:
        print(f"\n{'='*60}")
        
        # è§£ææ–‡ä»¶å¤¹ä¿¡æ¯
        folder_info = parse_folder_info(folder_name)
        print(f"åˆ†æé…ç½®: {folder_info['description']}")
        print(f"æ–‡ä»¶å¤¹: {folder_name}")
        print(f"{'='*60}")
        
        # åŠ è½½å¼‚å¸¸ç”¨æˆ·åˆ—è¡¨
        abnormal_users = load_abnormal_users_from_folder(folder_name)
        
        # è®¡ç®—ç›¸å…³æ€§
        print(f"  - å¼€å§‹è®¡ç®—ç›¸å…³æ€§...")
        correlations, original_count, excluded_count, remaining_count = calculate_correlations_without_abnormal(
            merged_df, abnormal_users, folder_info)
        
        # ä¿å­˜ç»“æœ
        csv_path, txt_path = save_results(correlations, original_count, excluded_count, 
                                        remaining_count, folder_info, output_dir)
        
        # å­˜å‚¨ç»“æœç”¨äºåç»­æ¯”è¾ƒ
        all_results[folder_name] = {
            'folder_info': folder_info,
            'correlations': correlations,
            'original_count': original_count,
            'excluded_count': excluded_count,
            'remaining_count': remaining_count
        }
    
    # ğŸ”¥ ä¿®æ”¹ï¼šç”ŸæˆåŠ¨æ€å¯¹æ¯”æ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ç”Ÿæˆå¯¹æ¯”æ±‡æ€»æŠ¥å‘Š...")
    
    summary_path = os.path.join(output_dir, 'comprehensive_comparison_summary.txt')
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("====== å¼‚å¸¸ç”¨æˆ·ç­›é€‰ç›¸å…³æ€§åˆ†æç»¼åˆå¯¹æ¯”æ±‡æ€» ======\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # é…ç½®æ¦‚è§ˆ
        f.write("=== åˆ†æé…ç½®æ¦‚è§ˆ ===\n")
        for i, folder_name in enumerate(abnormal_folders, 1):
            folder_info = all_results[folder_name]['folder_info']
            f.write(f"{i}. {folder_info['short_name']}: {folder_info['description']}\n")
        f.write("\n")
        
        # æ•°æ®é‡å¯¹æ¯”
        f.write("=== æ•°æ®é‡å¯¹æ¯” ===\n")
        f.write(f"{'é…ç½®':<15} {'åŸå§‹ç”¨æˆ·æ•°':<12} {'æ’é™¤ç”¨æˆ·æ•°':<12} {'å‰©ä½™ç”¨æˆ·æ•°':<12} {'æ’é™¤æ¯”ä¾‹':<10} {'ä¿ç•™æ¯”ä¾‹'}\n")
        f.write("-" * 75 + "\n")
        for folder_name in abnormal_folders:
            result = all_results[folder_name]
            folder_info = result['folder_info']
            exclude_pct = result['excluded_count'] / result['original_count'] * 100
            remain_pct = result['remaining_count'] / result['original_count'] * 100
            
            f.write(f"{folder_info['short_name']:<15} {result['original_count']:<12} {result['excluded_count']:<12} "
                   f"{result['remaining_count']:<12} {exclude_pct:<10.2f}% {remain_pct:.2f}%\n")
        
        # ğŸ”¥ ä¿®æ”¹ï¼šåŠ¨æ€è·å–æ‰€æœ‰ç‰¹å¾è¿›è¡Œå¯¹æ¯”
        all_features = set()
        for folder_name in abnormal_folders:
            all_features.update(all_results[folder_name]['correlations'].keys())
        
        features = sorted(list(all_features))  # æ’åºä¿è¯ä¸€è‡´æ€§
        
        f.write(f"\n=== Spearmanç›¸å…³ç³»æ•°å¯¹æ¯” (å…±{len(features)}ä¸ªç‰¹å¾) ===\n")
        # æ„å»ºåˆ—æ ‡é¢˜
        header = f"{'ç‰¹å¾':<35} "
        for folder_name in abnormal_folders:
            folder_info = all_results[folder_name]['folder_info']
            header += f"{folder_info['short_name']:<15} "
        f.write(header + "\n")
        f.write("-" * (35 + 15 * len(abnormal_folders)) + "\n")
        
        for feature in features:
            line = f"{feature:<35} "
            for folder_name in abnormal_folders:
                if feature in all_results[folder_name]['correlations']:
                    corr = all_results[folder_name]['correlations'][feature]['spearman_corr']
                    if pd.isna(corr):
                        line += f"{'N/A':<15}"
                    else:
                        line += f"{corr:<15.4f}"
                else:
                    line += f"{'N/A':<15}"
            f.write(line + "\n")
        
        f.write(f"\n=== Kendallç›¸å…³ç³»æ•°å¯¹æ¯” (å…±{len(features)}ä¸ªç‰¹å¾) ===\n")
        # æ„å»ºåˆ—æ ‡é¢˜
        header = f"{'ç‰¹å¾':<35} "
        for folder_name in abnormal_folders:
            folder_info = all_results[folder_name]['folder_info']
            header += f"{folder_info['short_name']:<15} "
        f.write(header + "\n")
        f.write("-" * (35 + 15 * len(abnormal_folders)) + "\n")
        
        for feature in features:
            line = f"{feature:<35} "
            for folder_name in abnormal_folders:
                if feature in all_results[folder_name]['correlations']:
                    corr = all_results[folder_name]['correlations'][feature]['kendall_corr']
                    if pd.isna(corr):
                        line += f"{'N/A':<15}"
                    else:
                        line += f"{corr:<15.4f}"
                else:
                    line += f"{'N/A':<15}"
            f.write(line + "\n")
        
        f.write(f"\n=== åˆ†æè¯´æ˜ ===\n")
        f.write(f"1. è‡ªåŠ¨æ£€æµ‹å¹¶åˆ†æäº† {len(features)} ä¸ªç½‘ç»œç‰¹å¾\n")
        f.write(f"2. å·²æ’é™¤éåˆ†æå­—æ®µ: user_id, center_node, avg_popularity, is_celebrity\n")
        f.write(f"3. Original: åŸå§‹ç½‘ç»œï¼Œæœªæ’é™¤ä»»ä½•ç”¨æˆ·\n")
        for folder_name in abnormal_folders:
            folder_info = all_results[folder_name]['folder_info']
            if folder_info['exclude_pct'] > 0:
                f.write(f"4. {folder_info['short_name']}: æ’é™¤å‰{folder_info['exclude_pct']}%å¼‚å¸¸ç”¨æˆ·\n")
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    print(f"\n{'='*60}")
    print(f"åˆ†æå®Œæˆï¼")
    print(f"æ€»è€—æ—¶: {duration}")
    print(f"å¤„ç†äº† {len(abnormal_folders)} ç§é…ç½®")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ç»¼åˆå¯¹æ¯”æ±‡æ€»æŠ¥å‘Š: {summary_path}")
    
    # æ‰“å°ç®€è¦ç»“æœ
    print(f"\n=== åˆ†æç»“æœé¢„è§ˆ ===")
    for folder_name in abnormal_folders:
        result = all_results[folder_name]
        folder_info = result['folder_info']
        print(f"{folder_info['short_name']}: æ’é™¤{result['excluded_count']}ç”¨æˆ·, å‰©ä½™{result['remaining_count']}ç”¨æˆ·")

if __name__ == "__main__":
    main()