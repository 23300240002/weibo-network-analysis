import os
import json
import time
import random
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from datetime import datetime
import re

# é…ç½®å‚æ•°
BASE_OUTPUT_DIR = 'C:/Tengfei/data/data/domain_network3'
PROGRESS_FILE_TEMPLATE = 'C:/Tengfei/data/crawler/fetch/progress_fans3_{}.json'

# çˆ¬å–é™åˆ¶å‚æ•°
MAX_FANS_PER_PAGE = 10
MAX_PAGES_LIMIT = 20

# é«˜ç²‰ä¸ç”¨æˆ·åˆ¤æ–­æ ‡å‡†
HIGH_FANS_ACTUAL_THRESHOLD = 100
HIGH_FANS_DISPLAY_THRESHOLD = 400

# é€Ÿåº¦å‚æ•°
SLEEP_MIN = 0.4
SLEEP_MAX = 0.6
BATCH_INTERVAL_MIN = 0.5
BATCH_INTERVAL_MAX = 1.0
CONSECUTIVE_EMPTY_THRESHOLD = 2

# åçˆ¬æ£€æµ‹å‚æ•°
CONSECUTIVE_ZERO_FANS_THRESHOLD = 3

# æµè¡Œåº¦è®¡ç®—å‚æ•°
MAX_POSTS_FOR_POPULARITY = 10

# ===== é…ç½®éƒ¨åˆ†ï¼šä¿®æ”¹è¿™é‡Œçš„ç”¨æˆ·ID =====
TARGET_USER_ID = "3855570307"
# =========================================

# å…¨å±€å˜é‡
processed_users = set()
users_data = {}
edges_data = []
edges_set = set()
node_categories = {"A": set(), "B": set(), "C": set()}  # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤Dç±»
high_fans_users = set()
seed_user_id = None
consecutive_zero_fans_count = 0
popularity_data = {}

class WeiboFansCrawler:
    def __init__(self, cookie_path='C:/Tengfei/data/crawler/crawler_for_weibo_fans-master/cookie.json'):
        self.driver = None
        self.cookie_path = cookie_path
        
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨"""
        print("æ­£åœ¨è®¾ç½®æµè§ˆå™¨...")
        
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--disable-javascript')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-extensions')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.set_page_load_timeout(8)
            self.driver.implicitly_wait(2)
            print("æµè§ˆå™¨è®¾ç½®æˆåŠŸ")
            return True
        except Exception as e:
            print(f"æµè§ˆå™¨è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def load_cookies(self):
        """åŠ è½½cookie"""
        if not os.path.exists(self.cookie_path):
            print(f"âŒ æœªæ‰¾åˆ°cookieæ–‡ä»¶: {self.cookie_path}")
            return False
        
        try:
            with open(self.cookie_path, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            
            self.driver.get('https://weibo.cn')
            time.sleep(2)
            
            for cookie in cookies:
                try:
                    self.driver.add_cookie(cookie)
                except Exception as e:
                    pass
            
            self.driver.refresh()
            time.sleep(2)
            print("CookieåŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ CookieåŠ è½½å¤±è´¥: {e}")
            return False

    def test_login_status(self):
        """æµ‹è¯•ç™»å½•çŠ¶æ€"""
        try:
            self.driver.get('https://weibo.cn')
            time.sleep(2)
            page_source = self.driver.page_source
            
            if 'ç™»å½•' in page_source and 'å¯†ç ' in page_source:
                print("âŒ éœ€è¦é‡æ–°è·å–Cookie")
                return False
            else:
                print("âœ… ç™»å½•çŠ¶æ€æ­£å¸¸")
                return True
                
        except Exception as e:
            print(f"âŒ ç™»å½•çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    def get_user_fans_count(self, user_id):
        """è·å–ç”¨æˆ·åœ¨å¾®åšæ˜¾ç¤ºçš„çœŸå®ç²‰ä¸æ•°ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºé«˜ç²‰ä¸ç”¨æˆ·ï¼‰"""
        try:
            profile_url = f'https://weibo.cn/u/{user_id}'
            self.driver.get(profile_url)
            time.sleep(random.uniform(0.2, 0.8))
            
            page_source = self.driver.page_source
            
            patterns = [
                (r'ç²‰ä¸\[(\d+\.?\d*)[ä¸‡]?\]', lambda x: int(float(x[:-1]) * 10000) if x.endswith('ä¸‡') else int(float(x))),
                (r'ç²‰ä¸\((\d+\.?\d*)[ä¸‡]?\)', lambda x: int(float(x[:-1]) * 10000) if x.endswith('ä¸‡') else int(float(x))),
                (r'(\d+\.?\d*)[ä¸‡]?ç²‰ä¸', lambda x: int(float(x[:-1]) * 10000) if x.endswith('ä¸‡') else int(float(x))),
            ]
            
            for pattern, converter in patterns:
                match = re.search(pattern, page_source, re.IGNORECASE)
                if match:
                    num_str = match.group(1)
                    full_match = match.group(0)
                    
                    if 'ä¸‡' in full_match:
                        fans_count = int(float(num_str) * 10000)
                    else:
                        fans_count = int(float(num_str))
                    
                    return fans_count
            
            return 0
            
        except Exception as e:
            return 0
    
    def crawl_user_fans(self, user_id):
        """çˆ¬å–ç”¨æˆ·çš„ç²‰ä¸åˆ—è¡¨ï¼ˆå—å¾®åš20é¡µé™åˆ¶ï¼‰"""
        print(f"  å¼€å§‹çˆ¬å–ç”¨æˆ· {user_id} çš„ç²‰ä¸...")
        
        try:
            fans_url = f'https://weibo.cn/{user_id}/fans'
            self.driver.get(fans_url)
            time.sleep(0.5)
            
            page_source = self.driver.page_source
            if 'ç”¨æˆ·ä¸å­˜åœ¨' in page_source or 'ç™»å½•' in page_source:
                print(f"  âŒ ç”¨æˆ· {user_id} ä¸å­˜åœ¨æˆ–éœ€è¦ç™»å½•")
                return []
            
            fans_data = []
            consecutive_empty_pages = 0
            
            for page in range(1, MAX_PAGES_LIMIT + 1):
                if page > 1:
                    try:
                        next_page_url = f'https://weibo.cn/{user_id}/fans?page={page}'
                        self.driver.get(next_page_url)
                        time.sleep(random.uniform(0.7, 1.0))
                    except Exception as e:
                        break
                
                try:
                    fan_elements = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/u/')]")
                    
                    page_fans = []
                    processed_ids = set()
                    
                    for element in fan_elements:
                        try:
                            fan_href = element.get_attribute('href')
                            fan_name = element.text.strip()
                            
                            if fan_href and '/u/' in fan_href:
                                fan_id = fan_href.split('/u/')[-1].split('?')[0].split('/')[0]
                                
                                if fan_id.isdigit() and fan_id not in processed_ids and fan_name:
                                    page_fans.append({
                                        'id': fan_id,
                                        'screen_name': fan_name
                                    })
                                    processed_ids.add(fan_id)
                        except Exception as e:
                            continue
                    
                    if len(page_fans) == 0:
                        consecutive_empty_pages += 1
                        if consecutive_empty_pages >= CONSECUTIVE_EMPTY_THRESHOLD:
                            break
                    else:
                        consecutive_empty_pages = 0
                        fans_data.extend(page_fans)
                        
                except Exception as e:
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= CONSECUTIVE_EMPTY_THRESHOLD:
                        break
                
                time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))
                
                if page % 10 == 0 and page > 0:
                    extra_wait = random.uniform(0.6, 1.0)
                    print(f"    å·²çˆ¬å– {page} é¡µï¼Œé¢å¤–ç­‰å¾… {extra_wait:.1f} ç§’é˜²åçˆ¬...")
                    time.sleep(extra_wait)
            
            actual_fans_count = len(fans_data)
            print(f"  âœ… ç”¨æˆ· {user_id} å®é™…çˆ¬å–åˆ° {actual_fans_count} ä¸ªç²‰ä¸")
            
            return fans_data
            
        except Exception as e:
            print(f"  âŒ çˆ¬å–ç”¨æˆ· {user_id} ç²‰ä¸æ—¶å‡ºé”™: {e}")
            return []
    
    # ğŸ”¥ ä¿®å¤ç‰ˆæµè¡Œåº¦è®¡ç®—ç›¸å…³æ–¹æ³•
    def extract_interactions_from_html(self, html_text):
        """ä»HTMLæ–‡æœ¬ä¸­æå–è½¬èµè¯„æ•°æ®"""
        interactions = {'reposts': 0, 'likes': 0, 'comments': 0}
        
        patterns = {
            'likes': [r'èµ\[(\d+)\]'],
            'reposts': [r'è½¬å‘\[(\d+)\]'],
            'comments': [r'è¯„è®º\[(\d+)\]']
        }
        
        for interaction_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, html_text)
                if matches:
                    try:
                        # å–æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆç”¨æˆ·è‡ªå·±çš„æ•°æ®ï¼‰
                        num = int(matches[-1])
                        interactions[interaction_type] = num
                    except:
                        continue
        
        return interactions
    
    def extract_time_from_html(self, html_text):
        """ä»HTMLæ–‡æœ¬ä¸­æå–æ—¶é—´"""
        time_patterns = [
            r'(\d{2}æœˆ\d{2}æ—¥ \d{2}:\d{2})',
            r'(\d+åˆ†é’Ÿå‰)',
            r'(\d+å°æ—¶å‰)',
            r'(\d+å¤©å‰)',
            r'(æ˜¨å¤© \d{1,2}:\d{2})',
            r'(ä»Šå¤© \d{1,2}:\d{2})',
        ]
        
        for pattern in time_patterns:
            matches = re.findall(pattern, html_text)
            if matches:
                return matches[-1]  # å–æœ€åä¸€ä¸ªåŒ¹é…
        
        return "æ—¶é—´æœªæ‰¾åˆ°"
    
    def process_single_weibo_div(self, weibo_div_element):
        """ğŸ”¥ ä¿®æ­£ç‰ˆï¼šä½¿ç”¨æœ€åä¸€ä¸ªdivæå–ç”¨æˆ·è‡ªå·±çš„è½¬èµè¯„æ•°æ®"""
        try:
            # è·å–å¾®åšdivçš„HTML
            div_html = weibo_div_element.get_attribute('outerHTML')
            
            # æŸ¥æ‰¾å¾®åšdivå†…çš„å­divå…ƒç´ 
            child_divs = weibo_div_element.find_elements(By.XPATH, "./div")
            
            # æå–å¾®åšå†…å®¹
            content = "å†…å®¹æœªæå–"
            try:
                ctt_element = weibo_div_element.find_element(By.CLASS_NAME, "ctt")
                content = ctt_element.text.strip()
                content = content[:100] + ('...' if len(content) > 100 else '')
            except:
                pass
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºè½¬å‘å¾®åš
            is_repost = 'è½¬å‘äº†' in div_html
            
            interactions = {'reposts': 0, 'likes': 0, 'comments': 0}
            post_time = "æ—¶é—´æœªæ‰¾åˆ°"
            
            # ğŸ”¥ å…³é”®ä¿®æ­£ï¼šå§‹ç»ˆä½¿ç”¨æœ€åä¸€ä¸ªdivï¼ˆæ— è®ºè½¬å‘è¿˜æ˜¯åŸåˆ›ï¼‰
            if len(child_divs) > 0:
                last_div = child_divs[-1]  # å–æœ€åä¸€ä¸ªdiv
                last_div_html = last_div.get_attribute('outerHTML')
                
                # ä»æœ€åä¸€ä¸ªdivæå–ç”¨æˆ·è‡ªå·±çš„è½¬èµè¯„æ•°æ®
                interactions = self.extract_interactions_from_html(last_div_html)
                
                # ä»æœ€åä¸€ä¸ªdivæå–æ—¶é—´
                post_time = self.extract_time_from_html(last_div_html)
            else:
                # æ²¡æœ‰å­divï¼Œä½¿ç”¨æ•´ä¸ªdiv
                interactions = self.extract_interactions_from_html(div_html)
                post_time = self.extract_time_from_html(div_html)
            
            return {
                'content': content,
                'time': post_time,
                'interactions': interactions,
                'total_interactions': sum(interactions.values()),
                'is_repost': is_repost
            }
            
        except Exception as e:
            return None
    
    def calculate_user_popularity(self, user_id, max_posts=MAX_POSTS_FOR_POPULARITY):
        """ğŸ”¥ ä¿®æ­£ç‰ˆï¼šè®¡ç®—ç”¨æˆ·æµè¡Œåº¦ - åŸºäºæœ€æ–°max_postsæ¡å¾®åš"""
        try:
            profile_url = f'https://weibo.cn/u/{user_id}'
            self.driver.get(profile_url)
            time.sleep(2)
            
            # ä½¿ç”¨Seleniumç›´æ¥æŸ¥æ‰¾å¾®åšdivå…ƒç´ 
            weibo_divs = self.driver.find_elements(By.XPATH, "//div[@class='c' and contains(@id, 'M_')]")
            
            if not weibo_divs:
                return 0.0
            
            posts_data = []
            
            for i, weibo_div in enumerate(weibo_divs):
                if len(posts_data) >= max_posts:
                    break
                
                # ğŸ”¥ ä½¿ç”¨ä¿®æ­£ç‰ˆå¤„ç†æ–¹æ³•
                post_data = self.process_single_weibo_div(weibo_div)
                
                if post_data and post_data['content'] != "å†…å®¹æœªæå–":
                    posts_data.append(post_data)
            
            if not posts_data:
                return 0.0
            
            # è®¡ç®—å¹³å‡æµè¡Œåº¦
            total_interactions = 0
            valid_posts = len(posts_data)
            
            for post in posts_data:
                interactions = post['interactions']
                post_total = interactions['likes'] + interactions['reposts'] + interactions['comments']
                total_interactions += post_total
            
            avg_popularity = total_interactions / valid_posts if valid_posts > 0 else 0.0
            
            return avg_popularity
                
        except Exception as e:
            print(f"  âš ï¸ è®¡ç®—ç”¨æˆ· {user_id} æµè¡Œåº¦å¤±è´¥: {e}")
            return 0.0
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.driver:
            self.driver.quit()

def ensure_dir(directory):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    if not os.path.exists(directory):
        os.makedirs(directory)

def save_progress(seed_user_id):
    """ä¿å­˜çˆ¬å–è¿›åº¦"""
    progress_file = PROGRESS_FILE_TEMPLATE.format(seed_user_id)
    ensure_dir(os.path.dirname(progress_file))
    
    progress_data = {
        "users": users_data,
        "edges": edges_data,
        "processed": list(processed_users),
        "categories": {k: list(v) for k, v in node_categories.items()},
        "high_fans_users": list(high_fans_users),
        "consecutive_zero_fans_count": consecutive_zero_fans_count,
        "popularity": popularity_data,
        "save_timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "total_users": len(users_data),
        "total_edges": len(edges_data)
    }
    
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    current_size = get_current_network_size()
    print(f"âœ… è¿›åº¦å·²ä¿å­˜: ç”¨æˆ· {len(users_data)} ä¸ª, è¾¹ {len(edges_data)} æ¡, ç½‘ç»œè§„æ¨¡ {current_size}")

def load_progress(seed_user_id):
    """åŠ è½½çˆ¬å–è¿›åº¦"""
    global processed_users, users_data, edges_data, edges_set, node_categories, high_fans_users, consecutive_zero_fans_count, popularity_data
    # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤Dç±»ç›¸å…³å…¨å±€å˜é‡
    
    progress_file = PROGRESS_FILE_TEMPLATE.format(seed_user_id)
    
    if not os.path.exists(progress_file):
        print("æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹çˆ¬å–")
        reset_global_data()
        return
    
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        processed_users = set(data.get("processed", []))
        users_data = data.get("users", {})
        edges_data = data.get("edges", [])
        high_fans_users = set(data.get("high_fans_users", []))
        # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤Dç±»æ•°æ®åŠ è½½
        # reserved_d_users = set(data.get("reserved_d_users", []))
        # d_users_quota = data.get("d_users_quota", 0)
        consecutive_zero_fans_count = data.get("consecutive_zero_fans_count", 0)
        popularity_data = data.get("popularity", {})
        
        # é‡å»ºè¾¹é›†åˆç”¨äºå¿«é€Ÿå»é‡
        edges_set = set(tuple(edge) if isinstance(edge, list) else edge for edge in edges_data)
        
        if "categories" in data:
            for k, v in data["categories"].items():
                if k in node_categories:  # ğŸ”¥ ä¿®æ”¹ï¼šåªåŠ è½½ABCç±»
                    node_categories[k] = set(v)
        
        save_time = data.get("save_timestamp", "æœªçŸ¥")
        current_size = get_current_network_size()
        print(f"âœ… å·²åŠ è½½è¿›åº¦ (ä¿å­˜äº {save_time}): ç”¨æˆ· {len(users_data)} ä¸ª, è¾¹ {len(edges_data)} æ¡, ç½‘ç»œè§„æ¨¡ {current_size}")
        if consecutive_zero_fans_count > 0:
            print(f"âš ï¸ åçˆ¬æ£€æµ‹çŠ¶æ€: å·²è¿ç»­é‡åˆ° {consecutive_zero_fans_count} ä¸ª0ç²‰ä¸ç”¨æˆ·")
        if popularity_data:
            print(f"âœ… å·²åŠ è½½ {len(popularity_data)} ä¸ªç”¨æˆ·çš„æµè¡Œåº¦æ•°æ®")
        
    except Exception as e:
        print(f"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å‡ºé”™: {e}ï¼Œä»å¤´å¼€å§‹")
        reset_global_data()

def reset_global_data():
    """é‡ç½®å…¨å±€æ•°æ®"""
    global processed_users, users_data, edges_data, edges_set, node_categories, high_fans_users, consecutive_zero_fans_count, popularity_data
    # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤Dç±»ç›¸å…³å˜é‡é‡ç½®
    processed_users = set()
    users_data = {}
    edges_data = []
    edges_set = set()
    node_categories = {"A": set(), "B": set(), "C": set()}  # åªä¿ç•™ABCç±»
    high_fans_users = set()
    # reserved_d_users = set()
    # d_users_quota = 0
    consecutive_zero_fans_count = 0
    popularity_data = {}

def check_anti_crawl_status(fans_count, user_id):
    """æ£€æŸ¥åçˆ¬çŠ¶æ€"""
    global consecutive_zero_fans_count
    
    if fans_count == 0:
        consecutive_zero_fans_count += 1
        print(f"  âš ï¸ è¿ç»­0ç²‰ä¸ç”¨æˆ·è®¡æ•°: {consecutive_zero_fans_count}/{CONSECUTIVE_ZERO_FANS_THRESHOLD}")
        
        if consecutive_zero_fans_count >= CONSECUTIVE_ZERO_FANS_THRESHOLD:
            print(f"\nğŸš¨ æ£€æµ‹åˆ°åçˆ¬æœºåˆ¶ï¼")
            print(f"ğŸš¨ è¿ç»­ {consecutive_zero_fans_count} ä¸ªç”¨æˆ·ç²‰ä¸æ•°ä¸º0ï¼Œå¯èƒ½è¢«å¾®åšåçˆ¬")
            print(f"ğŸš¨ ä¸ºäº†å®‰å…¨èµ·è§ï¼Œç¨‹åºå°†è‡ªåŠ¨ç»ˆæ­¢")
            print(f"ğŸš¨ æœ€åå¤„ç†çš„ç”¨æˆ·: {user_id}")
            print(f"ğŸš¨ è¯·ç¨åé‡æ–°å¯åŠ¨ç¨‹åºï¼Œæˆ–æ›´æ¢Cookieåç»§ç»­")
            
            # ä¿å­˜å½“å‰è¿›åº¦
            save_progress(seed_user_id)
            
            # æŠ›å‡ºå¼‚å¸¸ä»¥ç»ˆæ­¢ç¨‹åº
            raise Exception(f"ANTI_CRAWL_DETECTED: è¿ç»­{consecutive_zero_fans_count}ä¸ªç”¨æˆ·0ç²‰ä¸")
    else:
        # é‡ç½®è®¡æ•°å™¨
        consecutive_zero_fans_count = 0

def process_user_fans(crawler, user_id, category):
    """ğŸ”¥ ä¿®æ”¹ç‰ˆï¼šå¤„ç†å•ä¸ªç”¨æˆ·çš„ç²‰ä¸åˆ—è¡¨ - ç§»é™¤Dç±»é€»è¾‘"""
    global processed_users, users_data, edges_data, edges_set, node_categories, popularity_data
    
    if user_id in processed_users:
        return 0, set()
    
    processed_users.add(user_id)
    
    if category and category in node_categories:
        node_categories[category].add(user_id)
    
    # ğŸ”¥ è®¡ç®—ç”¨æˆ·æµè¡Œåº¦
    if user_id not in popularity_data:
        print(f"  ğŸ¯ è®¡ç®—ç”¨æˆ· {user_id} çš„æµè¡Œåº¦...")
        avg_popularity = crawler.calculate_user_popularity(user_id, MAX_POSTS_FOR_POPULARITY)
        popularity_data[user_id] = avg_popularity
        print(f"  âœ… ç”¨æˆ· {user_id} å¹³å‡æµè¡Œåº¦: {avg_popularity:.2f}")
    
    # çˆ¬å–ç²‰ä¸åˆ—è¡¨
    fans_users = crawler.crawl_user_fans(user_id)
    
    # åçˆ¬æ£€æµ‹
    fans_count = len(fans_users) if fans_users else 0
    check_anti_crawl_status(fans_count, user_id)
    
    if not fans_users:
        return 0, set()
    
    # è·å–å¾®åšæ˜¾ç¤ºçš„ç²‰ä¸æ•°ï¼Œç”¨äºé«˜ç²‰ä¸ç”¨æˆ·åˆ¤æ–­
    true_fans_count = crawler.get_user_fans_count(user_id)
    actual_fans_count = len(fans_users)
    
    # é«˜ç²‰ä¸ç”¨æˆ·åˆ¤æ–­é€»è¾‘
    if actual_fans_count > HIGH_FANS_ACTUAL_THRESHOLD and true_fans_count > HIGH_FANS_DISPLAY_THRESHOLD:
        high_fans_users.add(user_id)
        print(f"  âš ï¸ ç”¨æˆ· {user_id} æ»¡è¶³é«˜ç²‰ä¸æ ‡å‡†ï¼šå®é™…çˆ¬å–{actual_fans_count}>100 ä¸” å¾®åšæ˜¾ç¤º{true_fans_count:,}>400")
    
    # ğŸ”¥ ä¿®æ”¹ï¼šç¡®ä¿å½“å‰å¤„ç†ç”¨æˆ·çš„å®Œæ•´ä¿¡æ¯è¢«æ­£ç¡®æ›´æ–°ï¼Œå¹¶æ·»åŠ ç±»åˆ«ä¿¡æ¯
    existing_screen_name = users_data.get(user_id, {}).get('screen_name', f'ç”¨æˆ·{user_id}')
    
    users_data[user_id] = {
        'screen_name': existing_screen_name,
        'display_fans_count': true_fans_count,
        'actual_fans_count': actual_fans_count,
        'avg_popularity': popularity_data.get(user_id, 0.0),
        'category': category  # ğŸ”¥ æ–°å¢ï¼šç”¨æˆ·ç±»åˆ«ä¿¡æ¯
    }
    
    valid_edges_added = 0
    new_users_discovered = set()
    ca_edges_added = 0
    cb_edges_added = 0
    cc_edges_added = 0
    
    for fan in fans_users:
        fan_id = str(fan.get('id'))
        
        # ğŸ”¥ ä¿®æ”¹ï¼šè¾¹çš„æ·»åŠ é€»è¾‘ - ç§»é™¤Dç±»ç›¸å…³é€»è¾‘
        edge = (user_id, fan_id)  # user_idï¼ˆåšä¸»ï¼‰â†’ fan_idï¼ˆç²‰ä¸ï¼‰
        should_add_edge = False
        
        if category == "A":
            # Aç±»ç”¨æˆ·ï¼šæ·»åŠ æ‰€æœ‰è¾¹ï¼ˆè¿™äº›å°†æˆä¸ºAâ†’Bè¾¹ï¼‰
            should_add_edge = True
            # å°†æ–°å‘ç°çš„ç²‰ä¸æ ‡è®°ä¸ºBç±»
            if fan_id not in node_categories["A"]:
                node_categories["B"].add(fan_id)
                # ğŸ”¥ ä¿®æ”¹ï¼šåªä¸ºABCç±»ç”¨æˆ·åˆ›å»ºç”¨æˆ·æ•°æ®
                if fan_id not in users_data:
                    users_data[fan_id] = {
                        'screen_name': fan.get('screen_name', ''),
                        'display_fans_count': 0,
                        'actual_fans_count': 0,
                        'avg_popularity': 0.0,
                        'category': 'B'  # ğŸ”¥ æ–°å¢ï¼šç±»åˆ«ä¿¡æ¯
                    }
        elif category == "B":
            # Bç±»ç”¨æˆ·ï¼šæ·»åŠ æ‰€æœ‰è¾¹ï¼Œæ–°å‘ç°çš„ç”¨æˆ·æˆä¸ºCç±»
            should_add_edge = True
            # å°†æ–°å‘ç°çš„ç²‰ä¸æ ‡è®°ä¸ºCç±»ï¼ˆå¦‚æœä¸æ˜¯Aæˆ–Bç±»ï¼‰
            if fan_id not in node_categories["A"] and fan_id not in node_categories["B"]:
                node_categories["C"].add(fan_id)
                new_users_discovered.add(fan_id)
                # ğŸ”¥ ä¿®æ”¹ï¼šåªä¸ºABCç±»ç”¨æˆ·åˆ›å»ºç”¨æˆ·æ•°æ®
                if fan_id not in users_data:
                    users_data[fan_id] = {
                        'screen_name': fan.get('screen_name', ''),
                        'display_fans_count': 0,
                        'actual_fans_count': 0,
                        'avg_popularity': 0.0,
                        'category': 'C'  # ğŸ”¥ æ–°å¢ï¼šç±»åˆ«ä¿¡æ¯
                    }
        elif category == "C":
            # ğŸ”¥ ä¿®æ”¹ï¼šCç±»ç”¨æˆ·å¤„ç†é€»è¾‘ - ç§»é™¤Dç±»ç›¸å…³é€»è¾‘
            if fan_id in node_categories["A"]:
                # Câ†’Aè¾¹ï¼šç›´æ¥æ·»åŠ 
                should_add_edge = True
                ca_edges_added += 1
            elif fan_id in node_categories["B"]:
                # Câ†’Bè¾¹ï¼šç›´æ¥æ·»åŠ 
                should_add_edge = True
                cb_edges_added += 1
            elif fan_id in node_categories["C"]:
                # Câ†’Cè¾¹ï¼šç›´æ¥æ·»åŠ 
                should_add_edge = True
                cc_edges_added += 1
            # ğŸ”¥ ä¿®æ”¹ï¼šå®Œå…¨ç§»é™¤Câ†’Dè¾¹çš„é€»è¾‘
            # å¯¹äºæŒ‡å‘ä¸åœ¨ABCç±»ä¸­çš„ç²‰ä¸ï¼Œç›´æ¥å¿½ç•¥ï¼Œä¸æ·»åŠ è¾¹ï¼Œä¹Ÿä¸åˆ›å»ºç”¨æˆ·æ•°æ®
        
        # ä½¿ç”¨é›†åˆè¿›è¡Œå¿«é€Ÿè¾¹å»é‡
        if should_add_edge and edge not in edges_set:
            edges_data.append(edge)
            edges_set.add(edge)
            valid_edges_added += 1
    
    total_fans_found = len(fans_users)
    
    if category == "C":
        print(f"    Cç±»ç”¨æˆ· {user_id}: çˆ¬å– {total_fans_found} ä¸ªç²‰ä¸ï¼ˆå¾®åšæ˜¾ç¤º: {true_fans_count:,}ï¼‰ï¼Œæœ‰æ•ˆè¾¹ {valid_edges_added} æ¡ï¼Œæµè¡Œåº¦ {popularity_data.get(user_id, 0.0):.2f}")
        if ca_edges_added > 0 or cb_edges_added > 0 or cc_edges_added > 0:
            print(f"      â””â”€ Câ†’A: {ca_edges_added}, Câ†’B: {cb_edges_added}, Câ†’C: {cc_edges_added}")
    else:
        print(f"    {category}ç±»ç”¨æˆ· {user_id}: çˆ¬å– {total_fans_found} ä¸ªç²‰ä¸ï¼ˆå¾®åšæ˜¾ç¤º: {true_fans_count:,}ï¼‰ï¼Œæœ‰æ•ˆè¾¹ {valid_edges_added} æ¡ï¼Œæ–°ç”¨æˆ· {len(new_users_discovered)} ä¸ªï¼Œæµè¡Œåº¦ {popularity_data.get(user_id, 0.0):.2f}")
    
    return total_fans_found, new_users_discovered

def get_current_network_size():
    """è·å–å½“å‰ç½‘ç»œæ€»äººæ•°"""
    return len(users_data)

def print_network_status():
    """æ‰“å°å½“å‰ç½‘ç»œçŠ¶æ€"""
    current_size = get_current_network_size()
    a_count = len(node_categories["A"])
    b_count = len(node_categories["B"])
    c_count = len(node_categories["C"])
    
    # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤Dç±»æ˜¾ç¤º
    print(f"ğŸ“Š ç½‘ç»œçŠ¶æ€: æ€»äººæ•° {current_size}, Aç±» {a_count}, Bç±» {b_count}, Cç±» {c_count}, è¾¹æ•° {len(edges_data)}")

def save_final_data(output_dir):
    """ä¿å­˜æœ€ç»ˆæ•°æ®"""
    ensure_dir(output_dir)
    
    # ğŸ”¥ ä¿®æ”¹ï¼šè¿‡æ»¤users_dataï¼Œç¡®ä¿åªåŒ…å«ABCç±»ç”¨æˆ·
    abc_users = node_categories["A"].union(node_categories["B"]).union(node_categories["C"])
    filtered_users_data = {user_id: data for user_id, data in users_data.items() if user_id in abc_users}
    
    # 1. ä¿å­˜ç”¨æˆ·æ•°æ®ï¼ˆåŒ…å«ä¸¤ä¸ªç²‰ä¸æ•°å­—æ®µã€æµè¡Œåº¦å’Œç±»åˆ«ä¿¡æ¯ï¼‰
    users_df = pd.DataFrame.from_dict(filtered_users_data, orient='index')
    users_df.index.name = 'user_id'
    users_df.reset_index(inplace=True)
    
    # ğŸ”¥ æ–°å¢ï¼šç¡®ä¿åˆ—é¡ºåºæ­£ç¡®
    column_order = ['user_id', 'screen_name', 'display_fans_count', 'actual_fans_count', 'avg_popularity', 'category']
    users_df = users_df.reindex(columns=column_order)
    
    users_df.to_csv(f'{output_dir}/users.csv', index=False, encoding='utf-8-sig')
    print(f"âœ… ç”¨æˆ·æ•°æ®å·²ä¿å­˜: {len(users_df)} ä¸ªç”¨æˆ·ï¼ˆåªåŒ…å«ABCç±»ï¼‰")
    
    # 2. ä¿å­˜è¾¹æ•°æ®
    edges_df = pd.DataFrame(edges_data, columns=['source', 'target'])
    edges_df.to_csv(f'{output_dir}/edges.csv', index=False, encoding='utf-8-sig')
    print(f"âœ… è¾¹æ•°æ®å·²ä¿å­˜: {len(edges_df)} æ¡è¾¹")
    
    # 3. ä¿å­˜æµè¡Œåº¦æ•°æ®ï¼ˆåªåŒ…å«ABCç±»ç”¨æˆ·ï¼‰
    abc_popularity_data = {user_id: popularity for user_id, popularity in popularity_data.items() if user_id in abc_users}
    popularity_df = pd.DataFrame.from_dict(abc_popularity_data, orient='index', columns=['avg_popularity'])
    popularity_df.index.name = 'user_id'
    popularity_df.reset_index(inplace=True)
    popularity_df.to_csv(f'{output_dir}/popularity.csv', index=False, encoding='utf-8-sig')
    print(f"âœ… æµè¡Œåº¦æ•°æ®å·²ä¿å­˜: {len(popularity_df)} ä¸ªç”¨æˆ·ï¼ˆåªåŒ…å«ABCç±»ï¼‰")
    
    # 4. ä¿å­˜é«˜ç²‰ä¸ç”¨æˆ·æ•°æ®
    high_fans_df = pd.DataFrame({'user_id': list(high_fans_users)})
    high_fans_df.to_csv(f'{output_dir}/high_fans_users.csv', index=False, encoding='utf-8-sig')
    print(f"âœ… é«˜ç²‰ä¸ç”¨æˆ·æ•°æ®å·²ä¿å­˜: {len(high_fans_df)} ä¸ªç”¨æˆ·")
    
    # ğŸ”¥ æ–°å¢ï¼šéªŒè¯æ•°æ®ä¸€è‡´æ€§
    print(f"\nğŸ“Š æ•°æ®ä¸€è‡´æ€§éªŒè¯:")
    print(f"  users.csvç”¨æˆ·æ•°: {len(users_df)}")
    print(f"  popularity.csvç”¨æˆ·æ•°: {len(popularity_df)}")
    print(f"  ABCç±»ç”¨æˆ·æ€»æ•°: {len(abc_users)}")
    if len(users_df) == len(popularity_df) == len(abc_users):
        print(f"  âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
    else:
        print(f"  âš ï¸ æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥")

def main():
    """ä¸»å‡½æ•°"""
    global seed_user_id
    
    start_time = datetime.now()
    print(f"ç²‰ä¸ç½‘ç»œçˆ¬å–å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print("å¾®åšç²‰ä¸ç½‘ç»œçˆ¬å–å™¨ v4.0 (ABCç±»ä¸“ç‰ˆ)")
    print(f"- è¾¹æ–¹å‘ï¼šåšä¸»â†’ç²‰ä¸ï¼ˆä½“ç°å½±å“åŠ›ï¼‰")
    print(f"- ğŸ”¥ ç½‘ç»œç»“æ„ï¼šåªæ„å»ºABCç±»ç½‘ç»œï¼Œç§»é™¤Dç±»ç”¨æˆ·é€»è¾‘")
    print(f"- ğŸ”¥ ç”¨æˆ·æ•°æ®ï¼šusers.csvåªåŒ…å«ABCç±»ç”¨æˆ·ï¼Œä¸popularity.csvä¸€è‡´")
    print(f"- ğŸ”¥ æ–°å¢å­—æ®µï¼šusers.csvæ–°å¢categoryåˆ—ï¼ˆA/B/Cï¼‰")
    print(f"- ğŸ”¥ æµè¡Œåº¦è®¡ç®—ï¼š(è½¬+èµ+è¯„)æ€»æ•°/å¾®åšæ•°ï¼ŒåŸºäºæœ€æ–°{MAX_POSTS_FOR_POPULARITY}æ¡å¾®åš")
    print(f"- ğŸ”¥ åçˆ¬æ£€æµ‹ï¼šè¿ç»­{CONSECUTIVE_ZERO_FANS_THRESHOLD}ä¸ªç”¨æˆ·0ç²‰ä¸å°†è‡ªåŠ¨ç»ˆæ­¢ç¨‹åº")
    print("=" * 80)
    
    # ä½¿ç”¨é…ç½®ä¸­çš„ç›®æ ‡ç”¨æˆ·ID
    seed_user_id = TARGET_USER_ID
    print(f"\nç›®æ ‡ç”¨æˆ·ID: {seed_user_id}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ensure_dir(BASE_OUTPUT_DIR)
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = WeiboFansCrawler()
    if not crawler.setup_driver():
        return
    
    if not crawler.load_cookies():
        print("è¯·å…ˆè·å–æœ‰æ•ˆçš„cookieæ–‡ä»¶")
        crawler.cleanup()
        return
    
    # æµ‹è¯•ç™»å½•çŠ¶æ€
    if not crawler.test_login_status():
        print("ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥cookieæ˜¯å¦æœ‰æ•ˆ")
        crawler.cleanup()
        return
    
    try:
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = f'{BASE_OUTPUT_DIR}/user_{seed_user_id}'
        
        # åŠ è½½è¿›åº¦æˆ–é‡ç½®æ•°æ®
        load_progress(seed_user_id)
        
        # å°†ç§å­ç”¨æˆ·æ ‡è®°ä¸ºAç±»
        node_categories["A"].add(seed_user_id)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šçˆ¬å–ç§å­ç”¨æˆ·çš„ç²‰ä¸(Bç±»)
        discovered_b_users = set()
        if seed_user_id not in processed_users:
            print(f"\n=== ç¬¬ä¸€é˜¶æ®µï¼šä»Aå‡ºå‘ï¼Œå‘ç°æ‰€æœ‰B ===")
            fans_count, new_users = process_user_fans(crawler, seed_user_id, "A")
            discovered_b_users = new_users
            print(f"ç§å­ç”¨æˆ· {seed_user_id} è·å¾— {fans_count} ä¸ªç²‰ä¸")
            print_network_status()
            save_progress(seed_user_id)
        else:
            print(f"\n=== ç¬¬ä¸€é˜¶æ®µï¼šç§å­ç”¨æˆ· {seed_user_id} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡ ===")
            # ä»ç°æœ‰è¾¹ä¸­é‡æ–°å‘ç°Bç±»ç”¨æˆ·
            for source, target in edges_data:
                if source == seed_user_id:
                    discovered_b_users.add(target)
                    node_categories["B"].add(target)
        
        print(f"\n=== ç¬¬äºŒé˜¶æ®µï¼šéå†Bï¼Œå‘ç°Bâ†’Aã€Bâ†’Bè¾¹ï¼Œè®°å½•Cç±»ç”¨æˆ· ===")
        print(f"å‘ç°Bç±»ç”¨æˆ·: {len(node_categories['B'])} ä¸ª")
        print_network_status()
        
        # å¤„ç†Bç±»ç”¨æˆ·
        b_users_to_process = [u for u in node_categories["B"] if u not in processed_users]
        print(f"éœ€è¦å¤„ç†çš„Bç±»ç”¨æˆ·: {len(b_users_to_process)} ä¸ª")
        
        for i, user_id in enumerate(b_users_to_process):
            print(f"\nå¤„ç†Bç±»ç”¨æˆ· [{i+1}/{len(b_users_to_process)}] {user_id}:")
            fans_count, new_users = process_user_fans(crawler, user_id, "B")
            
            if (i + 1) % 5 == 0:
                print_network_status()
                save_progress(seed_user_id)
                
                extra_wait = random.uniform(3.0, 6.0)
                print(f"  å·²å¤„ç†{i+1}ä¸ªBç±»ç”¨æˆ·ï¼Œé¢å¤–ç­‰å¾… {extra_wait:.1f} ç§’é˜²åçˆ¬...")
                time.sleep(extra_wait)
            
            time.sleep(random.uniform(BATCH_INTERVAL_MIN, BATCH_INTERVAL_MAX))
        
        save_progress(seed_user_id)
        print_network_status()
        
        # ğŸ”¥ ä¿®æ”¹ï¼šç¬¬ä¸‰é˜¶æ®µ - ç§»é™¤Dç±»é…é¢è®¡ç®—
        print(f"\n=== ç¬¬ä¸‰é˜¶æ®µï¼šéå†Cï¼Œå‘ç°Câ†’Aã€Câ†’Bã€Câ†’Cè¾¹ ===")
        print(f"å‘ç°Cç±»ç”¨æˆ·: {len(node_categories['C'])} ä¸ª")
        print(f"âš ï¸ æ³¨æ„ï¼šåªæ„å»ºABCç±»ç½‘ç»œï¼Œå¿½ç•¥æŒ‡å‘Dç±»çš„è¾¹")
        
        # å¤„ç†Cç±»ç”¨æˆ·
        c_users_to_process = [u for u in node_categories["C"] if u not in processed_users]
        print(f"éœ€è¦å¤„ç†çš„Cç±»ç”¨æˆ·: {len(c_users_to_process)} ä¸ª")
        
        for i, user_id in enumerate(c_users_to_process):
            print(f"\nå¤„ç†Cç±»ç”¨æˆ· [{i+1}/{len(c_users_to_process)}] {user_id}:")
            fans_count, new_users = process_user_fans(crawler, user_id, "C")
            
            if (i + 1) % 5 == 0:
                print_network_status()
                save_progress(seed_user_id)
                
                extra_wait = random.uniform(3.0, 6.0)
                print(f"  å·²å¤„ç†{i+1}ä¸ªCç±»ç”¨æˆ·ï¼Œé¢å¤–ç­‰å¾… {extra_wait:.1f} ç§’é˜²åçˆ¬...")
                time.sleep(extra_wait)
            
            time.sleep(random.uniform(BATCH_INTERVAL_MIN, BATCH_INTERVAL_MAX))
        
        save_progress(seed_user_id)
        print_network_status()
        
        # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤Dç±»å¤„ç†é˜¶æ®µ
        print(f"\n=== ABCç±»ç½‘ç»œæ„å»ºå®Œæˆ ===")
        print(f"æ— éœ€å¤„ç†Dç±»ç”¨æˆ·ï¼ŒABCç±»ç½‘ç»œå·²å®Œæ•´")
        
        # ä¿å­˜æœ€ç»ˆæ•°æ®
        save_final_data(output_dir)
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        final_a_count = len(node_categories["A"])
        final_b_count = len(node_categories["B"])
        final_c_count = len(node_categories["C"])
        final_network_size = get_current_network_size()
        
        print(f"\n{'='*80}")
        print(f"ç”¨æˆ· {seed_user_id} çš„ABCç±»ç²‰ä¸ç½‘ç»œçˆ¬å–å®Œæˆ!")
        print(f"{'='*80}")
        print(f"æœ€ç»ˆç½‘ç»œè§„æ¨¡: {final_network_size} äººï¼ˆåªåŒ…å«ABCç±»ï¼‰")
        print(f"æ€»è¾¹æ•°: {len(edges_data)} æ¡")
        print(f"èŠ‚ç‚¹åˆ†å¸ƒ:")
        print(f"  Aç±»(ç§å­): {final_a_count} äºº")
        print(f"  Bç±»(ç²‰ä¸): {final_b_count} äºº")
        print(f"  Cç±»(ç²‰ä¸çš„ç²‰ä¸): {final_c_count} äºº")
        print(f"é«˜ç²‰ä¸ç”¨æˆ·æ•°é‡: {len(high_fans_users)} ä¸ª")
        print(f"ğŸ”¥ æµè¡Œåº¦æ•°æ®: {len(popularity_data)} ä¸ªç”¨æˆ·")
        
        # æµè¡Œåº¦ç»Ÿè®¡
        if popularity_data:
            popularity_values = list(popularity_data.values())
            print(f"\nğŸ”¥ æµè¡Œåº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡æµè¡Œåº¦: {sum(popularity_values)/len(popularity_values):.2f}")
            print(f"  æœ€é«˜æµè¡Œåº¦: {max(popularity_values):.2f}")
            print(f"  æœ€ä½æµè¡Œåº¦: {min(popularity_values):.2f}")
        
        print(f"\næ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
        print(f"\nâœ… ABCç±»ç²‰ä¸ç½‘ç»œæ„å»ºå®Œæˆï¼")
        print(f"âœ… è¾¹æ–¹å‘æ­£ç¡®ï¼šåšä¸»â†’ç²‰ä¸")
        print(f"âœ… ğŸ”¥ ç½‘ç»œç²¾ç®€ï¼šåªåŒ…å«ABCç±»ç”¨æˆ·ï¼Œç§»é™¤Dç±»å†—ä½™")
        print(f"âœ… ğŸ”¥ æ•°æ®ä¸€è‡´ï¼šusers.csvä¸popularity.csvç”¨æˆ·æ•°ä¸€è‡´")
        print(f"âœ… ğŸ”¥ ç±»åˆ«æ ‡è¯†ï¼šusers.csvæ–°å¢categoryåˆ—ï¼ˆA/B/Cï¼‰")
        print(f"âœ… ğŸ”¥ æ•°æ®å®Œæ•´æ€§ï¼šåŒæ—¶è·å¾—ç½‘ç»œç»“æ„å’Œå‡†ç¡®å½±å“åŠ›")
        
    except Exception as e:
        if "ANTI_CRAWL_DETECTED" in str(e):
            print(f"\nç¨‹åºå› åçˆ¬æ£€æµ‹è€Œå®‰å…¨ç»ˆæ­¢")
            print(f"å½“å‰è¿›åº¦å·²è‡ªåŠ¨ä¿å­˜ï¼Œå¯ç¨åé‡æ–°å¯åŠ¨ç¨‹åºç»§ç»­")
        else:
            print(f"\nç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {e}")
            save_progress(seed_user_id)
        
    finally:
        crawler.cleanup()
    
    end_time = datetime.now()
    duration = end_time - start_time
    print(f"\næ€»è€—æ—¶: {duration}")

if __name__ == "__main__":
    main()