import os
import json
import time
import random
import math
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from datetime import datetime
import re

# é…ç½®å‚æ•°
BASE_OUTPUT_DIR = 'C:/Tengfei/data/data/domain_network2'
PROGRESS_FILE_TEMPLATE = 'C:/Tengfei/data/crawler/fetch/progress_fans_{}.json'

# æ— æ”¾ç¼©å‚æ•°
MAX_FANS_LIMIT = 500  # æ¯ä¸ªç”¨æˆ·æœ€å¤šçˆ¬500ä¸ªç²‰ä¸
HIGH_FANS_THRESHOLD = 500  # è¶…è¿‡500ç²‰ä¸çš„ç”¨æˆ·è®°å½•ä¸ºé«˜ç²‰ä¸ç”¨æˆ·

# ä¼˜åŒ–åçš„å‚æ•°
SLEEP_MIN = 0.5
SLEEP_MAX = 1.5
BATCH_INTERVAL_MIN = 1.0
BATCH_INTERVAL_MAX = 3.0
ZERO_FANS_THRESHOLD = 10

# ğŸ”¥ æ–°çš„ä¼˜åŒ–å‚æ•°
MAX_PAGES_LIMIT = 50  # ç»Ÿä¸€æœ€å¤§é¡µæ•°é™åˆ¶
CONSECUTIVE_EMPTY_THRESHOLD = 2  # è¿ç»­ç©ºé¡µé¢é˜ˆå€¼ï¼šä»8æ”¹ä¸º2

# ===== é…ç½®éƒ¨åˆ†ï¼šä¿®æ”¹è¿™é‡Œçš„ç”¨æˆ·ID =====
TARGET_USER_ID = "6027167937"
# =========================================

# å…¨å±€å˜é‡
processed_users = set()
users_data = {}
edges_data = []
popularity_data = {}
node_categories = {"A": set(), "B": set(), "C": set()}
seed_user_id = None
high_fans_users = {}  # è®°å½•è¶…è¿‡ä¸Šé™çš„ç”¨æˆ·å®é™…ç²‰ä¸æ•°

class WeiboFansCrawler:
    def __init__(self, cookie_path='C:/Tengfei/data/crawler/crawler_for_weibo_fans-master/cookie.json'):
        self.driver = None
        self.cookie_path = cookie_path
        
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨"""
        print("æ­£åœ¨è®¾ç½®æµè§ˆå™¨...")
        
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--disable-javascript')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-extensions')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # è®¾ç½®æ›´çŸ­çš„è¶…æ—¶æ—¶é—´
            self.driver.set_page_load_timeout(8)
            self.driver.implicitly_wait(2)
            
            print("æµè§ˆå™¨è®¾ç½®æˆåŠŸ")
            return True
        except Exception as e:
            print(f"æµè§ˆå™¨è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def load_cookies(self):
        """åŠ è½½cookie"""
        if not os.path.exists(self.cookie_path):
            print(f"âŒ æœªæ‰¾åˆ°cookieæ–‡ä»¶: {self.cookie_path}")
            return False
        
        try:
            with open(self.cookie_path, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            
            self.driver.get('https://weibo.cn')
            time.sleep(1.5)
            
            for cookie in cookies:
                try:
                    self.driver.add_cookie(cookie)
                except Exception as e:
                    pass
            
            self.driver.refresh()
            time.sleep(1.5)
            print("CookieåŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ CookieåŠ è½½å¤±è´¥: {e}")
            return False

    def test_login_status(self):
        """æµ‹è¯•ç™»å½•çŠ¶æ€"""
        try:
            self.driver.get('https://weibo.cn')
            time.sleep(1.5)
            page_source = self.driver.page_source
            
            if 'ç™»å½•' in page_source and 'å¯†ç ' in page_source:
                print("âŒ éœ€è¦é‡æ–°è·å–Cookie")
                return False
            else:
                print("âœ… ç™»å½•çŠ¶æ€æ­£å¸¸")
                return True
                
        except Exception as e:
            print(f"âŒ ç™»å½•çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    def get_user_fans_count(self, user_id):
        """è·å–ç”¨æˆ·çš„çœŸå®ç²‰ä¸æ•°"""
        try:
            profile_url = f'https://weibo.cn/u/{user_id}'
            self.driver.get(profile_url)
            time.sleep(random.uniform(0.8, 1.5))
            
            page_source = self.driver.page_source
            
            # ç®€åŒ–çš„ç²‰ä¸æ•°æå–æ¨¡å¼
            patterns = [
                (r'ç²‰ä¸\[(\d+\.?\d*)[ä¸‡]?\]', lambda x: int(float(x[:-1]) * 10000) if x.endswith('ä¸‡') else int(float(x))),
                (r'ç²‰ä¸\((\d+\.?\d*)[ä¸‡]?\)', lambda x: int(float(x[:-1]) * 10000) if x.endswith('ä¸‡') else int(float(x))),
                (r'(\d+\.?\d*)[ä¸‡]?ç²‰ä¸', lambda x: int(float(x[:-1]) * 10000) if x.endswith('ä¸‡') else int(float(x))),
            ]
            
            for pattern, converter in patterns:
                match = re.search(pattern, page_source, re.IGNORECASE)
                if match:
                    num_str = match.group(1)
                    full_match = match.group(0)
                    
                    if 'ä¸‡' in full_match:
                        fans_count = int(float(num_str) * 10000)
                    else:
                        fans_count = int(float(num_str))
                    
                    return fans_count
            
            return 0
            
        except Exception as e:
            return 0
    
    def determine_crawl_strategy(self, total_fans, user_id):
        """ç¡®å®šçˆ¬å–ç­–ç•¥ï¼šæ— æ”¾ç¼©ï¼Œä½†æœ‰ä¸Šé™"""
        global high_fans_users
        
        if total_fans <= 0:
            return 0, "ç²‰ä¸æ•°ä¸º0ï¼Œè·³è¿‡"
        
        if total_fans <= MAX_FANS_LIMIT:
            # å®Œæ•´çˆ¬å–
            target_sample_size = total_fans
            strategy = f"å®Œæ•´çˆ¬å–: {total_fans} ä¸ªç²‰ä¸"
        else:
            # è¶…è¿‡ä¸Šé™ï¼Œåªçˆ¬500ä¸ªï¼Œå¹¶è®°å½•
            target_sample_size = MAX_FANS_LIMIT
            strategy = f"è¶…è¿‡ä¸Šé™ï¼Œçˆ¬å–å‰ {MAX_FANS_LIMIT} ä¸ªç²‰ä¸"
            
            # è®°å½•é«˜ç²‰ä¸ç”¨æˆ·
            high_fans_users[user_id] = {
                'actual_fans_count': total_fans,
                'crawled_count': MAX_FANS_LIMIT,
                'coverage_ratio': MAX_FANS_LIMIT / total_fans,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            print(f"  âš ï¸ ç”¨æˆ· {user_id} ç²‰ä¸æ•° {total_fans:,} è¶…è¿‡ä¸Šé™ {HIGH_FANS_THRESHOLD}ï¼Œå·²è®°å½•")
        
        print(f"  ç­–ç•¥: {strategy}")
        return target_sample_size, strategy
    
    def crawl_user_fans(self, user_id):
        """çˆ¬å–ç”¨æˆ·çš„ç²‰ä¸åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼šè¿ç»­2é¡µæ— æ•°æ®å°±åœæ­¢ï¼Œæœ€å¤§50é¡µï¼‰"""
        # è·å–ç”¨æˆ·çœŸå®ç²‰ä¸æ•°
        total_fans = self.get_user_fans_count(user_id)
        
        if total_fans == 0:
            return []
        
        # ç¡®å®šçˆ¬å–ç­–ç•¥ï¼ˆæ— æ”¾ç¼©ï¼Œä½†æœ‰ä¸Šé™ï¼‰
        target_sample_size, strategy = self.determine_crawl_strategy(total_fans, user_id)
        
        if target_sample_size == 0:
            return []
        
        print(f"  éœ€è¦çˆ¬å– {target_sample_size} ä¸ªç²‰ä¸ï¼ˆæ€»ç²‰ä¸æ•°: {total_fans:,}ï¼‰")
        print(f"  æœ€å¤§é¡µæ•°é™åˆ¶: {MAX_PAGES_LIMIT}, è¿ç»­ç©ºé¡µé˜ˆå€¼: {CONSECUTIVE_EMPTY_THRESHOLD}")
        
        try:
            fans_url = f'https://weibo.cn/{user_id}/fans'
            self.driver.get(fans_url)
            time.sleep(0.8)
            
            page_source = self.driver.page_source
            if 'ç”¨æˆ·ä¸å­˜åœ¨' in page_source or 'ç™»å½•' in page_source:
                return []
            
            fans_data = []
            consecutive_empty_pages = 0
            
            # ğŸ”¥ ä¼˜åŒ–ï¼šç»Ÿä¸€ä½¿ç”¨50é¡µé™åˆ¶
            for page in range(1, MAX_PAGES_LIMIT + 1):
                if page > 1:
                    try:
                        next_page_url = f'https://weibo.cn/{user_id}/fans?page={page}'
                        self.driver.get(next_page_url)
                        time.sleep(random.uniform(0.3, 1.0))
                    except Exception as e:
                        break
                
                # å¿«é€ŸæŸ¥æ‰¾ç²‰ä¸é“¾æ¥
                try:
                    fan_elements = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/u/')]")
                    
                    page_fans = []
                    processed_ids = set()
                    
                    for element in fan_elements:
                        try:
                            fan_href = element.get_attribute('href')
                            fan_name = element.text.strip()
                            
                            if fan_href and '/u/' in fan_href:
                                fan_id = fan_href.split('/u/')[-1].split('?')[0].split('/')[0]
                                
                                if fan_id.isdigit() and fan_id not in processed_ids and fan_name:
                                    page_fans.append({
                                        'id': fan_id,
                                        'screen_name': fan_name,
                                        'followers_count': 0,
                                        'friends_count': 0,
                                        'statuses_count': 0,
                                        'verified': False,
                                        'description': ''
                                    })
                                    processed_ids.add(fan_id)
                        except Exception as e:
                            continue
                    
                    if len(page_fans) == 0:
                        consecutive_empty_pages += 1
                        # ğŸ”¥ ä¼˜åŒ–ï¼šè¿ç»­2é¡µæ— æ•°æ®å°±åœæ­¢ï¼ˆä»8æ”¹ä¸º2ï¼‰
                        if consecutive_empty_pages >= CONSECUTIVE_EMPTY_THRESHOLD:
                            print(f"    è¿ç»­ {consecutive_empty_pages} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                            break
                    else:
                        consecutive_empty_pages = 0
                        fans_data.extend(page_fans)
                    
                    # ğŸ”¥ é‡è¦ï¼šåªæœ‰çœŸæ­£è¾¾åˆ°ç›®æ ‡æ•°é‡æ‰åœæ­¢
                    if len(fans_data) >= target_sample_size:
                        print(f"    å·²è·å– {len(fans_data)} ä¸ªç²‰ä¸ï¼Œè¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œåœæ­¢çˆ¬å–")
                        break
                        
                except Exception as e:
                    consecutive_empty_pages += 1
                    # ğŸ”¥ ä¼˜åŒ–ï¼šè¿ç»­2é¡µå¼‚å¸¸ä¹Ÿåœæ­¢
                    if consecutive_empty_pages >= CONSECUTIVE_EMPTY_THRESHOLD:
                        break
                
                # å¤§å¹…å‡å°‘ç­‰å¾…æ—¶é—´
                time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))
                
                # å‡å°‘é¢å¤–ç­‰å¾…çš„é¢‘ç‡å’Œæ—¶é—´
                if page % 25 == 0:
                    time.sleep(random.uniform(0.5, 2.0))
                    print(f"    å·²çˆ¬å– {page} é¡µï¼Œè·å¾— {len(fans_data)} ä¸ªç²‰ä¸")
            
            # æˆªå–åˆ°ç›®æ ‡æ•°é‡
            if len(fans_data) > target_sample_size:
                fans_data = fans_data[:target_sample_size]
            
            print(f"  âœ… æœ€ç»ˆè·å– {len(fans_data)} ä¸ªç²‰ä¸ (ç›®æ ‡: {target_sample_size})")
            
            return fans_data
            
        except Exception as e:
            return []
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.driver:
            self.driver.quit()

def ensure_dir(directory):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    if not os.path.exists(directory):
        os.makedirs(directory)

def save_progress(seed_user_id):
    """ä¿å­˜çˆ¬å–è¿›åº¦ï¼ˆå¢å¼ºç‰ˆï¼Œæ›´é¢‘ç¹ä¿å­˜ï¼‰"""
    progress_file = PROGRESS_FILE_TEMPLATE.format(seed_user_id)
    ensure_dir(os.path.dirname(progress_file))
    
    # ä¿å­˜å®Œæ•´è¿›åº¦
    progress_data = {
        "users": users_data,
        "edges": edges_data,
        "popularity": popularity_data,
        "processed": list(processed_users),
        "categories": {k: list(v) for k, v in node_categories.items()},
        "high_fans_users": high_fans_users,
        "save_timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "total_users": len(users_data),
        "total_edges": len(edges_data)
    }
    
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è¿›åº¦å·²ä¿å­˜: ç”¨æˆ· {len(users_data)} ä¸ª, è¾¹ {len(edges_data)} æ¡")

def save_high_fans_report(output_dir):
    """ä¿å­˜è¶…è¿‡ä¸Šé™çš„ç”¨æˆ·æŠ¥å‘Š"""
    if not high_fans_users:
        print(f"\nğŸ“Š æ— è¶…è¿‡ä¸Šé™ {HIGH_FANS_THRESHOLD} çš„ç”¨æˆ·")
        return
    
    ensure_dir(output_dir)
    
    # ä¿å­˜JSONæ ¼å¼
    report_file = f'{output_dir}/high_fans_users_report.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(high_fans_users, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜CSVæ ¼å¼ä¾¿äºæŸ¥çœ‹
    df_data = []
    for user_id, info in high_fans_users.items():
        df_data.append({
            'user_id': user_id,
            'actual_fans_count': info['actual_fans_count'],
            'crawled_count': info['crawled_count'],
            'coverage_ratio': f"{info['coverage_ratio']:.4f}",
            'timestamp': info['timestamp']
        })
    
    df = pd.DataFrame(df_data)
    df.to_csv(f'{output_dir}/high_fans_users_report.csv', index=False, encoding='utf-8-sig')
    
    # ä¿å­˜TXTæ ¼å¼ï¼ˆä¾¿äºå¿«é€ŸæŸ¥çœ‹ï¼‰
    with open(f'{output_dir}/high_fans_users_summary.txt', 'w', encoding='utf-8') as f:
        f.write(f"é«˜ç²‰ä¸ç”¨æˆ·æ€»ç»“æŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ä¸Šé™é˜ˆå€¼: {HIGH_FANS_THRESHOLD} ç²‰ä¸\n")
        f.write(f"è¶…è¿‡ä¸Šé™ç”¨æˆ·æ•°: {len(high_fans_users)}\n\n")
        
        f.write("è¯¦ç»†åˆ—è¡¨:\n")
        for user_id, info in sorted(high_fans_users.items(), key=lambda x: x[1]['actual_fans_count'], reverse=True):
            f.write(f"ç”¨æˆ· {user_id}: {info['actual_fans_count']:,} ç²‰ä¸ â†’ çˆ¬å– {info['crawled_count']} ä¸ª ({info['coverage_ratio']:.2%})\n")
    
    print(f"\nğŸ“Š é«˜ç²‰ä¸ç”¨æˆ·æŠ¥å‘Šå·²ä¿å­˜:")
    print(f"   JSON: {report_file}")
    print(f"   CSV: {output_dir}/high_fans_users_report.csv")
    print(f"   TXT: {output_dir}/high_fans_users_summary.txt")
    print(f"   å…± {len(high_fans_users)} ä¸ªç”¨æˆ·ç²‰ä¸æ•°è¶…è¿‡ {HIGH_FANS_THRESHOLD}")

def load_progress(seed_user_id):
    """åŠ è½½çˆ¬å–è¿›åº¦ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    global processed_users, users_data, edges_data, popularity_data, node_categories, high_fans_users
    
    progress_file = PROGRESS_FILE_TEMPLATE.format(seed_user_id)
    
    if not os.path.exists(progress_file):
        print("æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹çˆ¬å–")
        reset_global_data()
        return
    
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        processed_users = set(data.get("processed", []))
        users_data = data.get("users", {})
        edges_data = data.get("edges", [])
        popularity_data = data.get("popularity", {})
        high_fans_users = data.get("high_fans_users", {})
        
        if "categories" in data:
            for k, v in data["categories"].items():
                node_categories[k] = set(v)
        
        save_time = data.get("save_timestamp", "æœªçŸ¥")
        print(f"âœ… å·²åŠ è½½è¿›åº¦ (ä¿å­˜äº {save_time}):")
        print(f"   ç”¨æˆ·: {len(users_data)} ä¸ª")
        print(f"   è¾¹: {len(edges_data)} æ¡") 
        print(f"   å·²å¤„ç†ç”¨æˆ·: {len(processed_users)} ä¸ª")
        if high_fans_users:
            print(f"   é«˜ç²‰ä¸ç”¨æˆ·: {len(high_fans_users)} ä¸ª")
        
    except Exception as e:
        print(f"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å‡ºé”™: {e}ï¼Œä»å¤´å¼€å§‹")
        reset_global_data()

def reset_global_data():
    """é‡ç½®å…¨å±€æ•°æ®"""
    global processed_users, users_data, edges_data, popularity_data, node_categories, high_fans_users
    processed_users = set()
    users_data = {}
    edges_data = []
    popularity_data = {}
    node_categories = {"A": set(), "B": set(), "C": set()}
    high_fans_users = {}

def process_user_fans(crawler, user_id, category=None):
    """å¤„ç†å•ä¸ªç”¨æˆ·çš„ç²‰ä¸åˆ—è¡¨ï¼ˆæ­£ç¡®çš„Cç±»è¾¹è¿‡æ»¤é€»è¾‘ï¼‰"""
    global processed_users, users_data, edges_data, popularity_data, node_categories
    
    if user_id in processed_users:
        return 0
    
    processed_users.add(user_id)
    
    if category and category in node_categories:
        node_categories[category].add(user_id)
    
    # çˆ¬å–ç²‰ä¸åˆ—è¡¨
    fans_users = crawler.crawl_user_fans(user_id)
    
    if not fans_users:
        return 0
    
    # ğŸ”¥ å…³é”®ï¼šè·å–å½“å‰æ‰€æœ‰ABCç±»ç”¨æˆ·ï¼ˆç”¨äºCç±»è¾¹è¿‡æ»¤ï¼‰
    all_abc_users = node_categories["A"].union(node_categories["B"]).union(node_categories["C"])
    
    # æ·»åŠ ç”¨æˆ·æ•°æ®å’Œè¾¹æ•°æ®
    valid_edges_added = 0
    total_fans_found = len(fans_users)
    
    for fan in fans_users:
        fan_id = str(fan.get('id'))
        
        # æ·»åŠ ç²‰ä¸ç”¨æˆ·ä¿¡æ¯
        if fan_id not in users_data:
            users_data[fan_id] = {
                'screen_name': fan.get('screen_name', ''),
                'followers_count': fan.get('followers_count', 0),
                'friends_count': fan.get('friends_count', 0),
                'statuses_count': fan.get('statuses_count', 0),
                'verified': fan.get('verified', False),
                'description': fan.get('description', '')
            }
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šCç±»ç”¨æˆ·è¾¹è¿‡æ»¤é€»è¾‘
        edge = (fan_id, user_id)
        
        if category == "C":
            # ğŸ¯ Cç±»ç”¨æˆ·ï¼šåªæœ‰å½“ç²‰ä¸æ˜¯ABCç±»ç”¨æˆ·æ—¶æ‰æ·»åŠ è¾¹
            if fan_id in all_abc_users:
                if edge not in edges_data:
                    edges_data.append(edge)
                    valid_edges_added += 1
            # å¦åˆ™å¿½ç•¥è¿™æ¡è¾¹ï¼ˆDç±»ç”¨æˆ·ï¼Œä¸‰è·³ä»¥å¤–ï¼‰
        else:
            # Aç±»å’ŒBç±»ç”¨æˆ·ï¼šæ·»åŠ æ‰€æœ‰è¾¹
            if edge not in edges_data:
                edges_data.append(edge)
                valid_edges_added += 1
    
    # æ·»åŠ å½“å‰ç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if user_id not in users_data:
        users_data[user_id] = {
            'screen_name': f'ç”¨æˆ·{user_id}',
            'followers_count': 0,
            'friends_count': 0,
            'statuses_count': 0,
            'verified': False,
            'description': ''
        }
    
    if category == "C":
        print(f"    Cç±»ç”¨æˆ· {user_id}: çˆ¬å– {total_fans_found} ä¸ªç²‰ä¸ï¼Œæœ‰æ•ˆè¾¹(æŒ‡å‘ABC) {valid_edges_added} æ¡")
    
    return total_fans_found

def process_batch_fans(crawler, users_to_process, category=None):
    """æ‰¹é‡å¤„ç†ç”¨æˆ·ç²‰ä¸ï¼ˆæ›´é¢‘ç¹ä¿å­˜è¿›åº¦ï¼‰"""
    if not users_to_process:
        return
    
    print(f"\nå¼€å§‹æ‰¹é‡å¤„ç† {len(users_to_process)} ä¸ª{category}ç±»ç”¨æˆ·ï¼ˆä¼˜åŒ–æ¨¡å¼ï¼šè¿ç»­{CONSECUTIVE_EMPTY_THRESHOLD}é¡µæ— æ•°æ®å³åœæ­¢ï¼‰")
    if category == "C":
        print("âš ï¸  Cç±»ç”¨æˆ·è¾¹è¿‡æ»¤ï¼šåªä¿ç•™æŒ‡å‘ABCç±»ç”¨æˆ·çš„è¾¹ï¼Œå¿½ç•¥Dç±»ç”¨æˆ·è¾¹")
    
    consecutive_zeros = 0
    
    for i, user_id in enumerate(users_to_process):
        try:
            print(f"\nå¤„ç† [{i+1}/{len(users_to_process)}] ç”¨æˆ· {user_id}:")
            fans_count = process_user_fans(crawler, user_id, category)
            
            print(f"  âœ… ç”¨æˆ· {user_id}: {fans_count} ç²‰ä¸")
            
            # æ£€æŸ¥åçˆ¬æœºåˆ¶
            if fans_count == 0:
                consecutive_zeros += 1
            else:
                consecutive_zeros = 0
            
            if consecutive_zeros >= ZERO_FANS_THRESHOLD:
                print(f"\nâš ï¸ æ£€æµ‹åˆ°è¿ç»­ {consecutive_zeros} ä¸ªç”¨æˆ·ç²‰ä¸æ•°ä¸º0ï¼Œä¼‘æ¯20ç§’...")
                time.sleep(20)
                consecutive_zeros = 0
        
        except Exception as e:
            print(f"å¤„ç†ç”¨æˆ· {user_id} æ—¶å‡ºé”™: {e}")
        
        # ğŸ”¥ æ›´é¢‘ç¹çš„è¿›åº¦ä¿å­˜ï¼šæ¯10ä¸ªç”¨æˆ·ä¿å­˜ä¸€æ¬¡
        if (i + 1) % 10 == 0:
            save_progress(seed_user_id)
            print(f"  è¿›åº¦å·²ä¿å­˜ ({i+1}/{len(users_to_process)})")
        
        # è¿›ä¸€æ­¥å‡å°‘æ‰¹æ¬¡é—´ç­‰å¾…
        if i < len(users_to_process) - 1:
            wait_time = random.uniform(BATCH_INTERVAL_MIN, BATCH_INTERVAL_MAX)
            if wait_time > 2.5:
                print(f"ç­‰å¾… {wait_time:.1f} ç§’...")
            time.sleep(wait_time)

def save_network_data(output_dir):
    """ä¿å­˜ç½‘ç»œæ•°æ®"""
    ensure_dir(output_dir)
    
    # ä¿å­˜ç”¨æˆ·æ•°æ®
    users_df = pd.DataFrame.from_dict(users_data, orient='index')
    users_df.index.name = 'user_id'
    users_df.reset_index(inplace=True)
    users_df.to_csv(f'{output_dir}/users.csv', index=False, encoding='utf-8-sig')
    
    # ä¿å­˜è¾¹æ•°æ®ï¼ˆç²‰ä¸å…³ç³»ï¼‰
    edges_df = pd.DataFrame(edges_data, columns=['source', 'target'])
    edges_df.to_csv(f'{output_dir}/edges.csv', index=False, encoding='utf-8-sig')
    
    # ä¿å­˜æµè¡Œåº¦æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    if popularity_data:
        popularity_df = pd.DataFrame.from_dict(popularity_data, orient='index')
        popularity_df.index.name = 'user_id'
        popularity_df.reset_index(inplace=True)
        popularity_df.to_csv(f'{output_dir}/popularity.csv', index=False, encoding='utf-8-sig')
    
    # ä¿å­˜èŠ‚ç‚¹ç±»åˆ«
    with open(f'{output_dir}/node_categories.json', 'w', encoding='utf-8') as f:
        json.dump({k: list(v) for k, v in node_categories.items()}, f, ensure_ascii=False)
    
    # ä¿å­˜é«˜ç²‰ä¸ç”¨æˆ·æŠ¥å‘Š
    save_high_fans_report(output_dir)

def main():
    """ä¸»å‡½æ•°"""
    global seed_user_id
    
    start_time = datetime.now()
    print(f"ç²‰ä¸ç½‘ç»œçˆ¬å–å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print("å¾®åšç²‰ä¸ç½‘ç»œçˆ¬å–å™¨ v14.0 (é€Ÿåº¦ä¼˜åŒ–ç‰ˆ)")
    print("- æ— æ”¾ç¼©ç­–ç•¥: å®Œæ•´çˆ¬å–æ‰€æœ‰ç²‰ä¸ï¼Œä¸Šé™500")
    print(f"- é€Ÿåº¦ä¼˜åŒ–: è¿ç»­{CONSECUTIVE_EMPTY_THRESHOLD}é¡µæ— æ•°æ®å³åœæ­¢ï¼Œæœ€å¤§{MAX_PAGES_LIMIT}é¡µ")
    print("- ç¡®ä¿Cç±»ç”¨æˆ·è¾¹è¿‡æ»¤é€»è¾‘æ­£ç¡®ï¼šåªä¿ç•™ABCè¾¹")
    print("- å¢å¼ºæ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼Œæ¯10ä¸ªç”¨æˆ·ä¿å­˜ä¸€æ¬¡")
    print("- å®Œæ•´çš„ABCç±»äºŒè·³ç²‰ä¸ç½‘ç»œ")
    print("=" * 80)
    
    # ä½¿ç”¨é…ç½®ä¸­çš„ç›®æ ‡ç”¨æˆ·ID
    seed_user_id = TARGET_USER_ID
    print(f"\nç›®æ ‡ç”¨æˆ·ID: {seed_user_id}")
    print(f"ç­–ç•¥: ç²‰ä¸æ•°â‰¤{MAX_FANS_LIMIT}å®Œæ•´çˆ¬å–ï¼Œ>{MAX_FANS_LIMIT}çˆ¬å–å‰{MAX_FANS_LIMIT}ä¸ª")
    print(f"é«˜ç²‰ä¸ç”¨æˆ·é˜ˆå€¼: {HIGH_FANS_THRESHOLD} ç²‰ä¸")
    print(f"ğŸ¯ äºŒè·³ç½‘ç»œè¦æ±‚: èŠ‚ç‚¹åªåŒ…å«ABCç±»ï¼Œè¾¹åªåŒ…å«ABCä¹‹é—´çš„æœ‰å‘è¾¹")
    print(f"âš¡ é€Ÿåº¦ä¼˜åŒ–: æœ€å¤§{MAX_PAGES_LIMIT}é¡µï¼Œè¿ç»­{CONSECUTIVE_EMPTY_THRESHOLD}é¡µæ— æ•°æ®å³åœæ­¢")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ensure_dir(BASE_OUTPUT_DIR)
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = WeiboFansCrawler()
    if not crawler.setup_driver():
        return
    
    if not crawler.load_cookies():
        print("è¯·å…ˆè¿è¡Œ get_cookie.py è·å–cookie")
        crawler.cleanup()
        return
    
    # æµ‹è¯•ç™»å½•çŠ¶æ€
    if not crawler.test_login_status():
        print("ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥cookieæ˜¯å¦æœ‰æ•ˆ")
        crawler.cleanup()
        return
    
    try:
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = f'{BASE_OUTPUT_DIR}/user_{seed_user_id}'
        
        # åŠ è½½è¿›åº¦æˆ–é‡ç½®æ•°æ®
        load_progress(seed_user_id)
        
        # å°†ç§å­ç”¨æˆ·æ ‡è®°ä¸ºAç±»
        node_categories["A"].add(seed_user_id)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šçˆ¬å–ç§å­ç”¨æˆ·çš„ç²‰ä¸(Bç±»)
        if seed_user_id not in processed_users:
            print(f"\n=== ç¬¬ä¸€é˜¶æ®µï¼šçˆ¬å–ç§å­ç”¨æˆ· {seed_user_id} çš„ç²‰ä¸(Bç±») ===")
            fans_count = process_user_fans(crawler, seed_user_id, "A")
            print(f"ç§å­ç”¨æˆ· {seed_user_id} è·å¾— {fans_count} ä¸ªç²‰ä¸")
            save_progress(seed_user_id)
        else:
            print(f"\n=== ç¬¬ä¸€é˜¶æ®µï¼šç§å­ç”¨æˆ· {seed_user_id} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡ ===")
        
        # åŠ¨æ€æ›´æ–°Bç±»ç”¨æˆ·ï¼ˆç§å­ç”¨æˆ·çš„ç²‰ä¸ï¼‰
        b_users = set()
        for source, target in edges_data:
            if target == seed_user_id:
                b_users.add(source)
                node_categories["B"].add(source)
        
        print(f"\n=== ç¬¬äºŒé˜¶æ®µï¼šçˆ¬å–Bç±»ç”¨æˆ·çš„ç²‰ä¸(Cç±») ===")
        print(f"å‘ç°Bç±»ç”¨æˆ·: {len(b_users)} ä¸ª")
        
        # è¿‡æ»¤å·²å¤„ç†çš„Bç±»ç”¨æˆ·
        b_users_to_process = [u for u in b_users if u not in processed_users]
        print(f"éœ€è¦å¤„ç†çš„Bç±»ç”¨æˆ·: {len(b_users_to_process)} ä¸ª")
        
        if b_users_to_process:
            process_batch_fans(crawler, b_users_to_process, "B")
            save_progress(seed_user_id)
        else:
            print("æ‰€æœ‰Bç±»ç”¨æˆ·å·²å¤„ç†å®Œæˆ")
        
        # åŠ¨æ€æ›´æ–°Cç±»ç”¨æˆ·ï¼ˆBç±»ç”¨æˆ·çš„ç²‰ä¸ï¼Œä½†ä¸æ˜¯Aæˆ–Bç±»ï¼‰
        a_and_b_users = node_categories["A"].union(node_categories["B"])
        c_users = set()
        
        for source, target in edges_data:
            if target in node_categories["B"] and source not in a_and_b_users:
                c_users.add(source)
                node_categories["C"].add(source)
        
        print(f"\n=== ç¬¬ä¸‰é˜¶æ®µï¼šçˆ¬å–Cç±»ç”¨æˆ·çš„ç²‰ä¸ï¼ˆè¡¥å…¨è¾¹ï¼Œåªä¿ç•™ABCè¾¹ï¼‰ ===")
        print(f"å‘ç°Cç±»ç”¨æˆ·: {len(c_users)} ä¸ª")
        print(f"âš ï¸ é‡è¦ï¼šCç±»ç”¨æˆ·çš„ç²‰ä¸è¾¹åªæœ‰æŒ‡å‘ABCç±»ç”¨æˆ·æ—¶æ‰ä¼šè¢«ä¿ç•™")
        
        # è¿‡æ»¤å·²å¤„ç†çš„Cç±»ç”¨æˆ·
        c_users_to_process = [u for u in c_users if u not in processed_users]
        print(f"éœ€è¦å¤„ç†çš„Cç±»ç”¨æˆ·: {len(c_users_to_process)} ä¸ª")
        
        if c_users_to_process:
            process_batch_fans(crawler, c_users_to_process, "C")
        else:
            print("æ‰€æœ‰Cç±»ç”¨æˆ·å·²å¤„ç†å®Œæˆ")
        
        # ä¿å­˜æœ€ç»ˆæ•°æ®
        save_network_data(output_dir)
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        final_a_count = len(node_categories["A"])
        final_b_count = len(node_categories["B"])
        final_c_count = len(node_categories["C"])
        
        print(f"\n{'='*80}")
        print(f"ç”¨æˆ· {seed_user_id} çš„ç²‰ä¸ç½‘ç»œçˆ¬å–å®Œæˆ!")
        print(f"{'='*80}")
        print(f"æ€»èŠ‚ç‚¹æ•°: {len(users_data)}, æ€»è¾¹æ•°: {len(edges_data)}")
        print(f"èŠ‚ç‚¹åˆ†å¸ƒ: Aç±»(ç§å­): {final_a_count}, Bç±»(ç²‰ä¸): {final_b_count}, Cç±»(ç²‰ä¸çš„ç²‰ä¸): {final_c_count}")
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
        
        # éªŒè¯è¾¹çš„å®Œæ•´æ€§
        print(f"\n=== è¾¹å®Œæ•´æ€§éªŒè¯ ===")
        edge_stats = {"Bâ†’A": 0, "Câ†’B": 0, "Câ†’A": 0, "Câ†’C": 0, "å…¶ä»–": 0}
        
        for source, target in edges_data:
            if source in node_categories["B"] and target in node_categories["A"]:
                edge_stats["Bâ†’A"] += 1
            elif source in node_categories["C"] and target in node_categories["B"]:
                edge_stats["Câ†’B"] += 1
            elif source in node_categories["C"] and target in node_categories["A"]:
                edge_stats["Câ†’A"] += 1
            elif source in node_categories["C"] and target in node_categories["C"]:
                edge_stats["Câ†’C"] += 1
            else:
                edge_stats["å…¶ä»–"] += 1
        
        for edge_type, count in edge_stats.items():
            print(f"{edge_type} è¾¹æ•°: {count}")
        
        # æ˜¾ç¤ºæ— æ”¾ç¼©æ•ˆæœ
        if high_fans_users:
            print(f"\nğŸ“Š é«˜ç²‰ä¸ç”¨æˆ·ç»Ÿè®¡:")
            print(f"   è¶…è¿‡ä¸Šé™ç”¨æˆ·æ•°: {len(high_fans_users)}")
            print(f"   è¯¦ç»†æŠ¥å‘Š: {output_dir}/high_fans_users_report.csv")
            
            # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
            sample_users = list(high_fans_users.items())[:3]
            for user_id, info in sample_users:
                actual = info['actual_fans_count']
                crawled = info['crawled_count']
                ratio = info['coverage_ratio']
                print(f"   ç”¨æˆ· {user_id}: {actual:,} ç²‰ä¸ â†’ çˆ¬å–{crawled} ({ratio:.2%})")
        
        print(f"âœ… å®Œæ•´çš„ABCç±»äºŒè·³ç²‰ä¸ç½‘ç»œæ„å»ºå®Œæˆï¼")
        print(f"âœ… Cç±»è¾¹è¿‡æ»¤å·²æ­£ç¡®å®ç°ï¼šåªä¿ç•™æŒ‡å‘ABCç±»ç”¨æˆ·çš„è¾¹")
        print(f"âœ… æ–­ç‚¹ç»­ä¼ åŠŸèƒ½å·²å¢å¼ºï¼šæ¯10ä¸ªç”¨æˆ·ä¿å­˜ä¸€æ¬¡è¿›åº¦")
        print(f"âš¡ é€Ÿåº¦ä¼˜åŒ–å·²åº”ç”¨ï¼šè¿ç»­{CONSECUTIVE_EMPTY_THRESHOLD}é¡µæ— æ•°æ®å³åœæ­¢ï¼ŒèŠ‚çœå¤§é‡æ—¶é—´")
        
    finally:
        crawler.cleanup()
    
    end_time = datetime.now()
    duration = end_time - start_time
    print(f"\næ€»è€—æ—¶: {duration}")

if __name__ == "__main__":
    main()