import os
import json
import time
import random
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from datetime import datetime
import re

# ğŸ¯ é…ç½®ï¼šåŒ¹é…fetch3.pyçš„è®¾ç½®
TARGET_NETWORK_PATH = 'C:/Tengfei/data/data/domain_network3/user_3855570307'
COOKIE_PATH = 'C:/Tengfei/data/crawler/crawler_for_weibo_fans-master/cookie.json'

# çˆ¬å–å‚æ•°ï¼ˆä¸fetch3.pyä¿æŒä¸€è‡´ï¼‰
MAX_PAGES_LIMIT = 20
CONSECUTIVE_EMPTY_THRESHOLD = 2
SLEEP_MIN = 0.4
SLEEP_MAX = 0.8
BATCH_INTERVAL_MIN = 0.5
BATCH_INTERVAL_MAX = 1.5

# è¿›åº¦ä¿å­˜å‚æ•°
SAVE_INTERVAL = 20  # æ¯å¤„ç†20ä¸ªç”¨æˆ·ä¿å­˜ä¸€æ¬¡è¿›åº¦

class WeiboMissedUsersFinder:
    def __init__(self, cookie_path=COOKIE_PATH):
        self.driver = None
        self.cookie_path = cookie_path
        
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨ï¼ˆä¸fetch3.pyç›¸åŒï¼‰"""
        print("æ­£åœ¨è®¾ç½®æµè§ˆå™¨...")
        
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--disable-javascript')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-extensions')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # è®¾ç½®è¶…æ—¶æ—¶é—´
            self.driver.set_page_load_timeout(8)
            self.driver.implicitly_wait(2)
            
            print("âœ… æµè§ˆå™¨è®¾ç½®æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def load_cookies(self):
        """åŠ è½½cookie"""
        if not os.path.exists(self.cookie_path):
            print(f"âŒ æœªæ‰¾åˆ°cookieæ–‡ä»¶: {self.cookie_path}")
            return False
        
        try:
            with open(self.cookie_path, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            
            self.driver.get('https://weibo.cn')
            time.sleep(2)
            
            for cookie in cookies:
                try:
                    self.driver.add_cookie(cookie)
                except Exception as e:
                    pass
            
            self.driver.refresh()
            time.sleep(2)
            print("âœ… CookieåŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ CookieåŠ è½½å¤±è´¥: {e}")
            return False

    def test_login_status(self):
        """æµ‹è¯•ç™»å½•çŠ¶æ€"""
        try:
            self.driver.get('https://weibo.cn')
            time.sleep(2)
            page_source = self.driver.page_source
            
            if 'ç™»å½•' in page_source and 'å¯†ç ' in page_source:
                print("âŒ éœ€è¦é‡æ–°è·å–Cookie")
                return False
            else:
                print("âœ… ç™»å½•çŠ¶æ€æ­£å¸¸")
                return True
                
        except Exception as e:
            print(f"âŒ ç™»å½•çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    def crawl_user_fans(self, user_id):
        """çˆ¬å–ç”¨æˆ·çš„ç²‰ä¸åˆ—è¡¨ï¼ˆä¸fetch3.pyç›¸åŒçš„æ–¹æ³•ï¼‰"""
        print(f"  ğŸ” é‡æ–°çˆ¬å–ç”¨æˆ· {user_id} çš„ç²‰ä¸...")
        
        try:
            fans_url = f'https://weibo.cn/{user_id}/fans'
            self.driver.get(fans_url)
            time.sleep(0.5)
            
            page_source = self.driver.page_source
            if 'ç”¨æˆ·ä¸å­˜åœ¨' in page_source or 'ç™»å½•' in page_source:
                print(f"  âŒ ç”¨æˆ· {user_id} ä¸å­˜åœ¨æˆ–éœ€è¦ç™»å½•")
                return []
            
            fans_data = []
            consecutive_empty_pages = 0
            
            # å—å¾®åšé™åˆ¶ï¼Œæœ€å¤šåªèƒ½çˆ¬20é¡µ
            for page in range(1, MAX_PAGES_LIMIT + 1):
                if page > 1:
                    try:
                        next_page_url = f'https://weibo.cn/{user_id}/fans?page={page}'
                        self.driver.get(next_page_url)
                        time.sleep(random.uniform(0.5, 1.0))
                    except Exception as e:
                        break
                
                # æŸ¥æ‰¾ç²‰ä¸é“¾æ¥
                try:
                    fan_elements = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/u/')]")
                    
                    page_fans = []
                    processed_ids = set()
                    
                    for element in fan_elements:
                        try:
                            fan_href = element.get_attribute('href')
                            fan_name = element.text.strip()
                            
                            if fan_href and '/u/' in fan_href:
                                fan_id = fan_href.split('/u/')[-1].split('?')[0].split('/')[0]
                                
                                if fan_id.isdigit() and fan_id not in processed_ids and fan_name:
                                    page_fans.append({
                                        'id': fan_id,
                                        'screen_name': fan_name
                                    })
                                    processed_ids.add(fan_id)
                        except Exception as e:
                            continue
                    
                    if len(page_fans) == 0:
                        consecutive_empty_pages += 1
                        if consecutive_empty_pages >= CONSECUTIVE_EMPTY_THRESHOLD:
                            break
                    else:
                        consecutive_empty_pages = 0
                        fans_data.extend(page_fans)
                        
                except Exception as e:
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= CONSECUTIVE_EMPTY_THRESHOLD:
                        break
                
                time.sleep(random.uniform(SLEEP_MIN, SLEEP_MAX))
                
                # ğŸ”¥ æ–°å¢ï¼šæ¯éš”10é¡µå¢åŠ é¢å¤–ç­‰å¾…ï¼Œé˜²æ­¢åçˆ¬
                if page % 10 == 0 and page > 0:
                    extra_wait = random.uniform(1.0, 2.0)
                    print(f"    å·²çˆ¬å– {page} é¡µï¼Œé¢å¤–ç­‰å¾… {extra_wait:.1f} ç§’é˜²åçˆ¬...")
                    time.sleep(extra_wait)
            
            actual_fans_count = len(fans_data)
            print(f"  âœ… ç”¨æˆ· {user_id} é‡æ–°çˆ¬å–åˆ° {actual_fans_count} ä¸ªç²‰ä¸")
            
            return fans_data
            
        except Exception as e:
            print(f"  âŒ çˆ¬å–ç”¨æˆ· {user_id} ç²‰ä¸æ—¶å‡ºé”™: {e}")
            return []
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.driver:
            self.driver.quit()

def load_network_data(network_path):
    """ğŸ”¥ ä¿®å¤ç‰ˆï¼šåªåŠ è½½edgesæ•°æ®ï¼Œä»edgesä¸­è·å–çœŸå®ç½‘ç»œç”¨æˆ·"""
    edges_file = os.path.join(network_path, 'edges.csv')
    
    if not os.path.exists(edges_file):
        print(f"âŒ æœªæ‰¾åˆ°edgesæ–‡ä»¶: {edges_file}")
        return None
    
    print(f"ğŸ“ åŠ è½½ç½‘ç»œæ•°æ®:")
    print(f"  - è¾¹æ•°æ®: {edges_file}")
    
    edges_df = pd.read_csv(edges_file)
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹ç¡®ä¿ä¸€è‡´æ€§
    edges_df['source'] = edges_df['source'].astype(str)
    edges_df['target'] = edges_df['target'].astype(str)
    
    print(f"  ğŸ“Š è¾¹æ•°æ®: {len(edges_df)} æ¡è¾¹")
    
    return edges_df

def get_network_users_from_edges(edges_df):
    """ğŸ”¥ ä¿®å¤ç‰ˆï¼šä»edgesè¡¨ä¸­è·å–çœŸå®ç½‘ç»œç”¨æˆ·ï¼ˆAã€Bã€Cç±»ç”¨æˆ·ï¼‰"""
    # ğŸ¯ å…³é”®ä¿®å¤ï¼šçœŸå®ç½‘ç»œç”¨æˆ· = edgesä¸­æ‰€æœ‰å‡ºç°è¿‡çš„ç”¨æˆ·ID
    source_users = set(edges_df['source'].unique())
    target_users = set(edges_df['target'].unique())
    network_users = source_users.union(target_users)
    
    print(f"ğŸ” çœŸå®ç½‘ç»œç”¨æˆ·ç»Ÿè®¡ï¼ˆä»…åŸºäºedgesï¼‰:")
    print(f"  ğŸ“Š sourceç”¨æˆ·æ•°: {len(source_users)}")
    print(f"  ğŸ“Š targetç”¨æˆ·æ•°: {len(target_users)}")
    print(f"  ğŸ“Š ç½‘ç»œæ€»ç”¨æˆ·æ•°: {len(network_users)} ï¼ˆè¿™æ‰æ˜¯çœŸå®çš„Aã€Bã€Cç±»ç”¨æˆ·ï¼‰")
    
    return network_users

def find_zero_outdegree_users_in_network(edges_df, network_users):
    """ğŸ”¥ ä¿®å¤ç‰ˆï¼šåªåœ¨çœŸå®ç½‘ç»œç”¨æˆ·ä¸­æ‰¾å‡ºåº¦ä¸º0çš„ç”¨æˆ·"""
    # è·å–æ‰€æœ‰åœ¨edgesä¸­ä½œä¸ºsourceå‡ºç°çš„ç”¨æˆ·ï¼ˆå³çˆ¬è¿‡ç²‰ä¸çš„ç”¨æˆ·ï¼‰
    users_with_crawled_fans = set(edges_df['source'].unique())
    
    # ğŸ¯ å…³é”®ä¿®å¤ï¼šåªåœ¨ç½‘ç»œç”¨æˆ·ä¸­æŸ¥æ‰¾å‡ºåº¦ä¸º0çš„ç”¨æˆ·
    zero_outdegree_users = network_users - users_with_crawled_fans
    
    print(f"\nğŸ” å‡ºåº¦åˆ†æç»“æœï¼ˆä»…é’ˆå¯¹çœŸå®ç½‘ç»œç”¨æˆ·ï¼‰:")
    print(f"  ğŸ“Š ç½‘ç»œæ€»ç”¨æˆ·æ•°: {len(network_users)}")
    print(f"  ğŸ“Š åœ¨edgesä¸­ä½œä¸ºsourceçš„ç”¨æˆ·æ•°: {len(users_with_crawled_fans)}")
    print(f"  ğŸ“Š åœ¨edgesä¸­ä»æœªä½œä¸ºsourceçš„ç”¨æˆ·æ•°: {len(zero_outdegree_users)}")
    print(f"  ğŸ“Š æ— å‡ºè¾¹ç”¨æˆ·æ¯”ä¾‹: {len(zero_outdegree_users)/len(network_users)*100:.1f}%")
    print(f"  âœ… è¿™äº›ç”¨æˆ·å¯èƒ½å› ä¸­æ–­ã€åçˆ¬ã€æ•…éšœæˆ–ç¡®å®æ— ç²‰ä¸è€Œç¼ºå°‘å‡ºè¾¹")
    
    return list(zero_outdegree_users)

def save_progress(processed_users, new_edges, network_path):
    """ä¿å­˜è¿›åº¦åˆ°ä¸´æ—¶æ–‡ä»¶"""
    progress_file = os.path.join(network_path, 'refind_progress.json')
    
    progress_data = {
        'processed_users': list(processed_users),
        'new_edges': new_edges,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_processed': len(processed_users),
        'total_new_edges': len(new_edges)
    }
    
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è¿›åº¦å·²ä¿å­˜: {len(processed_users)} ä¸ªç”¨æˆ·å®Œæˆ, {len(new_edges)} æ¡æ–°è¾¹")

def load_progress(network_path):
    """åŠ è½½è¿›åº¦"""
    progress_file = os.path.join(network_path, 'refind_progress.json')
    
    if not os.path.exists(progress_file):
        return set(), []
    
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress_data = json.load(f)
        
        processed_users = set(progress_data.get('processed_users', []))
        new_edges = progress_data.get('new_edges', [])
        timestamp = progress_data.get('timestamp', 'æœªçŸ¥')
        
        print(f"ğŸ“ åŠ è½½è¿›åº¦æ–‡ä»¶: {progress_file}")
        print(f"  ğŸ“Š å·²å¤„ç†ç”¨æˆ·: {len(processed_users)} ä¸ª")
        print(f"  ğŸ“Š å·²å‘ç°æ–°è¾¹: {len(new_edges)} æ¡")
        print(f"  ğŸ“Š ä¿å­˜æ—¶é—´: {timestamp}")
        
        return processed_users, new_edges
        
    except Exception as e:
        print(f"âŒ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
        return set(), []

def save_final_results(original_edges_df, new_edges, network_path):
    """ä¿å­˜æœ€ç»ˆç»“æœ"""
    if not new_edges:
        print("âœ… æ²¡æœ‰å‘ç°æ–°è¾¹ï¼Œæ— éœ€æ›´æ–°edges.csv")
        return
    
    # åˆ›å»ºæ–°è¾¹çš„DataFrame
    new_edges_df = pd.DataFrame(new_edges, columns=['source', 'target'])
    
    # åˆå¹¶åŸæœ‰è¾¹å’Œæ–°è¾¹
    updated_edges_df = pd.concat([original_edges_df, new_edges_df], ignore_index=True)
    
    # å»é‡ï¼ˆé˜²æ­¢é‡å¤è¾¹ï¼‰
    before_dedup = len(updated_edges_df)
    updated_edges_df = updated_edges_df.drop_duplicates()
    after_dedup = len(updated_edges_df)
    
    if before_dedup > after_dedup:
        print(f"âš ï¸ å»é‡: {before_dedup} â†’ {after_dedup} (-{before_dedup-after_dedup}æ¡é‡å¤è¾¹)")
    
    # ä¿å­˜å¤‡ä»½
    backup_dir = os.path.join(network_path, 'backup')
    os.makedirs(backup_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_file = os.path.join(backup_dir, f'edges_backup_{timestamp}.csv')
    original_edges_df.to_csv(backup_file, index=False, encoding='utf-8-sig')
    print(f"âœ… åŸå§‹edgeså·²å¤‡ä»½: {backup_file}")
    
    # ä¿å­˜æ›´æ–°åçš„edges
    edges_file = os.path.join(network_path, 'edges.csv')
    updated_edges_df.to_csv(edges_file, index=False, encoding='utf-8-sig')
    print(f"âœ… æ›´æ–°åçš„edgeså·²ä¿å­˜: {edges_file}")
    print(f"  ğŸ“Š åŸå§‹è¾¹æ•°: {len(original_edges_df)}")
    print(f"  ğŸ“Š æ–°å¢è¾¹æ•°: {len(new_edges)}")
    print(f"  ğŸ“Š æœ€ç»ˆè¾¹æ•°: {len(updated_edges_df)}")
    
    # ä¿å­˜æ–°è¾¹è¯¦æƒ…
    new_edges_file = os.path.join(network_path, f'new_edges_found_{timestamp}.csv')
    new_edges_df.to_csv(new_edges_file, index=False, encoding='utf-8-sig')
    print(f"âœ… æ–°å‘ç°è¾¹çš„è¯¦æƒ…: {new_edges_file}")
    
    # åˆ é™¤è¿›åº¦æ–‡ä»¶
    progress_file = os.path.join(network_path, 'refind_progress.json')
    if os.path.exists(progress_file):
        os.remove(progress_file)
        print(f"âœ… è¿›åº¦æ–‡ä»¶å·²æ¸…ç†")

def main():
    """ä¸»å‡½æ•°"""
    start_time = datetime.now()
    print(f"é—æ¼ç”¨æˆ·æŸ¥æ‰¾å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    print("å¾®åšç²‰ä¸ç½‘ç»œé—æ¼ç”¨æˆ·æŸ¥æ‰¾å™¨ v2.0 (ä¿®å¤ç‰ˆ)")
    print(f"ğŸ¯ ç›®æ ‡ç½‘ç»œ: {TARGET_NETWORK_PATH}")
    print(f"ğŸ” æŸ¥æ‰¾å¯¹è±¡: çœŸå®ç½‘ç»œä¸­å‡ºåº¦ä¸º0çš„ç”¨æˆ·ï¼ˆä»…é™edgesä¸­çš„Aã€Bã€Cç±»ç”¨æˆ·ï¼‰")
    print(f"ğŸš€ å¤„ç†æ–¹æ³•: é‡æ–°çˆ¬å–è¿™äº›ç”¨æˆ·çš„ç²‰ä¸åˆ—è¡¨ï¼Œè¡¥å……é—æ¼çš„è¾¹")
    print(f"ğŸ›¡ï¸ è¾¹è¿‡æ»¤: åªæ·»åŠ æŒ‡å‘ç½‘ç»œä¸­å·²æœ‰ç”¨æˆ·çš„è¾¹ï¼Œä¸¥æ ¼ä¿æŒç½‘ç»œè¾¹ç•Œ")
    print(f"ğŸ”„ æ–­ç‚¹ç»­ä¼ : æ”¯æŒä¸­æ–­åç»§ç»­")
    print(f"ğŸ’¾ æ•°æ®å®‰å…¨: è‡ªåŠ¨å¤‡ä»½åŸå§‹edges.csv")
    print(f"ğŸ”¥ ä¿®å¤å†…å®¹: åªåˆ†æedgesç½‘ç»œä¸­çš„ç”¨æˆ·ï¼Œä¸åŒ…å«users.csvä¸­çš„å†—ä½™Dç±»ç”¨æˆ·")
    print("=" * 80)
    
    # æ£€æŸ¥ç›®æ ‡ç½‘ç»œè·¯å¾„
    if not os.path.exists(TARGET_NETWORK_PATH):
        print(f"âŒ ç›®æ ‡ç½‘ç»œè·¯å¾„ä¸å­˜åœ¨: {TARGET_NETWORK_PATH}")
        print(f"âŒ è¯·ç¡®è®¤fetch3.pyå·²ç»å¼€å§‹å¹¶ç”Ÿæˆäº†åŸºç¡€æ•°æ®æ–‡ä»¶")
        return False
    
    # ğŸ”¥ ä¿®å¤ï¼šåªåŠ è½½edgesæ•°æ®
    print(f"\nğŸ” ç¬¬ä¸€æ­¥ï¼šåŠ è½½ç½‘ç»œæ•°æ®")
    edges_df = load_network_data(TARGET_NETWORK_PATH)
    
    if edges_df is None:
        print(f"âŒ æ— æ³•åŠ è½½ç½‘ç»œæ•°æ®")
        return False
    
    # ğŸ”¥ ä¿®å¤ï¼šä»edgesä¸­è·å–çœŸå®ç½‘ç»œç”¨æˆ·
    print(f"\nğŸ” ç¬¬äºŒæ­¥ï¼šè¯†åˆ«çœŸå®ç½‘ç»œç”¨æˆ·")
    network_users = get_network_users_from_edges(edges_df)
    
    # ğŸ”¥ ä¿®å¤ï¼šåªåœ¨çœŸå®ç½‘ç»œç”¨æˆ·ä¸­æ‰¾å‡ºåº¦ä¸º0çš„ç”¨æˆ·
    print(f"\nğŸ” ç¬¬ä¸‰æ­¥ï¼šåˆ†æçœŸå®ç½‘ç»œä¸­çš„å‡ºåº¦ä¸º0ç”¨æˆ·")
    zero_outdegree_users = find_zero_outdegree_users_in_network(edges_df, network_users)
    
    if not zero_outdegree_users:
        print(f"âœ… çœŸå®ç½‘ç»œä¸­æ‰€æœ‰ç”¨æˆ·éƒ½å·²çˆ¬è¿‡ç²‰ä¸ï¼Œæ— éœ€æŸ¥æ‰¾é—æ¼ç”¨æˆ·")
        return True
    
    # åŠ è½½è¿›åº¦
    print(f"\nğŸ”„ ç¬¬å››æ­¥ï¼šæ£€æŸ¥è¿›åº¦çŠ¶æ€")
    processed_users, new_edges = load_progress(TARGET_NETWORK_PATH)
    
    # è®¡ç®—éœ€è¦å¤„ç†çš„ç”¨æˆ·
    users_to_process = set(zero_outdegree_users) - processed_users
    
    print(f"\nğŸ“‹ ç¬¬äº”æ­¥ï¼šè®¡ç®—å¤„ç†è®¡åˆ’")
    print(f"  ğŸ“Š çœŸå®ç½‘ç»œç”¨æˆ·æ€»æ•°: {len(network_users)}")
    print(f"  ğŸ“Š å‡ºåº¦ä¸º0çš„ç”¨æˆ·æ€»æ•°: {len(zero_outdegree_users)}")
    print(f"  ğŸ“Š å·²å¤„ç†ç”¨æˆ·æ•°: {len(processed_users)}")
    print(f"  ğŸ“Š å¾…å¤„ç†ç”¨æˆ·æ•°: {len(users_to_process)}")
    print(f"  ğŸ“Š å·²å‘ç°æ–°è¾¹æ•°: {len(new_edges)}")
    print(f"  ğŸ›¡ï¸ ç½‘ç»œè¾¹ç•Œä¿æŠ¤: åªæœ‰æŒ‡å‘ç½‘ç»œä¸­ {len(network_users)} ä¸ªçœŸå®ç”¨æˆ·çš„è¾¹æ‰ä¼šè¢«æ·»åŠ ")
    
    if len(users_to_process) == 0:
        print(f"âœ… çœŸå®ç½‘ç»œä¸­æ‰€æœ‰å‡ºåº¦ä¸º0çš„ç”¨æˆ·å·²å¤„ç†å®Œæˆï¼")
        # ä¿å­˜æœ€ç»ˆç»“æœ
        save_final_results(edges_df, new_edges, TARGET_NETWORK_PATH)
        return True
    
    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    print(f"\nâš ï¸ é¢„è®¡éœ€è¦é‡æ–°çˆ¬å– {len(users_to_process)} ä¸ªç”¨æˆ·çš„ç²‰ä¸")
    print(f"âš ï¸ æŒ‰å¹³å‡æ¯ç”¨æˆ·5ç§’è®¡ç®—ï¼Œå¤§çº¦éœ€è¦ {len(users_to_process) * 5 / 60:.1f} åˆ†é’Ÿ")
    print(f"âœ… ç›¸æ¯”ä¹‹å‰çš„é”™è¯¯è®¡ç®—ï¼ˆ37ä¸‡ç”¨æˆ·ï¼‰ï¼Œç°åœ¨åªéœ€å¤„ç† {len(users_to_process)} ä¸ªçœŸå®ç½‘ç»œç”¨æˆ·")
    
    confirm = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
    if confirm != 'y':
        print("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return False
    
    # åˆå§‹åŒ–çˆ¬è™«
    print(f"\nğŸš€ ç¬¬å…­æ­¥ï¼šå¼€å§‹é‡æ–°çˆ¬å–")
    finder = WeiboMissedUsersFinder()
    
    if not finder.setup_driver():
        return False
    
    if not finder.load_cookies():
        print("è¯·å…ˆç¡®ä¿cookieæ–‡ä»¶æœ‰æ•ˆ")
        finder.cleanup()
        return False
    
    if not finder.test_login_status():
        print("ç™»å½•çŠ¶æ€æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥cookie")
        finder.cleanup()
        return False
    
    try:
        processed_count = len(processed_users)
        total_users = len(zero_outdegree_users)
        users_to_process_list = list(users_to_process)
        
        print(f"å¼€å§‹å¤„ç†å‰©ä½™çš„ {len(users_to_process_list)} ä¸ªç”¨æˆ·...")
        
        batch_start_time = datetime.now()
        consecutive_errors = 0
        new_edges_in_batch = []
        filtered_edges_count = 0  # ç»Ÿè®¡è¢«è¿‡æ»¤çš„è¾¹æ•°
        
        for i, user_id in enumerate(users_to_process_list):
            processed_count += 1
            completion = processed_count / total_users * 100
            
            print(f"\nå¤„ç†ç”¨æˆ· {user_id} [{i+1}/{len(users_to_process_list)}] (æ€»è¿›åº¦: {completion:.1f}%):")
            
            try:
                # é‡æ–°çˆ¬å–ç”¨æˆ·çš„ç²‰ä¸åˆ—è¡¨
                fans_data = finder.crawl_user_fans(user_id)
                processed_users.add(user_id)
                
                if fans_data:
                    # ğŸ›¡ï¸ è¾¹è¿‡æ»¤é€»è¾‘ï¼šåªæ·»åŠ æŒ‡å‘çœŸå®ç½‘ç»œç”¨æˆ·çš„è¾¹
                    user_new_edges = []
                    user_filtered_edges = 0
                    
                    for fan in fans_data:
                        fan_id = str(fan.get('id'))
                        if fan_id and fan_id != user_id:
                            new_edge = (user_id, fan_id)  # ç”¨æˆ·â†’ç²‰ä¸ï¼ˆä¸fetch3.pyä¿æŒä¸€è‡´ï¼‰
                            
                            # ğŸ›¡ï¸ å…³é”®ï¼šæ£€æŸ¥ç²‰ä¸æ˜¯å¦å±äºçœŸå®ç½‘ç»œä¸­çš„ç”¨æˆ·
                            if fan_id not in network_users:
                                user_filtered_edges += 1
                                filtered_edges_count += 1
                                continue  # è·³è¿‡ï¼šç²‰ä¸ä¸åœ¨çœŸå®ç½‘ç»œä¸­ï¼Œå¯èƒ½æ˜¯Dç±»æˆ–æ›´è¿œçš„ç”¨æˆ·
                            
                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆåœ¨åŸå§‹è¾¹æˆ–æ–°è¾¹ä¸­ï¼‰
                            edge_exists = False
                            
                            # æ£€æŸ¥åŸå§‹è¾¹
                            if not edges_df[(edges_df['source'] == user_id) & (edges_df['target'] == fan_id)].empty:
                                edge_exists = True
                            
                            # æ£€æŸ¥æ–°è¾¹
                            if not edge_exists and new_edge in new_edges:
                                edge_exists = True
                            
                            if not edge_exists:
                                new_edges.append(new_edge)
                                new_edges_in_batch.append(new_edge)
                                user_new_edges.append(new_edge)
                    
                    print(f"  âœ… ç”¨æˆ· {user_id}: å‘ç° {len(fans_data)} ä¸ªç²‰ä¸, æ–°å¢ {len(user_new_edges)} æ¡è¾¹")
                    if user_filtered_edges > 0:
                        print(f"    ğŸ›¡ï¸ å·²è¿‡æ»¤ {user_filtered_edges} æ¡æŒ‡å‘ç½‘ç»œå¤–ç”¨æˆ·çš„è¾¹")
                    
                    if len(user_new_edges) > 0:
                        print(f"    ğŸ”¥ å‘ç°é—æ¼ï¼ç”¨æˆ· {user_id} å®é™…æœ‰ {len(fans_data)} ä¸ªç²‰ä¸")
                else:
                    print(f"  âœ… ç”¨æˆ· {user_id}: ç¡®å®æ²¡æœ‰ç²‰ä¸")
                
                # é‡ç½®é”™è¯¯è®¡æ•°
                consecutive_errors = 0
                
                # æ¯å¤„ç†SAVE_INTERVALä¸ªç”¨æˆ·ä¿å­˜ä¸€æ¬¡è¿›åº¦
                if (i + 1) % SAVE_INTERVAL == 0:
                    save_progress(processed_users, new_edges, TARGET_NETWORK_PATH)
                    
                    # è®¡ç®—é€Ÿåº¦ç»Ÿè®¡
                    batch_duration = datetime.now() - batch_start_time
                    avg_time_per_user = batch_duration.total_seconds() / SAVE_INTERVAL
                    remaining_users = len(users_to_process_list) - (i + 1)
                    estimated_remaining_time = remaining_users * avg_time_per_user / 60
                    
                    print(f"  ğŸ“Š æ‰¹æ¬¡å®Œæˆ: å¹³å‡æ¯ç”¨æˆ· {avg_time_per_user:.1f} ç§’")
                    print(f"  ğŸ“Š æœ¬æ‰¹æ¬¡å‘ç°æ–°è¾¹: {len(new_edges_in_batch)} æ¡")
                    print(f"  ğŸ“Š æœ¬æ‰¹æ¬¡è¿‡æ»¤è¾¹æ•°: {filtered_edges_count} æ¡")
                    print(f"  ğŸ“Š é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining_time:.1f} åˆ†é’Ÿ")
                    
                    # é‡ç½®æ‰¹æ¬¡è®¡æ—¶
                    batch_start_time = datetime.now()
                    new_edges_in_batch.clear()
                    filtered_edges_count = 0
                
                # éšæœºç­‰å¾…
                wait_time = random.uniform(SLEEP_MIN, SLEEP_MAX)
                time.sleep(wait_time)
                
            except Exception as e:
                print(f"  âŒ ç”¨æˆ· {user_id} å¤„ç†å¤±è´¥: {e}")
                consecutive_errors += 1
                
                # å¦‚æœè¿ç»­é”™è¯¯è¿‡å¤šï¼Œå¢åŠ ç­‰å¾…æ—¶é—´
                if consecutive_errors >= 3:
                    print(f"  âš ï¸ è¿ç»­ {consecutive_errors} ä¸ªé”™è¯¯ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´...")
                    time.sleep(random.uniform(5.0, 10.0))
                
                # ä»ç„¶æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆé¿å…é‡å¤å°è¯•ï¼‰
                processed_users.add(user_id)
                
                continue
            
            # æ‰¹æ¬¡é—´ç­‰å¾…
            if i < len(users_to_process_list) - 1:
                batch_wait = random.uniform(BATCH_INTERVAL_MIN, BATCH_INTERVAL_MAX)
                time.sleep(batch_wait)
        
        # ä¿å­˜æœ€åçš„è¿›åº¦
        save_progress(processed_users, new_edges, TARGET_NETWORK_PATH)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        save_final_results(edges_df, new_edges, TARGET_NETWORK_PATH)
        
        print(f"\n" + "="*80)
        print(f"é—æ¼ç”¨æˆ·æŸ¥æ‰¾å®Œæˆï¼")
        print(f"="*80)
        print(f"âœ… å¤„ç†ç”¨æˆ·æ•°: {len(zero_outdegree_users)}")
        print(f"âœ… å‘ç°æ–°è¾¹æ•°: {len(new_edges)}")
        print(f"ğŸ›¡ï¸ æ€»è¿‡æ»¤è¾¹æ•°: {filtered_edges_count}")
        
        if len(new_edges) > 0:
            print(f"ğŸ‰ å‘ç° {len(new_edges)} æ¡é—æ¼çš„è¾¹ï¼")
            print(f"ğŸ“Š è¿™äº›è¾¹å·²è¡¥å……åˆ°edges.csvä¸­")
            print(f"ğŸ’¡ è¯´æ˜ï¼šä¹‹å‰çš„çˆ¬å–è¿‡ç¨‹ä¸­ç¡®å®å­˜åœ¨é—æ¼")
        else:
            print(f"âœ… æ²¡æœ‰å‘ç°é—æ¼çš„è¾¹")
            print(f"ğŸ’¡ è¯´æ˜ï¼šå‡ºåº¦ä¸º0çš„ç”¨æˆ·ç¡®å®æ²¡æœ‰ç²‰ä¸")
        
        if filtered_edges_count > 0:
            print(f"\nğŸ›¡ï¸ è¾¹è¿‡æ»¤ä¿æŠ¤ç»Ÿè®¡:")
            print(f"  - æ€»å…±è¿‡æ»¤äº† {filtered_edges_count} æ¡æŒ‡å‘ç½‘ç»œå¤–ç”¨æˆ·çš„è¾¹")
            print(f"  - è¿™äº›è¾¹æŒ‡å‘çš„æ˜¯Dç±»æˆ–æ›´è¿œè·ç¦»çš„ç”¨æˆ·ï¼Œæ­£ç¡®è¢«è¿‡æ»¤")
            print(f"  - ç½‘ç»œè¾¹ç•Œå¾—åˆ°ä¸¥æ ¼ä¿æŒï¼Œç¬¦åˆfetch3.pyçš„è®¾è®¡åŸåˆ™")
        
        # ç»Ÿè®¡è¢«æ¢å¤çš„ç”¨æˆ·
        recovered_users = set()
        for source, target in new_edges:
            recovered_users.add(source)
        
        if recovered_users:
            print(f"\nğŸ“Š è¢«æ¢å¤çš„ç”¨æˆ·ç»Ÿè®¡:")
            print(f"  - æ€»è®¡ {len(recovered_users)} ä¸ªç”¨æˆ·è¢«æ¢å¤äº†ç²‰ä¸")
            print(f"  - è¿™äº›ç”¨æˆ·ä¹‹å‰è¢«é”™è¯¯åœ°æ ‡è®°ä¸ºæ— ç²‰ä¸")
            
            # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
            sample_users = list(recovered_users)[:5]
            for user_id in sample_users:
                user_new_edges = [(s, t) for s, t in new_edges if s == user_id]
                print(f"    ç”¨æˆ· {user_id}: æ¢å¤äº† {len(user_new_edges)} ä¸ªç²‰ä¸")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        # ä¿å­˜å½“å‰è¿›åº¦
        save_progress(processed_users, new_edges, TARGET_NETWORK_PATH)
        print(f"âœ… å½“å‰è¿›åº¦å·²ä¿å­˜ï¼Œå¯ç¨åç»§ç»­")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        # ä¿å­˜å½“å‰è¿›åº¦
        save_progress(processed_users, new_edges, TARGET_NETWORK_PATH)
        
    finally:
        finder.cleanup()
    
    end_time = datetime.now()
    duration = end_time - start_time
    print(f"\næ€»è€—æ—¶: {duration}")

if __name__ == "__main__":
    main()