import os
import json
import time
import random
import requests
import pandas as pd
import signal
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from datetime import datetime
import re

# åŸºæœ¬é…ç½®
BASE_OUTPUT_DIR = 'C:/Tengfei/data/data/topic_networks'
COOKIE_PATH = 'C:/Tengfei/data/crawler/crawler_for_weibo_fans-master/cookie.json'

# ç¿»é¡µé™åˆ¶ï¼ˆweibo.cnï¼‰
MAX_PAGES_PER_USER = 20

# é€Ÿåº¦å‚æ•°ï¼ˆä¸fetch4ä¸€è‡´ï¼‰
SLEEP_MIN = 0.4
SLEEP_MAX = 0.6
BATCH_INTERVAL_MIN = 0.5
BATCH_INTERVAL_MAX = 1.0

# è¿‘åæ¡å½±å“åŠ›
MAX_POSTS_FOR_POPULARITY = 10

# è¿è¡Œæ€
should_exit = False

def signal_handler(signum, frame):
    global should_exit
    print("\nâš ï¸ æ•è·åˆ°ä¸­æ–­ä¿¡å·ï¼Œå‡†å¤‡å®‰å…¨é€€å‡º...")
    should_exit = True

class TagAdderCrawler:
    def __init__(self):
        self.driver_com = None  # ç”¨äº weibo.com è·å–ç²‰ä¸æ•°ï¼ˆé˜ˆå€¼è¿‡æ»¤ï¼‰
        self.driver_cn = None   # ç”¨äº weibo.cn ç²‰ä¸/å…³æ³¨/å½±å“åŠ›
        self.existing_nodes = set()
        self.edges_data = []    # ç°æœ‰ç½‘ç»œçš„è¾¹ï¼ˆç”¨äºè¿½åŠ åå†™å›ï¼‰
        self.edges_set = set()
        self.popularity_map = {}  # ç°æœ‰ popularity æ˜ å°„ user_id -> avg_popularity
        self.users_df = None      # ç°æœ‰ users.csvï¼ˆè‹¥å­˜åœ¨ï¼‰
        self.network_dir = None

    def setup_drivers(self):
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        try:
            # weibo.comï¼ˆä»…ç”¨äºç²‰ä¸é˜ˆå€¼ï¼‰
            self.driver_com = webdriver.Chrome(options=chrome_options)
            self.driver_com.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            # weibo.cnï¼ˆç²‰ä¸/å…³æ³¨/å½±å“åŠ›ï¼‰
            chrome_options_cn = Options()
            chrome_options_cn.add_argument('--no-sandbox')
            chrome_options_cn.add_argument('--disable-dev-shm-usage')
            chrome_options_cn.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options_cn.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options_cn.add_argument('--user-data-dir=C:/temp/chrome_profile_cn')
            self.driver_cn = webdriver.Chrome(options=chrome_options_cn)
            self.driver_cn.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            return True
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def load_cookies_cn(self):
        try:
            with open(COOKIE_PATH, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            self.driver_cn.get('https://weibo.cn')
            time.sleep(1.5)
            for cookie in cookies:
                try:
                    self.driver_cn.add_cookie(cookie)
                except:
                    pass
            self.driver_cn.refresh()
            time.sleep(1.0)
            return True
        except Exception as e:
            print(f"âŒ CookieåŠ è½½å¤±è´¥: {e}")
            return False

    def extract_users_from_search(self, html_content, seen):
        """ä»s.weibo.comçš„HTMLä¸­æå–ç”¨æˆ·ID"""
        user_ids = []
        patterns = [
            r'href="//weibo\.com/(\d+)/[^"]*"',
            r'href="https?://weibo\.com/(\d+)/[^"]*"',
            r'href="//weibo\.com/u/(\d+)[^"]*"',
            r'href="https?://weibo\.com/u/(\d+)[^"]*"',
            r'"idstr":"(\d+)"',
            r'"user":\s*{\s*"id":(\d+)',
        ]
        for pattern in patterns:
            for m in re.findall(pattern, html_content):
                if m.isdigit() and len(m) >= 6 and m not in seen:
                    user_ids.append(m)
        # å»é‡
        out, page_seen = [], set()
        for uid in user_ids:
            if uid not in page_seen:
                out.append(uid)
                page_seen.add(uid)
        return out

    def check_user_fans_count(self, user_id):
        """weibo.comæ¡£æ¡ˆé¡µè§£æç²‰ä¸æ•°ï¼ˆç”¨äºé˜ˆå€¼è¿‡æ»¤ï¼‰"""
        try:
            self.driver_com.get(f'https://weibo.com/u/{user_id}')
            time.sleep(random.uniform(1.0, 1.5))
            html = self.driver_com.page_source
            patterns = [
                r'<span[^>]*>([0-9]+\.?[0-9]*[ä¸‡]?)</span>\s*ç²‰ä¸',
                r'>([0-9]+\.?[0-9]*[ä¸‡]?)</span>\s*ç²‰ä¸',
                r'([0-9]+\.?[0-9]*[ä¸‡]?)\s*ç²‰ä¸',
                r'ç²‰ä¸[^>]*>([0-9]+\.?[0-9]*[ä¸‡]?)',
            ]
            for p in patterns:
                ms = re.findall(p, html)
                if ms:
                    s = ms[0].strip()
                    if 'ä¸‡' in s:
                        return int(float(s.replace('ä¸‡', '')) * 10000)
                    return int(float(s))
        except:
            pass
        return 0

    def calculate_user_popularity(self, user_id, max_posts=MAX_POSTS_FOR_POPULARITY):
        """weibo.cnä¸Šè®¡ç®—è¿‘10æ¡å¹³å‡è½¬èµè¯„"""
        try:
            self.driver_cn.get(f'https://weibo.cn/u/{user_id}')
            time.sleep(1.0)
            weibo_divs = self.driver_cn.find_elements(By.XPATH, "//div[@class='c' and contains(@id, 'M_')]")
            if not weibo_divs:
                return 0.0
            def extract_interactions(html_text):
                res = {'reposts': 0, 'likes': 0, 'comments': 0}
                pats = {'likes': [r'èµ\[(\d+)\]'], 'reposts': [r'è½¬å‘\[(\d+)\]'], 'comments': [r'è¯„è®º\[(\d+)\]']}
                for k, ps in pats.items():
                    for p in ps:
                        ms = re.findall(p, html_text)
                        if ms:
                            try:
                                res[k] = int(ms[-1])
                            except:
                                pass
                return res
            posts = []
            for div in weibo_divs:
                if len(posts) >= max_posts:
                    break
                try:
                    last_div = div.find_elements(By.XPATH, "./div")[-1] if div.find_elements(By.XPATH, "./div") else div
                    interactions = extract_interactions(last_div.get_attribute('outerHTML'))
                    posts.append(interactions)
                except:
                    continue
            if not posts:
                return 0.0
            tot = sum(p['likes'] + p['reposts'] + p['comments'] for p in posts)
            return tot / len(posts)
        except:
            return 0.0

    def crawl_cn_ids(self, url_template):
        """é€šç”¨ weibo.cn ç¿»é¡µæŠ“å– /fans æˆ– /follow çš„ç”¨æˆ·IDé›†åˆ"""
        try:
            ids = []
            consecutive_empty = 0
            for page in range(1, MAX_PAGES_PER_USER + 1):
                url = f"{url_template}?page={page}" if page > 1 else url_template
                self.driver_cn.get(url)
                time.sleep(random.uniform(0.5, 1.0))
                elems = self.driver_cn.find_elements(By.XPATH, "//a[contains(@href, '/u/')]")
                page_ids, seen = [], set()
                for a in elems:
                    try:
                        href = a.get_attribute('href')
                        name = a.text.strip()
                        if not href or '/u/' not in href or not name:
                            continue
                        uid = href.split('/u/')[-1].split('?')[0].split('/')[0]
                        if uid.isdigit() and uid not in seen:
                            page_ids.append(uid); seen.add(uid)
                    except:
                        continue
                if not page_ids:
                    consecutive_empty += 1
                    if consecutive_empty >= 2:
                        break
                else:
                    consecutive_empty = 0
                    ids.extend(page_ids)
                if should_exit:
                    break
            return set(ids)
        except:
            return set()

    def crawl_user_fans_ids_cn(self, user_id):
        return self.crawl_cn_ids(f'https://weibo.cn/{user_id}/fans')

    def crawl_user_following_ids_cn(self, user_id):
        return self.crawl_cn_ids(f'https://weibo.cn/{user_id}/follow')

    def cleanup(self):
        try:
            if self.driver_com:
                self.driver_com.quit()
        except Exception as e:
            print(f"å…³é—­weibo.comæµè§ˆå™¨å‡ºé”™: {e}")
        try:
            if self.driver_cn:
                self.driver_cn.quit()
        except Exception as e:
            print(f"å…³é—­weibo.cnæµè§ˆå™¨å‡ºé”™: {e}")

def prompt_keywords_targets(max_items=20):
    items = []
    print("\nè¯·è¾“å…¥æœ€å¤š20ä¸ªå…³é”®è¯åŠç›®æ ‡äººæ•°ï¼ˆå›è½¦ç»“æŸï¼‰ï¼š")
    for i in range(1, max_items + 1):
        topic = input(f"- å…³é”®è¯{i}: ").strip()
        if not topic:
            break
        while True:
            t = input("  ç›®æ ‡äººæ•°ï¼ˆæ•´æ•°ï¼Œä¾‹å¦‚ 5000ï¼‰: ").strip()
            try:
                v = int(t)
                if v <= 0:
                    print("  è¯·è¾“å…¥æ­£æ•´æ•°"); continue
                items.append((topic, v))
                break
            except:
                print("  è¯·è¾“å…¥æœ‰æ•ˆæ•´æ•°")
    return items

def prompt_threshold():
    while True:
        t = input("è¯·è¾“å…¥ç²‰ä¸é˜ˆå€¼ï¼ˆæ•´æ•°ï¼›0è¡¨ç¤ºä¸è®¾ï¼‰: ").strip()
        try:
            v = int(t)
            if v < 0:
                print("è¯·è¾“å…¥â‰¥0çš„æ•´æ•°"); continue
            return v
        except:
            print("è¯·è¾“å…¥æœ‰æ•ˆæ•´æ•°")

def prompt_existing_network_dir():
    while True:
        d = input("è¯·è¾“å…¥ç°æœ‰ç½‘ç»œç›®å½•ï¼ˆä¾‹å¦‚ C:/Tengfei/data/data/topic_networks/topic_combinedï¼‰: ").strip()
        if os.path.isdir(d):
            return d
        print("ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")

def collect_topic_users(crawler: TagAdderCrawler, topic: str, target: int, threshold: int):
    """ä» s.weibo.com æŒ‰tagæ”¶é›†ç”¨æˆ·IDï¼Œåº”ç”¨ç²‰ä¸é˜ˆå€¼ï¼ˆ0ä¸è®¾ï¼‰"""
    topic_encoded = requests.utils.quote(f"#{topic}#")
    page = 1
    seen = set()
    picked = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8',
        'Referer': 'https://weibo.com/',
        'Connection': 'keep-alive'
    }
    cookies = {}
    try:
        with open(COOKIE_PATH, 'r', encoding='utf-8') as f:
            cl = json.load(f)
        for c in cl:
            cookies[c['name']] = c['value']
    except:
        pass

    consecutive_empty = 0
    max_consecutive_empty = 3

    while len(picked) < target and not should_exit:
        page_url = f"https://s.weibo.com/weibo/{topic_encoded}" if page == 1 else f"https://s.weibo.com/weibo/{topic_encoded}&page={page}"
        print(f"  - è¯·æ±‚ç¬¬{page}é¡µ: {page_url}")
        try:
            time.sleep(random.uniform(1.0, 1.5))
            resp = requests.get(page_url, headers=headers, cookies=cookies, timeout=15)
            if resp.status_code != 200:
                print(f"    çŠ¶æ€ç  {resp.status_code}ï¼Œé‡è¯•ä¸‹ä¸€é¡µ")
                consecutive_empty += 1
                if consecutive_empty >= max_consecutive_empty:
                    break
                page += 1
                continue

            uids = crawler.extract_users_from_search(resp.text, seen)
            if not uids:
                consecutive_empty += 1
                print(f"    ç©ºé¡µï¼ˆè¿ç»­{consecutive_empty}ï¼‰")
                if consecutive_empty >= max_consecutive_empty:
                    break
                page += 1
                continue
            consecutive_empty = 0

            for uid in uids:
                if uid in seen:
                    continue
                seen.add(uid)
                # é˜ˆå€¼è¿‡æ»¤ï¼ˆ0è¡¨ç¤ºä¸è®¾ï¼‰
                fans = crawler.check_user_fans_count(uid) if threshold > 0 else 0
                if threshold > 0 and fans > threshold:
                    print(f"    è·³è¿‡ç”¨æˆ· {uid}ï¼ˆç²‰ä¸ {fans} > é˜ˆå€¼ {threshold}ï¼‰")
                    continue
                picked.append(uid)
                print(f"    [+] æ”¶å½•ç”¨æˆ· {uid}ï¼ˆç²‰ä¸ {fans if threshold>0 else 'N/A'}ï¼‰ | {len(picked)}/{target}")
                time.sleep(random.uniform(0.3, 0.6))
                if len(picked) >= target or should_exit:
                    break

            page += 1
            if page > 200:
                print("    è¾¾åˆ°æœ€å¤§é¡µæ•°200ï¼Œåœæ­¢ç¿»é¡µ")
                break
        except Exception as e:
            print(f"    è¯·æ±‚å¼‚å¸¸: {e}")
            break

    return picked

def load_existing_network(crawler: TagAdderCrawler, network_dir: str):
    crawler.network_dir = network_dir
    edges_path = os.path.join(network_dir, 'edges.csv')
    users_path = os.path.join(network_dir, 'users.csv')
    popularity_path = os.path.join(network_dir, 'popularity.csv')

    if not os.path.exists(edges_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°edges.csv: {edges_path}")

    edges_df = pd.read_csv(edges_path)
    crawler.edges_data = []
    crawler.edges_set = set()
    for _, row in edges_df.iterrows():
        s = str(row['source']); t = str(row['target'])
        crawler.edges_data.append((s, t))
        crawler.edges_set.add((s, t))

    existing_nodes = set(edges_df['source'].astype(str)).union(set(edges_df['target'].astype(str)))
    if os.path.exists(users_path):
        users_df = pd.read_csv(users_path)
        users_df['user_id'] = users_df['user_id'].astype(str)
        crawler.users_df = users_df
        existing_nodes |= set(users_df['user_id'].astype(str))
    crawler.existing_nodes = existing_nodes

    crawler.popularity_map = {}
    if os.path.exists(popularity_path):
        pop_df = pd.read_csv(popularity_path)
        if 'user_id' in pop_df.columns and 'avg_popularity' in pop_df.columns:
            for _, r in pop_df.iterrows():
                crawler.popularity_map[str(r['user_id'])] = float(r['avg_popularity'])

    print(f"ğŸ“¥ å·²åŠ è½½ç°æœ‰ç½‘ç»œ: èŠ‚ç‚¹â‰ˆ{len(existing_nodes)}ï¼Œè¾¹ {len(crawler.edges_data)}")

def save_new_tag_users_map(mapping: dict, output_dir: str):
    os.makedirs(output_dir, exist_ok=True)
    out_path = os.path.join(output_dir, 'new_tag_users.json')
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ å·²ä¿å­˜æ–°æ ‡ç­¾ç”¨æˆ·æ˜ å°„: {out_path}")

def write_back_network(crawler: TagAdderCrawler):
    """å°†è¿½åŠ åçš„è¾¹ä¸äººæ°”å†™å›ç°æœ‰ç½‘ç»œç›®å½•"""
    edges_df = pd.DataFrame(crawler.edges_data, columns=['source', 'target'])
    edges_path = os.path.join(crawler.network_dir, 'edges.csv')
    edges_df.to_csv(edges_path, index=False, encoding='utf-8-sig')

    # popularity.csv è¿½åŠ /è¦†ç›–ï¼ˆä»¥mapä¸ºå‡†ï¼‰
    pop_items = sorted(crawler.popularity_map.items(), key=lambda x: x[0])
    pop_df = pd.DataFrame(pop_items, columns=['user_id', 'avg_popularity'])
    pop_path = os.path.join(crawler.network_dir, 'popularity.csv')
    pop_df.to_csv(pop_path, index=False, encoding='utf-8-sig')

    print(f"ğŸ’¾ å·²å†™å› edges.csv ä¸ popularity.csv åˆ°: {crawler.network_dir}")
    print(f"   è¾¹æ€»æ•°: {len(crawler.edges_data)}ï¼Œå½±å“åŠ›ç”¨æˆ·æ•°: {len(crawler.popularity_map)}")

def integrate_new_users_to_network(crawler: TagAdderCrawler, new_user_ids: list):
    """ä¸ºæ–°äººåœ¨ç°ç½‘ä¸­è¡¥å……â€œç²‰ä¸è¾¹ï¼šè¢«å…³æ³¨è€…â†’ç²‰ä¸ï¼ˆåšä¸»â†’ç²‰ä¸ï¼‰â€ï¼Œå¹¶æŒ‰éœ€å†™å…¥å½±å“åŠ›"""
    added_nodes = 0
    edges_added = 0
    processed = 0

    for uid in new_user_ids:
        if should_exit:
            break
        processed += 1
        print(f"\n[æ–°äºº {processed}/{len(new_user_ids)}] å¤„ç† {uid} ...")

        # æŠ“å–ç²‰ä¸/å…³æ³¨é›†åˆï¼ˆweibo.cnï¼‰
        fans_ids = crawler.crawl_user_fans_ids_cn(uid)          # ç²‰ä¸é›†åˆï¼šç°ç½‘ç²‰ä¸ u å‘½ä¸­ => (uid, u)
        follow_ids = crawler.crawl_user_following_ids_cn(uid)   # å…³æ³¨é›†åˆï¼šæ–°äººå…³æ³¨ç°ç½‘ v => (v, uid)

        # ä¸ç°ç½‘èŠ‚ç‚¹æ±‚äº¤
        fans_in_existing = fans_ids & crawler.existing_nodes
        follow_in_existing = follow_ids & crawler.existing_nodes

        # ç²‰ä¸è¾¹æ–¹å‘ï¼šè¢«å…³æ³¨è€…â†’ç²‰ä¸ï¼ˆåšä¸»â†’ç²‰ä¸ï¼‰
        new_edges = 0

        # æ–°äººå…³æ³¨äº†ç°ç½‘ä¸­çš„äºº vï¼švï¼ˆè¢«å…³æ³¨è€…/åšä¸»ï¼‰â†’ uidï¼ˆç²‰ä¸ï¼‰
        for v in follow_in_existing:
            e = (str(v), str(uid))
            if e not in crawler.edges_set:
                crawler.edges_data.append(e)
                crawler.edges_set.add(e)
                new_edges += 1

        # ç°ç½‘ç”¨æˆ· u æ˜¯æ–°äººçš„ç²‰ä¸ï¼šuidï¼ˆè¢«å…³æ³¨è€…/åšä¸»ï¼‰â†’ uï¼ˆç²‰ä¸ï¼‰
        for u in fans_in_existing:
            e = (str(uid), str(u))
            if e not in crawler.edges_set:
                crawler.edges_data.append(e)
                crawler.edges_set.add(e)
                new_edges += 1

        if new_edges > 0:
            # ä»…å½“æœ‰è¿è¾¹æ—¶ï¼Œè®°å½•å…¶è¿‘åæ¡å¹³å‡è½¬èµè¯„
            if str(uid) not in crawler.popularity_map:
                popularity = crawler.calculate_user_popularity(uid)
                crawler.popularity_map[str(uid)] = float(popularity)
            edges_added += new_edges
            added_nodes += 1
            print(f"  -> æ–°å¢è¾¹ {new_edges} æ¡ï¼ˆå…³æ³¨å‘½ä¸­ {len(follow_in_existing)}ï¼Œç²‰ä¸å‘½ä¸­ {len(fans_in_existing)}ï¼‰")
        else:
            print("  -> æœªå‘½ä¸­ä»»ä½•ç°ç½‘è¿æ¥ï¼Œè·³è¿‡åŠ å…¥ç½‘ç»œï¼ˆä¸å†™å…¥å½±å“åŠ›ï¼‰")

        # è½»é‡èŠ‚æµ
        time.sleep(random.uniform(0.5, 1.0))

    print(f"\nâœ… æ–°äººæ•´åˆå®Œæˆï¼šåŠ å…¥ç½‘ç»œçš„æ–°äºº {added_nodes} äººï¼Œæ–°å¢è¾¹ {edges_added} æ¡")

def main():
    signal.signal(signal.SIGINT, signal_handler)

    print("fetch4_adderï¼šæŒ‰tagæ‰¹é‡æ‰¾äººå¹¶æ‰©å……ç°æœ‰ç½‘ç»œï¼ˆç²‰ä¸è¾¹ï¼šè¢«å…³æ³¨è€…â†’ç²‰ä¸ï¼Œåšä¸»â†’ç²‰ä¸ï¼‰")
    crawler = TagAdderCrawler()
    if not crawler.setup_drivers():
        return
    if not crawler.load_cookies_cn():
        crawler.cleanup()
        return

    try:
        # 1) å…³é”®è¯ä¸äººæ•°
        items = prompt_keywords_targets(max_items=20)
        if not items:
            print("æœªè¾“å…¥å…³é”®è¯ï¼Œç¨‹åºç»“æŸã€‚")
            return

        # 2) ç²‰ä¸é˜ˆå€¼
        threshold = prompt_threshold()
        print(f"ç²‰ä¸é˜ˆå€¼: {threshold}ï¼ˆ0è¡¨ç¤ºä¸è®¾ï¼‰")

        # 3) é€å…³é”®è¯æ”¶é›†ç¬¦åˆé˜ˆå€¼çš„tagç”¨æˆ·
        topic_users_map = {}
        for topic, target in items:
            if should_exit:
                break
            print(f"\n=== å…³é”®è¯ #{topic}# | ç›®æ ‡äººæ•°: {target} ===")
            users = collect_topic_users(crawler, topic, target, threshold)
            topic_users_map[topic] = users
            print(f"å…³é”®è¯ #{topic}# æ”¶é›†å®Œæˆï¼š{len(users)}/{target}")

        # 4) è¾“å‡ºä¸€ä¸ªæ˜ å°„è¡¨ï¼ˆå•æ–‡ä»¶ï¼‰
        adder_dir = os.path.join(BASE_OUTPUT_DIR, 'topic_adder')
        save_new_tag_users_map(topic_users_map, adder_dir)

        if should_exit:
            print("âš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜æ ‡ç­¾ç”¨æˆ·æ˜ å°„ï¼Œæœªè¿›è¡Œç½‘ç»œæ•´åˆã€‚")
            return

        # 5) è½½å…¥ç°æœ‰ç½‘ç»œï¼Œç„¶åæ•´åˆæ–°äººï¼ˆweibo.cn ç²‰ä¸/å…³æ³¨ï¼Œä¸¤å‘å‘½ä¸­åˆ™åŠ è¾¹ï¼‰
        network_dir = prompt_existing_network_dir()
        load_existing_network(crawler, network_dir)

        # å°†æ‰€æœ‰å…³é”®è¯å¾—åˆ°çš„æ–°äººåˆå¹¶å»é‡
        all_new_users = set()
        for arr in topic_users_map.values():
            all_new_users.update([str(u) for u in arr])

        # æ•´åˆæ–°äººå¹¶å†™å›ç½‘ç»œ
        integrate_new_users_to_network(crawler, list(all_new_users))
        write_back_network(crawler)

        print("\nğŸ‰ å…¨éƒ¨å®Œæˆã€‚")
    except KeyboardInterrupt:
        print("\nâš ï¸ ä¸­æ–­ï¼Œå·²å°½åŠ›ä¿å­˜å½“å‰çŠ¶æ€ã€‚")
    finally:
        crawler.cleanup()

if __name__ == "__main__":
    main()