import os
import json
import time
import random
import requests
import pandas as pd
import signal
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from datetime import datetime
import re

# é…ç½®å‚æ•°ï¼ˆä¸fetch4ä¿æŒä¸€è‡´ï¼‰
BASE_OUTPUT_DIR = 'C:/Tengfei/data/data/topic_networks'
COOKIE_PATH = 'C:/Tengfei/data/crawler/crawler_for_weibo_fans-master/cookie.json'

# ç¿»é¡µä¸ç²‰ä¸é¡µè®¾ç½®
MAX_PAGES_PER_USER = 20

# é€Ÿåº¦å‚æ•°ï¼ˆä¸fetch4ä¸€è‡´ï¼‰
SLEEP_MIN = 0.4
SLEEP_MAX = 0.6
BATCH_INTERVAL_MIN = 0.5
BATCH_INTERVAL_MAX = 1.0

# æµè¡Œåº¦è®¡ç®—å‚æ•°
MAX_POSTS_FOR_POPULARITY = 10

# å…¨å±€æ•°æ®ï¼ˆä»…Aç±»ï¼‰
node_categories = {"A": set()}
edges_data = []
edges_set = set()
users_data = {}
popularity_data = {}

# è¿è¡Œæ€
crawler = None
output_dir = None
should_exit = False

# è¿›åº¦/è®¡åˆ’
topics_processed = []    # å·²å¤„ç†å…³é”®è¯ï¼ˆç”¨äºinfoï¼‰
topic_nodes_map = {}     # æ¯ä¸ªå…³é”®è¯çš„ç”¨æˆ·é›†åˆï¼ˆç”¨äºäººæ•°åˆ¤æ–­ï¼ŒæŒä¹…åŒ–ï¼‰
topic_plan = []          # [{topic, target, finished_first_phase, count_A, count_total}]
processed_users = set()  # ç¬¬äºŒé˜¶æ®µå·²å¤„ç†è¿‡çš„Aç±»ç”¨æˆ·ï¼ˆæŒä¹…åŒ–ï¼‰
run_config = {           # è¿è¡Œé…ç½®ï¼ˆæŒä¹…åŒ–ï¼‰
    "high_fans_threshold": 0
}

def signal_handler(signum, frame):
    """å¤„ç†Ctrl+Cä¿¡å·"""
    global should_exit
    print("\nâš ï¸ æ•è·åˆ°ä¸­æ–­ä¿¡å·ï¼Œå‡†å¤‡å®‰å…¨é€€å‡º...")
    should_exit = True
    if output_dir:
        save_progress(output_dir)
    if crawler:
        crawler.cleanup()
    print("âœ… è¿›åº¦å·²ä¿å­˜ï¼Œèµ„æºå·²é‡Šæ”¾ã€‚")

class TopicNetworkCrawler:
    def __init__(self):
        self.driver_com = None  # ç”¨äº weibo.com è·å–ç²‰ä¸æ•°ï¼ˆAé˜ˆå€¼è¿‡æ»¤ï¼‰
        self.driver_cn = None   # ç”¨äº weibo.cn çˆ¬ç²‰ä¸é¡µ

    def setup_drivers(self):
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])

        try:
            # weibo.com é©±åŠ¨ï¼ˆç”¨äºè·å–Aç±»ç²‰ä¸æ•°ï¼‰
            self.driver_com = webdriver.Chrome(options=chrome_options)
            self.driver_com.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            # weibo.cn é©±åŠ¨ï¼ˆç”¨äºç²‰ä¸é¡µï¼‰
            chrome_options_cn = Options()
            chrome_options_cn.add_argument('--no-sandbox')
            chrome_options_cn.add_argument('--disable-dev-shm-usage')
            chrome_options_cn.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options_cn.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options_cn.add_argument('--user-data-dir=C:/temp/chrome_profile_cn')

            self.driver_cn = webdriver.Chrome(options=chrome_options_cn)
            self.driver_cn.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            return True
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥: {e}")
            return False

    def load_cookies_cn(self):
        try:
            with open(COOKIE_PATH, 'r', encoding='utf-8') as f:
                cookies = json.load(f)

            self.driver_cn.get('https://weibo.cn')
            time.sleep(2)

            for cookie in cookies:
                try:
                    self.driver_cn.add_cookie(cookie)
                except:
                    pass

            self.driver_cn.refresh()
            time.sleep(2)
            return True
        except Exception as e:
            print(f"âŒ CookieåŠ è½½å¤±è´¥: {e}")
            return False

    def extract_users_from_page(self, html_content, seen_users):
        """ä»HTMLé¡µé¢æå–ç”¨æˆ·IDï¼ˆä¸fetch4ä¸€è‡´ï¼‰"""
        user_ids = []
        patterns = [
            r'href="//weibo\.com/(\d+)/[^"]*"',
            r'href="https?://weibo\.com/(\d+)/[^"]*"',
            r'href="//weibo\.com/u/(\d+)[^"]*"',
            r'href="https?://weibo\.com/u/(\d+)[^"]*"',
            r'"idstr":"(\d+)"',
            r'"user":\s*{\s*"id":(\d+)',
        ]
        for pattern in patterns:
            matches = re.findall(pattern, html_content)
            for match in matches:
                if len(match) >= 8 and match.isdigit() and match not in seen_users:
                    user_ids.append(match)

        # å»é‡
        unique_users = []
        page_seen = set()
        for uid in user_ids:
            if uid not in page_seen:
                unique_users.append(uid)
                page_seen.add(uid)
        return unique_users

    def check_user_fans_count(self, user_id):
        """æ£€æŸ¥ç”¨æˆ·ç²‰ä¸æ•° - åŸºäºweibo.comé¡µé¢ï¼ˆç”¨äºAç±»é˜ˆå€¼è¿‡æ»¤ï¼‰"""
        try:
            profile_url = f'https://weibo.com/u/{user_id}'
            self.driver_com.get(profile_url)
            time.sleep(random.uniform(0.5, 1.0))
            page_source = self.driver_com.page_source

            patterns = [
                r'<span[^>]*>([0-9]+\.?[0-9]*[ä¸‡]?)</span>\s*ç²‰ä¸',
                r'>([0-9]+\.?[0-9]*[ä¸‡]?)</span>\s*ç²‰ä¸',
                r'([0-9]+\.?[0-9]*[ä¸‡]?)\s*ç²‰ä¸',
                r'ç²‰ä¸[^>]*>([0-9]+\.?[0-9]*[ä¸‡]?)',
            ]
            for pattern in patterns:
                matches = re.findall(pattern, page_source)
                if matches:
                    fans_str = matches[0].strip()
                    if 'ä¸‡' in fans_str:
                        num_str = fans_str.replace('ä¸‡', '')
                        fans_count = int(float(num_str) * 10000)
                    else:
                        fans_count = int(float(fans_str))
                    return fans_count
        except:
            pass
        return 0

    def crawl_user_fans_cn(self, user_id):
        """çˆ¬å–ç”¨æˆ·ç²‰ä¸ï¼ˆweibo.cnï¼‰ï¼Œä»…è§£æç²‰ä¸IDä¸æ˜µç§°ï¼ˆç”¨äºç¬¬äºŒé˜¶æ®µè¡¥è¾¹ï¼‰"""
        try:
            fans_url = f'https://weibo.cn/{user_id}/fans'
            self.driver_cn.get(fans_url)
            time.sleep(0.5)

            page_source = self.driver_cn.page_source
            if 'ç”¨æˆ·ä¸å­˜åœ¨' in page_source or 'ç™»å½•' in page_source:
                return []

            fans_data = []
            consecutive_empty_pages = 0

            for page in range(1, MAX_PAGES_PER_USER + 1):
                if page > 1:
                    try:
                        next_page_url = f'https://weibo.cn/{user_id}/fans?page={page}'
                        self.driver_cn.get(next_page_url)
                        time.sleep(random.uniform(0.5, 1.0))
                    except:
                        break

                try:
                    fan_elements = self.driver_cn.find_elements(By.XPATH, "//a[contains(@href, '/u/')]")
                    page_fans = []
                    processed_ids = set()

                    for element in fan_elements:
                        try:
                            fan_href = element.get_attribute('href')
                            fan_name = element.text.strip()
                            if not fan_href or '/u/' not in fan_href or not fan_name:
                                continue
                            fan_id = fan_href.split('/u/')[-1].split('?')[0].split('/')[0]
                            if not (fan_id.isdigit() and fan_id not in processed_ids):
                                continue
                            page_fans.append({
                                'id': fan_id,
                                'screen_name': fan_name,
                            })
                            processed_ids.add(fan_id)
                        except:
                            continue

                    if len(page_fans) == 0:
                        consecutive_empty_pages += 1
                        if consecutive_empty_pages >= 2:
                            break
                    else:
                        consecutive_empty_pages = 0
                        fans_data.extend(page_fans)

                except:
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= 2:
                        break

                time.sleep(random.uniform(0.5, 1.0))

            return fans_data
        except:
            return []

    def calculate_user_popularity(self, user_id, max_posts=MAX_POSTS_FOR_POPULARITY):
        """è®¡ç®—ç”¨æˆ·è¿‘10æ¡å¾®åšçš„å¹³å‡è½¬èµè¯„ï¼ˆä¸fetch4ä¸€è‡´ï¼Œå¯é€‰ï¼‰"""
        try:
            profile_url = f'https://weibo.cn/u/{user_id}'
            self.driver_cn.get(profile_url)
            time.sleep(1)
            weibo_divs = self.driver_cn.find_elements(By.XPATH, "//div[@class='c' and contains(@id, 'M_')]")
            if not weibo_divs:
                return 0.0

            posts_data = []
            for i, weibo_div in enumerate(weibo_divs):
                if len(posts_data) >= max_posts:
                    break
                post_data = self.process_single_weibo_div(weibo_div)
                if post_data and post_data['content'] != "å†…å®¹æœªæå–":
                    posts_data.append(post_data)

            if not posts_data:
                return 0.0

            total_interactions = 0
            valid_posts = len(posts_data)
            for post in posts_data:
                interactions = post['interactions']
                post_total = interactions['likes'] + interactions['reposts'] + interactions['comments']
                total_interactions += post_total

            avg_popularity = total_interactions / valid_posts if valid_posts > 0 else 0.0
            return avg_popularity
        except:
            return 0.0

    def process_single_weibo_div(self, weibo_div_element):
        """å¤„ç†å•ä¸ªå¾®åšdivï¼ˆä¸fetch4ä¸€è‡´ï¼‰"""
        try:
            div_html = weibo_div_element.get_attribute('outerHTML')
            child_divs = weibo_div_element.find_elements(By.XPATH, "./div")

            content = "å†…å®¹æœªæå–"
            try:
                ctt_element = weibo_div_element.find_element(By.CLASS_NAME, "ctt")
                content = ctt_element.text.strip()
                content = content[:100] + ('...' if len(content) > 100 else '')
            except:
                pass

            interactions = {'reposts': 0, 'likes': 0, 'comments': 0}
            if len(child_divs) > 0:
                last_div = child_divs[-1]
                last_div_html = last_div.get_attribute('outerHTML')
                interactions = self.extract_interactions_from_html(last_div_html)
            else:
                interactions = self.extract_interactions_from_html(div_html)

            return {
                'content': content,
                'interactions': interactions,
                'total_interactions': sum(interactions.values())
            }
        except:
            return None

    def extract_interactions_from_html(self, html_text):
        """æå–è½¬èµè¯„æ•°æ®ï¼ˆä¸fetch4ä¸€è‡´ï¼‰"""
        interactions = {'reposts': 0, 'likes': 0, 'comments': 0}
        patterns = {
            'likes': [r'èµ\[(\d+)\]'],
            'reposts': [r'è½¬å‘\[(\d+)\]'],
            'comments': [r'è¯„è®º\[(\d+)\]']
        }
        for interaction_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, html_text)
                if matches:
                    try:
                        num = int(matches[-1])
                        interactions[interaction_type] = num
                    except:
                        continue
        return interactions

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.driver_com:
                self.driver_com.quit()
                self.driver_com = None
        except Exception as e:
            print(f"å…³é—­weibo.comæµè§ˆå™¨æ—¶å‡ºé”™: {e}")
        try:
            if self.driver_cn:
                self.driver_cn.quit()
                self.driver_cn = None
        except Exception as e:
            print(f"å…³é—­weibo.cnæµè§ˆå™¨æ—¶å‡ºé”™: {e}")

def ensure_topic_bucket(topic):
    """ç¡®ä¿æ¯ä¸ªå…³é”®è¯æœ‰è‡ªå·±çš„é›†åˆ"""
    if topic not in topic_nodes_map:
        topic_nodes_map[topic] = set()

def get_topic_entry(topic):
    """è·å–æˆ–åˆ›å»ºtopic_planä¸­çš„æ¡ç›®"""
    global topic_plan
    for entry in topic_plan:
        if entry.get("topic") == topic:
            return entry
    entry = {
        "topic": topic,
        "target": 0,
        "finished_first_phase": False,
        "count_A": 0,
        "count_total": 0
    }
    topic_plan.append(entry)
    return entry

def update_topic_counts(topic, finished_flag=None):
    """æ›´æ–°æŸä¸ªå…³é”®è¯çš„A/æ€»è®¡æ•°ï¼Œå¹¶å¯é€‰æ›´æ–°å®Œæˆæ ‡è®°"""
    ensure_topic_bucket(topic)
    bucket = topic_nodes_map.get(topic, set())
    entry = get_topic_entry(topic)
    count_total = len(bucket)
    count_a = sum(1 for uid in bucket if uid in node_categories["A"])
    entry["count_A"] = count_a
    entry["count_total"] = count_total
    if finished_flag is not None:
        entry["finished_first_phase"] = bool(finished_flag)

def get_current_network_size():
    return len(node_categories["A"])

def save_progress(output_dir):
    os.makedirs(output_dir, exist_ok=True)
    progress_file = os.path.join(output_dir, 'progress_equal.json')
    # ä¿å­˜å‰åˆ·æ–°æ¯ä¸ªtopicè®¡æ•°
    for entry in topic_plan:
        update_topic_counts(entry["topic"])
    progress_data = {
        "users": users_data,
        "edges": edges_data,
        "processed_phase2": list(processed_users),  # ç¬¬äºŒé˜¶æ®µå·²å¤„ç†A
        "categories": {k: list(v) for k, v in node_categories.items()},
        "popularity": popularity_data,
        "save_timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "total_users": len(users_data),
        "total_edges": len(edges_data),
        "topics_processed": topics_processed,
        "topic_plan": topic_plan,
        "topic_nodes_map": {k: list(v) for k, v in topic_nodes_map.items()},
        "run_config": run_config
    }
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜ -> æ–‡ä»¶: {progress_file}")
    print(f"   å½“å‰æ€»ç”¨æˆ·: {len(users_data)}ï¼Œæ€»è¾¹æ•°: {len(edges_data)}ï¼Œæ—¶é—´: {progress_data['save_timestamp']}")

def load_progress(output_dir):
    global processed_users, users_data, edges_data, edges_set, node_categories, popularity_data
    global topics_processed, topic_plan, topic_nodes_map, run_config
    progress_file = os.path.join(output_dir, 'progress_equal.json')
    if not os.path.exists(progress_file):
        return False
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        processed_users = set(data.get("processed_phase2", []))
        users_data = data.get("users", {})
        edges_data = data.get("edges", [])
        popularity_data = data.get("popularity", {})
        edges_set = set(tuple(edge) if isinstance(edge, list) else edge for edge in edges_data)
        if "categories" in data:
            for k, v in data["categories"].items():
                if k in node_categories:
                    node_categories[k] = set(v)
        topics_processed = data.get("topics_processed", [])
        topic_plan = data.get("topic_plan", [])
        loaded_map = data.get("topic_nodes_map", {})
        topic_nodes_map = {k: set(v) for k, v in loaded_map.items()}
        run_config = data.get("run_config", run_config)
        print(f"ğŸ“¥ å·²åŠ è½½è¿›åº¦: ç”¨æˆ· {len(users_data)} ä¸ªï¼Œè¾¹ {len(edges_data)} æ¡ï¼Œç¬¬äºŒé˜¶æ®µå·²å¤„ç† {len(processed_users)} ä¸ªAç±»")
        if topic_plan:
            print(f"   å…³é”®è¯è®¡åˆ’: {len(topic_plan)} ä¸ªï¼ˆå«ç›®æ ‡ä¸å®ŒæˆçŠ¶æ€ï¼‰")
        return True
    except Exception as e:
        print(f"âŒ åŠ è½½è¿›åº¦å¤±è´¥: {e}")
        return False

def save_final_data(output_dir, topic_label):
    os.makedirs(output_dir, exist_ok=True)

    users_df = pd.DataFrame.from_dict(users_data, orient='index')
    users_df.index.name = 'user_id'
    users_df.reset_index(inplace=True)
    column_order = ['user_id', 'screen_name', 'fans_count', 'category']
    users_df = users_df.reindex(columns=column_order)
    users_df.to_csv(f'{output_dir}/users.csv', index=False, encoding='utf-8-sig')

    edges_df = pd.DataFrame(edges_data, columns=['source', 'target'])
    edges_df.to_csv(f'{output_dir}/edges.csv', index=False, encoding='utf-8-sig')

    popularity_df = pd.DataFrame.from_dict(popularity_data, orient='index', columns=['avg_popularity'])
    popularity_df.index.name = 'user_id'
    popularity_df.reset_index(inplace=True)
    popularity_df.to_csv(f'{output_dir}/popularity.csv', index=False, encoding='utf-8-sig')

    info_path = f'{output_dir}/network_info.json'
    with open(info_path, 'w', encoding='utf-8') as f:
        info = {
            "topic": topic_label,
            "æ¨¡å¼": "equal_A_only",
            "Aç±»èŠ‚ç‚¹æ•°": len(node_categories["A"]),
            "èŠ‚ç‚¹æ•°": len(users_df),
            "è¾¹æ•°": len(edges_df),
            "å…³é”®è¯åˆ—è¡¨": topics_processed,
            "é«˜å½±å“åŠ›é˜ˆå€¼": run_config.get("high_fans_threshold", 0),
            "çˆ¬å–æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        json.dump(info, f, ensure_ascii=False, indent=2)
    print("ğŸ§¾ æœ€ç»ˆæ–‡ä»¶å·²è¾“å‡ºï¼šusers.csv, edges.csv, popularity.csv, network_info.json")
    print(f"   æ€»ç”¨æˆ·: {len(users_df)}ï¼Œæ€»è¾¹æ•°: {len(edges_df)}ï¼ŒAç±»: {len(node_categories['A'])}")

def prompt_keyword_targets(max_items=20):
    """äº¤äº’å¼è¾“å…¥æœ€å¤š20ä¸ªå…³é”®è¯å’Œç›®æ ‡äººæ•°"""
    items = []
    print("\nè¯·è¾“å…¥æœ€å¤š20ä¸ªå…³é”®è¯åŠç›®æ ‡äººæ•°ï¼ˆå›è½¦è·³è¿‡ç»“æŸï¼‰ï¼š")
    for idx in range(1, max_items + 1):
        topic = input(f"- å…³é”®è¯{idx}: ").strip()
        if not topic:
            break
        while True:
            t = input(f"  ç›®æ ‡äººæ•°ï¼ˆæ•´æ•°ï¼Œä¾‹å¦‚ 5000ï¼‰: ").strip()
            if not t:
                print("  ç›®æ ‡äººæ•°ä¸èƒ½ä¸ºç©º")
                continue
            try:
                target = int(t)
                if target <= 0:
                    print("  è¯·è¾“å…¥æ­£æ•´æ•°")
                    continue
                break
            except:
                print("  è¯·è¾“å…¥æœ‰æ•ˆçš„æ•´æ•°")
        items.append((topic, target))
    return items

def initialize_topic_plan_from_items(items):
    """æ ¹æ®ç”¨æˆ·è¾“å…¥åˆå§‹åŒ– topic_plan ä¸ topic_nodes_map"""
    global topic_plan, topic_nodes_map, topics_processed
    topic_plan = []
    topics_processed = []
    topic_nodes_map = {}
    for topic, target in items:
        topic_plan.append({
            "topic": topic,
            "target": target,
            "finished_first_phase": False,
            "count_A": 0,
            "count_total": 0
        })
        ensure_topic_bucket(topic)

def update_or_create_user(user_id, fans_count, screen_name, category='A'):
    """åˆ›å»º/æ›´æ–°ç”¨æˆ·å­—å…¸"""
    if user_id not in users_data:
        users_data[user_id] = {
            'screen_name': screen_name if screen_name else f'ç”¨æˆ·{user_id}',
            'fans_count': int(fans_count) if fans_count is not None else 0,
            'category': category
        }
    else:
        # ä¿ç•™å·²æœ‰æ˜µç§°ï¼Œå¦‚æœ‰æ–°çš„ç²‰ä¸æ•°åˆ™æ›´æ–°
        if fans_count is not None:
            users_data[user_id]['fans_count'] = int(fans_count)
        if 'category' not in users_data[user_id]:
            users_data[user_id]['category'] = category

def run_first_phase_for_topic_equal(topic, target_size, high_threshold):
    """ç¬¬ä¸€é˜¶æ®µï¼šä»…æ”¶é›†Aç±»ï¼ˆå‘è¿‡è¯¥tagçš„äººï¼‰ï¼Œå¹¶æŒ‰é˜ˆå€¼è¿‡æ»¤é«˜ç²‰ä¸A"""
    global should_exit

    ensure_topic_bucket(topic)
    topic_bucket = topic_nodes_map[topic]

    print("\n" + "="*80)
    print(f"å¼€å§‹ç¬¬ä¸€é˜¶æ®µï¼ˆä»…Aï¼‰ï¼š#{topic}#ï¼Œç›®æ ‡äººæ•°: {target_size}ï¼Œé«˜ç²‰ä¸é˜ˆå€¼: {high_threshold}ï¼ˆ0è¡¨ç¤ºä¸è®¾ï¼‰")
    print("="*80)

    try:
        topic_encoded = requests.utils.quote(f"#{topic}#")
        page = 1
        seen_users = set()
        reach_target = False
        a_added_for_topic = 0
        consecutive_empty_pages = 0
        max_consecutive_empty = 3

        while not reach_target and not should_exit:
            print(f"  è¯·æ±‚ç¬¬{page}é¡µ...")
            if page == 1:
                page_url = f"https://s.weibo.com/weibo/{topic_encoded}"
            else:
                page_url = f"https://s.weibo.com/weibo/{topic_encoded}&page={page}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.8',
                'Referer': 'https://weibo.com/',
                'Connection': 'keep-alive'
            }
            cookies = {}
            try:
                with open(COOKIE_PATH, 'r', encoding='utf-8') as f:
                    cookies_list = json.load(f)
                for cookie in cookies_list:
                    cookies[cookie['name']] = cookie['value']
            except:
                pass

            try:
                time.sleep(random.uniform(0.7, 1.4))
                response = requests.get(page_url, headers=headers, cookies=cookies, timeout=15)
                if response.status_code != 200:
                    print(f"  çŠ¶æ€ç  {response.status_code}ï¼Œåœæ­¢è¯¥é¡µ")
                    break

                user_ids = crawler.extract_users_from_page(response.text, seen_users)
                if not user_ids:
                    consecutive_empty_pages += 1
                    print(f"  æœ¬é¡µæœªæå–åˆ°æ–°ç”¨æˆ·ï¼Œè¿ç»­ç©ºé¡µ: {consecutive_empty_pages}")
                    if consecutive_empty_pages >= max_consecutive_empty:
                        print("  è¿ç»­ç©ºé¡µè¾¾åˆ°é˜ˆå€¼ï¼Œç»“æŸè¯¥å…³é”®è¯çš„ç¬¬ä¸€é˜¶æ®µæ‰«æ")
                        break
                else:
                    consecutive_empty_pages = 0

                for user_id in user_ids:
                    if should_exit or reach_target:
                        break

                    # å¦‚å·²åœ¨Aï¼Œç›´æ¥è®¡å…¥è¯¥å…³é”®è¯æ¡¶ï¼ˆä¸é‡å¤æŸ¥é˜ˆå€¼ï¼‰
                    if user_id in node_categories["A"]:
                        if user_id not in topic_bucket:
                            topic_bucket.add(user_id)
                            a_added_for_topic += 1
                            update_topic_counts(topic)
                            print(f"  [A+] å…³é”®è¯æ¡¶æ–°å¢å·²æœ‰A {user_id} | è¯¥å…³é”®è¯ç´¯è®¡ {len(topic_bucket)}/{target_size} | å…¨å±€A {len(node_categories['A'])}")
                        if len(topic_bucket) >= target_size:
                            reach_target = True
                            break
                        continue

                    # è·å–Açš„ç²‰ä¸æ•°å¹¶è¿›è¡Œé˜ˆå€¼è¿‡æ»¤ï¼ˆ0=ä¸è®¾é˜ˆå€¼ï¼‰
                    fans_count = crawler.check_user_fans_count(user_id)
                    if high_threshold and fans_count > high_threshold:
                        print(f"  è·³è¿‡Aç±»ç”¨æˆ· {user_id}ï¼ˆç²‰ä¸ {fans_count} > é˜ˆå€¼ {high_threshold}ï¼‰")
                        continue

                    # è®°å½•Aï¼ˆå…¨å±€ï¼‰
                    node_categories["A"].add(user_id)
                    update_or_create_user(user_id, fans_count, screen_name=f'ç”¨æˆ·{user_id}', category='A')
                    topic_bucket.add(user_id)
                    a_added_for_topic += 1

                    # è¾“å‡ºAä¿¡æ¯
                    print(f"  [A {a_added_for_topic}] æ”¶å½•Aç±»ç”¨æˆ· {user_id} | ç²‰ä¸æ•° {fans_count} | å…³é”®è¯ç´¯è®¡ {len(topic_bucket)}/{target_size} | å…¨å±€A {len(node_categories['A'])}")

                    # å¯é€‰ï¼šè®¡ç®—è¿‘10æ¡å½±å“åŠ›ï¼ˆä¸fetch4ä¸€è‡´ï¼‰
                    if user_id not in popularity_data:
                        popularity_data[user_id] = crawler.calculate_user_popularity(user_id)

                    # è¾¾æ ‡åˆ¤å®š
                    if len(topic_bucket) >= target_size:
                        reach_target = True
                        break

                    # å®šæœŸä¿å­˜
                    if (a_added_for_topic % 10) == 0:
                        save_progress(output_dir)

                    time.sleep(random.uniform(0.5, 1.5))

                seen_users.update(user_ids)
                page += 1

            except Exception as e:
                print(f"  ç¬¬ä¸€é˜¶æ®µè¯·æ±‚å¼‚å¸¸: {e}")
                break

        # æ ‡è®°å®ŒæˆçŠ¶æ€
        update_topic_counts(topic, finished_flag=True if reach_target else False)
        entry = get_topic_entry(topic)
        print(f"å…³é”®è¯ #{topic}# ç¬¬ä¸€é˜¶æ®µå®ŒæˆçŠ¶æ€: {'å·²å®Œæˆ' if entry.get('finished_first_phase') else 'æœªå®Œæˆ'} | "
              f"è¯¥å…³é”®è¯ç´¯è®¡: {entry.get('count_total', 0)} | å…¨å±€A: {len(node_categories['A'])}")

        # ä¿å­˜ä¸€æ¬¡
        save_progress(output_dir)

    except KeyboardInterrupt:
        should_exit = True
        save_progress(output_dir)
        print("âœ… å·²ä¿å­˜è¿›åº¦ï¼ˆç”¨æˆ·ä¸­æ–­ï¼‰")
    except Exception as e:
        print(f"ç¨‹åºå¼‚å¸¸: {e}")
        save_progress(output_dir)

def run_second_phase_global_equal():
    """ç¬¬äºŒé˜¶æ®µï¼šä»…ä¸ºAç±»ä¹‹é—´è¡¥è¾¹ï¼ˆAâ†’Aï¼‰"""
    global should_exit

    print("\n=== ç¬¬äºŒé˜¶æ®µï¼šè¡¥å…¨Aç±»ä¹‹é—´çš„è¾¹ï¼ˆAâ†’Aï¼‰ ===")
    a_users_to_process = [u for u in node_categories["A"] if u not in processed_users]
    total_a = len(a_users_to_process)
    print(f"éœ€è¦å¤„ç†çš„Aç±»ç”¨æˆ·: {total_a} ä¸ª")

    for i, user_id in enumerate(a_users_to_process, start=1):
        if should_exit:
            break

        print(f"[A {i}/{total_a}] å¤„ç†Aç±»ç”¨æˆ· {user_id} çš„ç²‰ä¸åˆ—è¡¨ï¼Œè¡¥Aâ†’Aè¾¹...")
        fans_users = crawler.crawl_user_fans_cn(user_id)
        new_edges = 0

        for fan in fans_users:
            fan_id = str(fan.get('id'))
            if fan_id in node_categories["A"]:
                edge = (user_id, fan_id)
                if edge not in edges_set:
                    edges_data.append(edge)
                    edges_set.add(edge)
                    new_edges += 1

        processed_users.add(user_id)
        print(f"  -> æœ¬Aæ–°å¢Aâ†’Aè¾¹ {new_edges} æ¡ | å½“å‰å…¨å±€ï¼šè¾¹ {len(edges_data)}")

        if (i % 10) == 0:
            save_progress(output_dir)
        time.sleep(random.uniform(0.5, 1.0))

def main():
    global crawler, should_exit, output_dir, topic_plan, topic_nodes_map, topics_processed, run_config

    signal.signal(signal.SIGINT, signal_handler)

    print("å¾®åšè¯é¢˜ç½‘ç»œçˆ¬å–å™¨ï¼ˆEqualæ¨¡å¼ï¼šä»…Aç±»ï¼ŒäºŒé˜¶æ®µè¡¥Aâ†’Aè¾¹ï¼‰")
    print("- æ”¯æŒæœ€å¤š20ä¸ªå…³é”®è¯ï¼Œæ¯ä¸ªå…³é”®è¯è®¾å®šç‹¬ç«‹ç›®æ ‡äººæ•°ï¼ˆä»…ç”¨äºè¯¥å…³é”®è¯è®¡æ•°ï¼‰")
    print("- ç¬¬äºŒé˜¶æ®µï¼šåªä¸ºAç±»ä¹‹é—´è¡¥è¾¹ï¼ˆAâ†’Aï¼‰")
    print("æŒ‰Ctrl+Cå¯éšæ—¶å®‰å…¨ä¸­æ–­")

    crawler = TopicNetworkCrawler()
    if not crawler.setup_drivers():
        return
    if not crawler.load_cookies_cn():
        crawler.cleanup()
        return

    try:
        # Equalæ¨¡å¼è¾“å‡ºç›®å½•
        output_dir = f'{BASE_OUTPUT_DIR}/topic_equal'
        os.makedirs(output_dir, exist_ok=True)
        has_prev = load_progress(output_dir)

        # è¾“å…¥å…³é”®è¯ä¸ç›®æ ‡
        items = []
        if has_prev and topic_plan:
            print("\næ£€æµ‹åˆ°å·²æœ‰çš„å…³é”®è¯è¿›åº¦ï¼š")
            for idx, entry in enumerate(topic_plan, 1):
                print(f"  {idx}. #{entry.get('topic')}# | target={entry.get('target')} | "
                      f"å®Œæˆ: {entry.get('finished_first_phase')} | è®¡æ•°: A={entry.get('count_A')}, æ€»={entry.get('count_total')}")
            choice = input("\næ˜¯å¦åœ¨ä¸Šè¿°è¿›åº¦ä¸Šç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
            if choice == 'y':
                items = [(e["topic"], e["target"]) for e in topic_plan]
            else:
                items = prompt_keyword_targets(max_items=20)
                if not items:
                    print("æœªè¾“å…¥ä»»ä½•å…³é”®è¯ï¼Œç¨‹åºé€€å‡ºã€‚")
                    return
                initialize_topic_plan_from_items(items)
        else:
            items = prompt_keyword_targets(max_items=20)
            if not items:
                print("æœªè¾“å…¥ä»»ä½•å…³é”®è¯ï¼Œç¨‹åºé€€å‡ºã€‚")
                return
            initialize_topic_plan_from_items(items)

        # è®¾ç½®é«˜å½±å“åŠ›é˜ˆå€¼ï¼ˆ0è¡¨ç¤ºä¸è®¾ï¼‰
        if not has_prev or ("high_fans_threshold" not in run_config):
            while True:
                t = input("è¯·è¾“å…¥é«˜å½±å“åŠ›é˜ˆå€¼ï¼ˆç²‰ä¸æ•°ï¼Œæ•´æ•°ï¼›0è¡¨ç¤ºä¸è®¾ç½®ï¼‰: ").strip()
                try:
                    v = int(t)
                    if v < 0:
                        print("è¯·è¾“å…¥â‰¥0çš„æ•´æ•°")
                        continue
                    run_config["high_fans_threshold"] = v
                    break
                except:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•´æ•°")
        print(f"é«˜å½±å“åŠ›é˜ˆå€¼: {run_config['high_fans_threshold']}ï¼ˆ0è¡¨ç¤ºä¸è®¾ï¼‰")

        save_progress(output_dir)

        start_time = datetime.now()

        # ç¬¬ä¸€é˜¶æ®µï¼šä»…å¤„ç†æœªå®Œæˆçš„å…³é”®è¯
        unfinished = [e for e in topic_plan if not e.get("finished_first_phase", False)]
        if unfinished:
            print(f"\nå…±æœ‰ {len(unfinished)} ä¸ªå…³é”®è¯æœªå®Œæˆç¬¬ä¸€é˜¶æ®µï¼Œå°†ç»§ç»­å¤„ç†ï¼š")
            for e in unfinished:
                print(f"  - #{e['topic']}# (target={e['target']}, å½“å‰æ€»={e.get('count_total',0)})")
            for entry in unfinished:
                if should_exit:
                    break
                topic = entry["topic"]
                target = entry["target"]
                print(f"\n[ç»­è·‘] å¤„ç†å…³é”®è¯: {topic}ï¼Œç›®æ ‡äººæ•°: {target}")
                run_first_phase_for_topic_equal(topic, target, run_config["high_fans_threshold"])
                if topic not in topics_processed:
                    topics_processed.append(topic)
                save_progress(output_dir)
        else:
            print("\næ‰€æœ‰å…³é”®è¯çš„ç¬¬ä¸€é˜¶æ®µå‡å·²å®Œæˆï¼Œå°†ç›´æ¥è¿›å…¥ç¬¬äºŒé˜¶æ®µã€‚")

        if should_exit:
            print("\nâš ï¸ ä¸­æ–­äºç¬¬ä¸€é˜¶æ®µï¼Œå·²ä¿å­˜è¿›åº¦ã€‚")
        else:
            # ç¬¬äºŒé˜¶æ®µï¼ˆç»Ÿä¸€è¡¥Aâ†’Aè¾¹ï¼‰
            run_second_phase_global_equal()
            # ä¿å­˜æœ€ç»ˆ
            label = "equal_" + "_".join([e["topic"] for e in topic_plan]) if topic_plan else "equal"
            save_final_data(output_dir, topic_label=label)
            print("\n=== æ‰€æœ‰å…³é”®è¯å¤„ç†å®Œæˆï¼Œå·²ç»Ÿä¸€è¡¥è¾¹å¹¶ä¿å­˜ç½‘ç»œ ===")
            print(f"æœ€ç»ˆç»Ÿè®¡: A={len(node_categories['A'])}, æ€»ç”¨æˆ·={get_current_network_size()}, æ€»è¾¹={len(edges_data)}")
            print(f"è¾“å‡ºç›®å½•: {output_dir}")

        end_time = datetime.now()
        print(f"\næ€»è€—æ—¶: {end_time - start_time}")

    except KeyboardInterrupt:
        should_exit = True
        if output_dir:
            save_progress(output_dir)
        print("âœ… å·²ä¿å­˜å½“å‰è¿›åº¦ï¼ˆç”¨æˆ·ä¸­æ–­ï¼‰")
    finally:
        if crawler:
            crawler.cleanup()

if __name__ == "__main__":
    main()