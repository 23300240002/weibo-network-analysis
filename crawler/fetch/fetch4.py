import os
import json
import time
import random
import requests
import pandas as pd
import signal
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from datetime import datetime
import re

# é…ç½®å‚æ•°
BASE_OUTPUT_DIR = 'C:/Tengfei/data/data/topic_networks'
COOKIE_PATH = 'C:/Tengfei/data/crawler/crawler_for_weibo_fans-master/cookie.json'
TARGET_TOPIC = "å­™é¢–è"  # é»˜è®¤å€¼ï¼Œä»…ç”¨äºæ‰“å°ï¼›å®é™…è¿è¡Œç”±ç”¨æˆ·è¾“å…¥æ›¿ä»£
MAX_NETWORK_SIZE = 5000  # é»˜è®¤å€¼ï¼Œä»…ç”¨äºæ‰“å°ï¼›å®é™…è¿è¡Œç”±ç”¨æˆ·è¾“å…¥æ›¿ä»£
CELEBRITY_FANS_THRESHOLD = 2000  # ä¿ç•™ä½†ä¸å†ç”¨äºA/Bè·³è¿‡é€»è¾‘

# æ–°å¢é˜ˆå€¼ï¼ˆæŒ‰è¦æ±‚ï¼‰
A_FANS_THRESHOLD_SKIP = 2000   # Aç±»ç²‰ä¸æ•°>2000 è·³è¿‡
B_FANS_THRESHOLD_SKIP = 1500   # Bç±»ç²‰ä¸æ•°>1500 è·³è¿‡æ‰©è¾¹ï¼ˆç¬¬ä¸€é˜¶æ®µç”¨äºè¿‡æ»¤æ‹Ÿå…¥Bï¼›ç¬¬äºŒé˜¶æ®µä¸å†åˆ¤æ–­ï¼‰

MAX_PAGES_PER_USER = 20

# é€Ÿåº¦å‚æ•°
SLEEP_MIN = 0.4
SLEEP_MAX = 0.6
BATCH_INTERVAL_MIN = 0.5
BATCH_INTERVAL_MAX = 1.0

# æµè¡Œåº¦è®¡ç®—å‚æ•°
MAX_POSTS_FOR_POPULARITY = 10

# å…¨å±€å˜é‡ï¼ˆåˆå¹¶å¤§ç½‘ç»œï¼‰
node_categories = {"A": set(), "B": set()}
edges_data = []
edges_set = set()
users_data = {}
popularity_data = {}
processed_users = set()

# è¿è¡Œæ€
crawler = None
output_dir = None            # åˆå¹¶å¤§ç½‘ç»œç›®å½•
should_exit = False
topics_processed = []        # è®°å½•å·²å¤„ç†å…³é”®è¯ï¼ˆç”¨äºinfoï¼‰
topic_nodes_map = {}         # æ¯ä¸ªå…³é”®è¯çš„ç”¨æˆ·é›†åˆï¼ˆç”¨äºäººæ•°åˆ¤æ–­ï¼Œç°å·²æŒä¹…åŒ–ï¼‰
topic_plan = []              # æ–°å¢ï¼šä¿å­˜æ¯ä¸ªå…³é”®è¯çš„è®¡åˆ’ä¸è¿›åº¦ [{topic, target, finished_first_phase, count_A, count_B, count_total}]

def signal_handler(signum, frame):
    """å¤„ç†Ctrl+Cä¿¡å·"""
    global should_exit
    print("\nâš ï¸ æ•è·åˆ°ä¸­æ–­ä¿¡å·ï¼Œå‡†å¤‡å®‰å…¨é€€å‡º...")
    should_exit = True
    if output_dir:
        save_progress(output_dir)
    if crawler:
        crawler.cleanup()
    print("âœ… è¿›åº¦å·²ä¿å­˜ï¼Œèµ„æºå·²é‡Šæ”¾ã€‚")

class TopicNetworkCrawler:
    def __init__(self):
        self.driver_com = None
        self.driver_cn = None
        
    def setup_drivers(self):
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        
        try:
            self.driver_com = webdriver.Chrome(options=chrome_options)
            self.driver_com.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            chrome_options_cn = Options()
            chrome_options_cn.add_argument('--no-sandbox')
            chrome_options_cn.add_argument('--disable-dev-shm-usage')
            chrome_options_cn.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options_cn.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options_cn.add_argument('--user-data-dir=C:/temp/chrome_profile_cn')
            
            self.driver_cn = webdriver.Chrome(options=chrome_options_cn)
            self.driver_cn.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            return True
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def load_cookies_cn(self):
        try:
            with open(COOKIE_PATH, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            
            self.driver_cn.get('https://weibo.cn')
            time.sleep(2)
            
            for cookie in cookies:
                try:
                    self.driver_cn.add_cookie(cookie)
                except:
                    pass
            
            self.driver_cn.refresh()
            time.sleep(2)
            return True
        except Exception as e:
            print(f"âŒ CookieåŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_topic_users(self, topic, max_users=200):
        """åŠ¨æ€ç¿»é¡µç›´åˆ°è·å¾—è¶³å¤Ÿç”¨æˆ·æˆ–æ— æ³•ç»§ç»­ç¿»é¡µï¼ˆä¿ç•™æ–¹æ³•ï¼Œä¾›éœ€è¦æ—¶ä½¿ç”¨ï¼‰"""
        global should_exit
        print(f"è·å–è¯é¢˜ #{topic}# çš„ç”¨æˆ·ï¼Œç›®æ ‡æ•°é‡: {max_users}")
        
        topic_encoded = requests.utils.quote(f"#{topic}#")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://weibo.com/',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        cookies = {}
        try:
            with open(COOKIE_PATH, 'r', encoding='utf-8') as f:
                cookies_list = json.load(f)
            for cookie in cookies_list:
                cookies[cookie['name']] = cookie['value']
        except:
            pass
        
        all_user_ids = []
        seen_users = set()
        consecutive_empty_pages = 0
        max_consecutive_empty = 3
        
        page = 1
        while len(all_user_ids) < max_users:
            if should_exit:
                print("æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæå‰é€€å‡º get_topic_users")
                break
            print(f"  æ­£åœ¨è¯·æ±‚ç¬¬{page}é¡µ...")
            
            if page == 1:
                page_url = f"https://s.weibo.com/weibo/{topic_encoded}"
            else:
                page_url = f"https://s.weibo.com/weibo/{topic_encoded}&page={page}"
            
            try:
                time.sleep(random.uniform(2, 4))
                response = requests.get(page_url, headers=headers, cookies=cookies, timeout=15)
                
                if response.status_code == 200:
                    page_user_ids = self.extract_users_from_page(response.text, seen_users)
                    if page_user_ids:
                        consecutive_empty_pages = 0
                        all_user_ids.extend(page_user_ids)
                        seen_users.update(page_user_ids)
                        print(f"    ç¬¬{page}é¡µæ–°å¢ {len(page_user_ids)} ä¸ªï¼Œç´¯è®¡ {len(all_user_ids)}")
                    else:
                        consecutive_empty_pages += 1
                        print(f"    ç¬¬{page}é¡µæ— æ–°ç”¨æˆ·ï¼Œè¿ç»­ç©ºé¡µ: {consecutive_empty_pages}")
                        if consecutive_empty_pages >= max_consecutive_empty:
                            print(f"    è¿ç»­{max_consecutive_empty}é¡µæ— æ–°ç”¨æˆ·ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                else:
                    print(f"    çŠ¶æ€ç : {response.status_code}ï¼Œç»§ç»­é‡è¯•")
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= max_consecutive_empty:
                        break
                        
            except Exception as e:
                print(f"    è¯·æ±‚å¼‚å¸¸: {e}")
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_consecutive_empty:
                    break
            
            page += 1
            if page > 200:
                print("    è¾¾åˆ°æœ€å¤§é¡µæ•°200ï¼Œåœæ­¢ç¿»é¡µ")
                break
        
        print(f"  å…±è·å– {len(all_user_ids)} ä¸ªç”¨æˆ·")
        return all_user_ids
    
    def get_users_by_browser_scroll_unlimited(self, topic, max_users, existing_users):
        """æµè§ˆå™¨æ»šåŠ¨è¡¥å……ç”¨æˆ·"""
        try:
            topic_encoded = requests.utils.quote(f"#{topic}#")
            search_url = f"https://s.weibo.com/weibo/{topic_encoded}"
            self.driver_com.get(search_url)
            time.sleep(3)
            
            all_user_ids = []
            consecutive_no_new = 0
            max_consecutive_no_new = 5
            
            scroll_round = 0
            while len(all_user_ids) < max_users and consecutive_no_new < max_consecutive_no_new:
                scroll_round += 1
                print(f"    æµè§ˆå™¨æ»šåŠ¨ç¬¬{scroll_round}è½®...")
                
                current_page_source = self.driver_com.page_source
                current_users = self.extract_users_from_page(current_page_source, existing_users)
                
                new_users = [uid for uid in current_users if uid not in existing_users and uid not in all_user_ids]
                
                if new_users:
                    all_user_ids.extend(new_users)
                    existing_users.update(new_users)
                    consecutive_no_new = 0
                    print(f"      æœ¬è½®æ–°å¢ {len(new_users)} ä¸ªï¼Œç´¯è®¡ {len(all_user_ids)}")
                else:
                    consecutive_no_new += 1
                    print(f"      æœ¬è½®æ— æ–°å¢ï¼Œè¿ç»­æ— æ–°å¢è½®æ•°: {consecutive_no_new}")
                
                self.driver_com.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(3, 5))
                
                try:
                    more_buttons = self.driver_com.find_elements(By.XPATH, 
                        "//a[contains(text(), 'æ›´å¤š') or contains(text(), 'more') or contains(@class, 'more')]")
                    if more_buttons:
                        more_buttons[0].click()
                        time.sleep(3)
                except:
                    pass
                
                if scroll_round >= 50:
                    print("      è¾¾åˆ°æœ€å¤§æ»šåŠ¨è½®æ•°50ï¼Œåœæ­¢æ»šåŠ¨")
                    break
            
            print(f"  æµè§ˆå™¨æ»šåŠ¨æ–°å¢ {len(all_user_ids)} ä¸ªç”¨æˆ·")
            return all_user_ids
            
        except Exception as e:
            print(f"  æµè§ˆå™¨æ»šåŠ¨æ¨¡å¼å¤±è´¥: {e}")
            return []
    
    def extract_users_from_page(self, html_content, seen_users):
        """ä»HTMLé¡µé¢æå–ç”¨æˆ·ID"""
        user_ids = []
        
        patterns = [
            r'href="//weibo\.com/(\d+)/[^"]*"',
            r'href="https?://weibo\.com/(\d+)/[^"]*"',
            r'href="//weibo\.com/u/(\d+)[^"]*"',
            r'href="https?://weibo\.com/u/(\d+)[^"]*"',
            r'"idstr":"(\d+)"',
            r'"user":\s*{\s*"id":(\d+)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content)
            for match in matches:
                if len(match) >= 8 and match.isdigit() and match not in seen_users:
                    user_ids.append(match)
        
        # å»é‡
        unique_users = []
        page_seen = set()
        for uid in user_ids:
            if uid not in page_seen:
                unique_users.append(uid)
                page_seen.add(uid)
        
        return unique_users
    
    def get_users_by_browser_scroll(self, topic, max_users):
        """ï¼ˆä¿ç•™ï¼‰æµè§ˆå™¨æ»šåŠ¨è·å–æ›´å¤šç”¨æˆ·"""
        try:
            topic_encoded = requests.utils.quote(f"#{topic}#")
            search_url = f"https://s.weibo.com/weibo/{topic_encoded}"
            
            self.driver_com.get(search_url)
            time.sleep(3)
            
            all_user_ids = []
            seen_users = set()
            
            for scroll_round in range(8):
                current_page_source = self.driver_com.page_source
                current_users = self.extract_users_from_page(current_page_source, seen_users)
                
                for uid in current_users:
                    if uid not in seen_users and len(all_user_ids) < max_users:
                        all_user_ids.append(uid)
                        seen_users.add(uid)
                
                self.driver_com.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(3, 5))
                
                try:
                    more_buttons = self.driver_com.find_elements(By.XPATH, 
                        "//a[contains(text(), 'æ›´å¤š') or contains(text(), 'more') or contains(@class, 'more')]")
                    if more_buttons:
                        more_buttons[0].click()
                        time.sleep(3)
                except:
                    pass
                
                if len(current_users) == 0 and scroll_round > 2:
                    break
            
            return all_user_ids
        except:
            return []
    
    def check_user_fans_count(self, user_id):
        """æ£€æŸ¥ç”¨æˆ·ç²‰ä¸æ•° - åŸºäºweibo.comé¡µé¢"""
        try:
            profile_url = f'https://weibo.com/u/{user_id}'
            self.driver_com.get(profile_url)
            time.sleep(random.uniform(2, 4))
            
            page_source = self.driver_com.page_source
            
            patterns = [
                r'<span[^>]*>([0-9]+\.?[0-9]*[ä¸‡]?)</span>\s*ç²‰ä¸',
                r'>([0-9]+\.?[0-9]*[ä¸‡]?)</span>\s*ç²‰ä¸',
                r'([0-9]+\.?[0-9]*[ä¸‡]?)\s*ç²‰ä¸',
                r'ç²‰ä¸[^>]*>([0-9]+\.?[0-9]*[ä¸‡]?)',
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, page_source)
                if matches:
                    fans_str = matches[0].strip()
                    if 'ä¸‡' in fans_str:
                        num_str = fans_str.replace('ä¸‡', '')
                        fans_count = int(float(num_str) * 10000)
                    else:
                        fans_count = int(float(fans_str))
                    return fans_count
        except:
            pass
        return 0
    
    def crawl_user_fans_cn(self, user_id):
        """çˆ¬å–ç”¨æˆ·ç²‰ä¸ï¼ˆweibo.cnï¼‰ï¼Œå¹¶ç›´æ¥è§£ææ¯ä¸ªç²‰ä¸çš„ç²‰ä¸æ•°ï¼ˆç²‰ä¸Xäºº / ç²‰ä¸Yä¸‡äººï¼‰"""
        import re
        try:
            def parse_cn_fans_count(text: str):
                # ä»…æ¥å—â€œç²‰ä¸<æ•°å­—>(ä¸‡)?äººâ€ä¸¤ç§æ ¼å¼ï¼Œå…¶ä»–ä¸€å¾‹è§†ä¸ºæ— æ•ˆï¼ˆè¿”å›Noneï¼‰
                # ä¾‹ï¼šç²‰ä¸1äºº / ç²‰ä¸2.3ä¸‡äºº
                m = re.search(r'ç²‰ä¸\s*([0-9]+(?:\.[0-9]+)?)\s*(ä¸‡)?\s*äºº', text)
                if not m:
                    return None
                num = float(m.group(1))
                if m.group(2):  # æœ‰â€œä¸‡â€
                    return int(num * 10000)
                return int(num)

            fans_url = f'https://weibo.cn/{user_id}/fans'
            self.driver_cn.get(fans_url)
            time.sleep(0.5)

            page_source = self.driver_cn.page_source
            if 'ç”¨æˆ·ä¸å­˜åœ¨' in page_source or 'ç™»å½•' in page_source:
                return []

            fans_data = []
            consecutive_empty_pages = 0

            for page in range(1, MAX_PAGES_PER_USER + 1):
                if page > 1:
                    try:
                        next_page_url = f'https://weibo.cn/{user_id}/fans?page={page}'
                        self.driver_cn.get(next_page_url)
                        time.sleep(random.uniform(0.5, 1.0))
                    except:
                        break

                try:
                    # ä»…è§£ææœ‰æ˜µç§°æ–‡æœ¬çš„<a href="/u/...">ï¼Œå…¶çˆ¶çº§tdåŒ…å«â€œç²‰ä¸Xäºº/ä¸‡äººâ€
                    fan_elements = self.driver_cn.find_elements(By.XPATH, "//a[contains(@href, '/u/')]")

                    page_fans = []
                    processed_ids = set()

                    for element in fan_elements:
                        try:
                            fan_href = element.get_attribute('href')
                            fan_name = element.text.strip()
                            if not fan_href or '/u/' not in fan_href or not fan_name:
                                # è·³è¿‡å¤´åƒé“¾æ¥ç­‰æ— æ–‡æœ¬çš„<a>
                                continue

                            fan_id = fan_href.split('/u/')[-1].split('?')[0].split('/')[0]
                            if not (fan_id.isdigit() and fan_id not in processed_ids):
                                continue

                            # å–è¯¥é“¾æ¥æ‰€åœ¨çš„å³ä¾§tdæ–‡æœ¬ï¼Œè§£æâ€œç²‰ä¸Xäºº/ä¸‡äººâ€
                            try:
                                td_text = element.find_element(By.XPATH, "./ancestor::td[1]").text
                            except:
                                td_text = ""

                            fans_count_cn = parse_cn_fans_count(td_text)

                            page_fans.append({
                                'id': fan_id,
                                'screen_name': fan_name,
                                'fans_count_cn': fans_count_cn  # å¯èƒ½ä¸ºNoneï¼ˆæ ¼å¼ä¸ç¬¦æ—¶ï¼‰
                            })
                            processed_ids.add(fan_id)

                        except:
                            continue

                    if len(page_fans) == 0:
                        consecutive_empty_pages += 1
                        if consecutive_empty_pages >= 2:
                            break
                    else:
                        consecutive_empty_pages = 0
                        fans_data.extend(page_fans)

                except:
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= 2:
                        break

                time.sleep(random.uniform(0.5, 1.0))

            return fans_data
        except:
            return []
    
    def calculate_user_popularity(self, user_id, max_posts=MAX_POSTS_FOR_POPULARITY):
        """è®¡ç®—ç”¨æˆ·è¿‘10æ¡å¾®åšçš„å¹³å‡è½¬èµè¯„ï¼ˆä¿ç•™ï¼›ä½ å½“å‰ä¸»ç”¨fetch3_helperçš„æ€»ä½“å½±å“åŠ›ï¼‰"""
        try:
            profile_url = f'https://weibo.cn/u/{user_id}'
            self.driver_cn.get(profile_url)
            time.sleep(2)
            
            weibo_divs = self.driver_cn.find_elements(By.XPATH, "//div[@class='c' and contains(@id, 'M_')]")
            if not weibo_divs:
                return 0.0
            
            posts_data = []
            for i, weibo_div in enumerate(weibo_divs):
                if len(posts_data) >= max_posts:
                    break
                post_data = self.process_single_weibo_div(weibo_div)
                if post_data and post_data['content'] != "å†…å®¹æœªæå–":
                    posts_data.append(post_data)
            
            if not posts_data:
                return 0.0
            
            total_interactions = 0
            valid_posts = len(posts_data)
            for post in posts_data:
                interactions = post['interactions']
                post_total = interactions['likes'] + interactions['reposts'] + interactions['comments']
                total_interactions += post_total
            
            avg_popularity = total_interactions / valid_posts if valid_posts > 0 else 0.0
            return avg_popularity
        except:
            return 0.0
    
    def process_single_weibo_div(self, weibo_div_element):
        """å¤„ç†å•ä¸ªå¾®åšdiv"""
        try:
            div_html = weibo_div_element.get_attribute('outerHTML')
            child_divs = weibo_div_element.find_elements(By.XPATH, "./div")
            
            content = "å†…å®¹æœªæå–"
            try:
                ctt_element = weibo_div_element.find_element(By.CLASS_NAME, "ctt")
                content = ctt_element.text.strip()
                content = content[:100] + ('...' if len(content) > 100 else '')
            except:
                pass
            
            interactions = {'reposts': 0, 'likes': 0, 'comments': 0}
            if len(child_divs) > 0:
                last_div = child_divs[-1]
                last_div_html = last_div.get_attribute('outerHTML')
                interactions = self.extract_interactions_from_html(last_div_html)
            else:
                interactions = self.extract_interactions_from_html(div_html)
            
            return {
                'content': content,
                'interactions': interactions,
                'total_interactions': sum(interactions.values())
            }
        except:
            return None
    
    def extract_interactions_from_html(self, html_text):
        """æå–è½¬èµè¯„æ•°æ®"""
        interactions = {'reposts': 0, 'likes': 0, 'comments': 0}
        patterns = {
            'likes': [r'èµ\[(\d+)\]'],
            'reposts': [r'è½¬å‘\[(\d+)\]'],
            'comments': [r'è¯„è®º\[(\d+)\]']
        }
        for interaction_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, html_text)
                if matches:
                    try:
                        num = int(matches[-1])
                        interactions[interaction_type] = num
                    except:
                        continue
        return interactions
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.driver_com:
                self.driver_com.quit()
                self.driver_com = None
        except Exception as e:
            print(f"å…³é—­weibo.comæµè§ˆå™¨æ—¶å‡ºé”™: {e}")
        try:
            if self.driver_cn:
                self.driver_cn.quit()
                self.driver_cn = None
        except Exception as e:
            print(f"å…³é—­weibo.cnæµè§ˆå™¨æ—¶å‡ºé”™: {e}")

def get_current_network_size():
    return len(node_categories["A"]) + len(node_categories["B"])

def get_topic_entry(topic):
    """è·å–æˆ–åˆ›å»ºtopic_planä¸­çš„æ¡ç›®"""
    global topic_plan
    for entry in topic_plan:
        if entry.get("topic") == topic:
            return entry
    # æœªæ‰¾åˆ°åˆ™åˆ›å»ºä¸€ä¸ªé»˜è®¤æ¡ç›®ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
    entry = {
        "topic": topic,
        "target": 0,
        "finished_first_phase": False,
        "count_A": 0,
        "count_B": 0,
        "count_total": 0
    }
    topic_plan.append(entry)
    return entry

def ensure_topic_bucket(topic):
    """ç¡®ä¿æ¯ä¸ªå…³é”®è¯æœ‰è‡ªå·±çš„è®¡æ•°é›†åˆï¼ˆç”¨äºåˆ¤æ–­ï¼Œç°æŒä¹…åŒ–ï¼‰"""
    if topic not in topic_nodes_map:
        topic_nodes_map[topic] = set()

def update_topic_counts(topic, finished_flag=None):
    """æ›´æ–°æŸä¸ªå…³é”®è¯çš„A/B/æ€»è®¡æ•°ï¼Œå¹¶å¯é€‰æ›´æ–°å®Œæˆæ ‡è®°"""
    ensure_topic_bucket(topic)
    bucket = topic_nodes_map.get(topic, set())
    entry = get_topic_entry(topic)
    count_total = len(bucket)
    # è®¡ç®—A/Bè®¡æ•°ï¼ˆæŒ‰å½“å‰å…¨å±€åˆ†ç±»äº¤é›†ï¼‰
    count_a = sum(1 for uid in bucket if uid in node_categories["A"])
    count_b = sum(1 for uid in bucket if uid in node_categories["B"])
    entry["count_A"] = count_a
    entry["count_B"] = count_b
    entry["count_total"] = count_total
    if finished_flag is not None:
        entry["finished_first_phase"] = bool(finished_flag)

def save_progress(output_dir):
    os.makedirs(output_dir, exist_ok=True)
    progress_file = os.path.join(output_dir, 'progress.json')
    # ä¿å­˜å‰å…ˆåˆ·æ–°æ¯ä¸ªtopicçš„è®¡æ•°
    for entry in topic_plan:
        update_topic_counts(entry["topic"])
    progress_data = {
        "users": users_data,
        "edges": edges_data,
        "processed": list(processed_users),
        "categories": {k: list(v) for k, v in node_categories.items()},
        "popularity": popularity_data,
        "save_timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "total_users": len(users_data),
        "total_edges": len(edges_data),
        "topics_processed": topics_processed,
        # æ–°å¢ï¼šå…³é”®è¯è®¡åˆ’ä¸è¿›åº¦
        "topic_plan": topic_plan,
        # æ–°å¢ï¼šæ¯ä¸ªå…³é”®è¯çš„å·²è®¡å…¥ç”¨æˆ·é›†åˆï¼ˆç”¨äºç²¾å‡†ç»­è·‘ï¼‰
        "topic_nodes_map": {k: list(v) for k, v in topic_nodes_map.items()}
    }
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜ -> æ–‡ä»¶: {progress_file}")
    print(f"   å½“å‰æ€»ç”¨æˆ·: {len(users_data)}ï¼Œæ€»è¾¹æ•°: {len(edges_data)}ï¼Œæ—¶é—´: {progress_data['save_timestamp']}")

def load_progress(output_dir):
    global processed_users, users_data, edges_data, edges_set, node_categories, popularity_data, topics_processed, topic_plan, topic_nodes_map
    progress_file = os.path.join(output_dir, 'progress.json')
    if not os.path.exists(progress_file):
        return False
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        processed_users = set(data.get("processed", []))
        users_data = data.get("users", {})
        edges_data = data.get("edges", [])
        popularity_data = data.get("popularity", {})
        edges_set = set(tuple(edge) if isinstance(edge, list) else edge for edge in edges_data)
        if "categories" in data:
            for k, v in data["categories"].items():
                if k in node_categories:
                    node_categories[k] = set(v)
        topics_processed = data.get("topics_processed", [])
        # æ–°å¢ï¼šåŠ è½½å…³é”®è¯è®¡åˆ’ä¸è®¡æ•°
        topic_plan = data.get("topic_plan", [])
        # æ–°å¢ï¼šåŠ è½½æ¯ä¸ªå…³é”®è¯çš„bucketé›†åˆ
        loaded_map = data.get("topic_nodes_map", {})
        topic_nodes_map = {k: set(v) for k, v in loaded_map.items()}
        print(f"ğŸ“¥ å·²åŠ è½½è¿›åº¦: ç”¨æˆ· {len(users_data)} ä¸ªï¼Œè¾¹ {len(edges_data)} æ¡ï¼Œå·²å¤„ç†ç”¨æˆ· {len(processed_users)} ä¸ª")
        if topic_plan:
            print(f"   å…³é”®è¯è®¡åˆ’: {len(topic_plan)} ä¸ªï¼ˆå«ç›®æ ‡ä¸å®ŒæˆçŠ¶æ€ï¼‰")
        return True
    except Exception as e:
        print(f"âŒ åŠ è½½è¿›åº¦å¤±è´¥: {e}")
        return False

def save_final_data(output_dir, topic_label):
    os.makedirs(output_dir, exist_ok=True)
    
    users_df = pd.DataFrame.from_dict(users_data, orient='index')
    users_df.index.name = 'user_id'
    users_df.reset_index(inplace=True)
    column_order = ['user_id', 'screen_name', 'fans_count', 'category']
    users_df = users_df.reindex(columns=column_order)
    users_df.to_csv(f'{output_dir}/users.csv', index=False, encoding='utf-8-sig')
    
    edges_df = pd.DataFrame(edges_data, columns=['source', 'target'])
    edges_df.to_csv(f'{output_dir}/edges.csv', index=False, encoding='utf-8-sig')
    
    popularity_df = pd.DataFrame.from_dict(popularity_data, orient='index', columns=['avg_popularity'])
    popularity_df.index.name = 'user_id'
    popularity_df.reset_index(inplace=True)
    popularity_df.to_csv(f'{output_dir}/popularity.csv', index=False, encoding='utf-8-sig')
    
    with open(f'{output_dir}/network_info.json', 'w', encoding='utf-8') as f:
        info = {
            "topic": topic_label,
            "èŠ‚ç‚¹æ•°": len(users_df),
            "è¾¹æ•°": len(edges_df),
            "Aç±»èŠ‚ç‚¹æ•°": len(node_categories["A"]),
            "Bç±»èŠ‚ç‚¹æ•°": len(node_categories["B"]),
            "å…³é”®è¯åˆ—è¡¨": topics_processed,
            "çˆ¬å–æ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        json.dump(info, f, ensure_ascii=False, indent=2)
    print("ğŸ§¾ æœ€ç»ˆæ–‡ä»¶å·²è¾“å‡ºï¼šusers.csv, edges.csv, popularity.csv, network_info.json")
    print(f"   æ€»ç”¨æˆ·: {len(users_df)}ï¼Œæ€»è¾¹æ•°: {len(edges_df)}ï¼ŒAç±»: {len(node_categories['A'])}ï¼ŒBç±»: {len(node_categories['B'])}")

def prompt_keyword_targets(max_items=5):
    """äº¤äº’å¼è¾“å…¥æœ€å¤š5ä¸ªå…³é”®è¯å’Œç›®æ ‡äººæ•°"""
    items = []
    print("\nè¯·è¾“å…¥æœ€å¤š5ä¸ªå…³é”®è¯åŠç›®æ ‡äººæ•°ï¼ˆå›è½¦è·³è¿‡ç»“æŸï¼‰ï¼š")
    for idx in range(1, max_items + 1):
        topic = input(f"- å…³é”®è¯{idx}: ").strip()
        if not topic:
            break
        while True:
            t = input(f"  ç›®æ ‡äººæ•°ï¼ˆæ•´æ•°ï¼Œä¾‹å¦‚ 5000ï¼‰: ").strip()
            if not t:
                print("  ç›®æ ‡äººæ•°ä¸èƒ½ä¸ºç©º")
                continue
            try:
                target = int(t)
                if target <= 0:
                    print("  è¯·è¾“å…¥æ­£æ•´æ•°")
                    continue
                break
            except:
                print("  è¯·è¾“å…¥æœ‰æ•ˆçš„æ•´æ•°")
        items.append((topic, target))
    return items

def initialize_topic_plan_from_items(items):
    """æ ¹æ®ç”¨æˆ·è¾“å…¥åˆå§‹åŒ– topic_plan ä¸ topic_nodes_map"""
    global topic_plan, topic_nodes_map, topics_processed
    topic_plan = []
    topics_processed = []
    topic_nodes_map = {}
    for topic, target in items:
        topic_plan.append({
            "topic": topic,
            "target": target,
            "finished_first_phase": False,
            "count_A": 0,
            "count_B": 0,
            "count_total": 0
        })
        ensure_topic_bucket(topic)

def run_first_phase_for_topic(topic, target_size):
    """ç¬¬ä¸€é˜¶æ®µï¼ˆä»…Aâ†’Bï¼‰ï¼šä¸¥æ ¼è¿‡æ»¤
       - Aç²‰ä¸æ•°>1500è·³è¿‡ï¼ˆweibo.comï¼‰
       - æ‹Ÿå…¥Bï¼šç”¨ weibo.cn è§£æåˆ°çš„ fans_count_cn è¿‡æ»¤ï¼Œ>1000 æˆ–è§£æä¸åˆ°åˆ™è·³è¿‡ï¼Œä¸å»ºè¾¹ä¸è®¡æ•°
       - å·²åœ¨Açš„ç²‰ä¸ï¼šå…è®¸åŠ è¾¹ï¼ˆä¸å—Bé˜ˆå€¼é™åˆ¶ï¼‰
    """
    global should_exit

    ensure_topic_bucket(topic)
    topic_bucket = topic_nodes_map[topic]

    print("\n" + "="*80)
    print(f"å¼€å§‹ç¬¬ä¸€é˜¶æ®µï¼ˆAâ†’Bï¼‰ï¼š#{topic}#ï¼Œç›®æ ‡äººæ•°: {target_size}ï¼ˆä»…ç”¨äºè¯¥å…³é”®è¯è®¡æ•°ï¼‰")
    print("="*80)

    try:
        topic_encoded = requests.utils.quote(f"#{topic}#")
        page = 1
        seen_users = set()
        reach_target = False
        a_processed_for_topic = 0
        no_more_users = False  # æ ‡è®°æ˜¯å¦å› æ— æ–°ç”¨æˆ·è€Œç»“æŸ

        while not reach_target and not should_exit:
            print(f"  è¯·æ±‚ç¬¬{page}é¡µ...")
            if page == 1:
                page_url = f"https://s.weibo.com/weibo/{topic_encoded}"
            else:
                page_url = f"https://s.weibo.com/weibo/{topic_encoded}&page={page}"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.8',
                'Referer': 'https://weibo.com/',
                'Connection': 'keep-alive'
            }
            cookies = {}
            try:
                with open(COOKIE_PATH, 'r', encoding='utf-8') as f:
                    cookies_list = json.load(f)
                for cookie in cookies_list:
                    cookies[cookie['name']] = cookie['value']
            except:
                pass

            try:
                time.sleep(random.uniform(2, 4))
                response = requests.get(page_url, headers=headers, cookies=cookies, timeout=15)
                if response.status_code != 200:
                    print(f"  çŠ¶æ€ç  {response.status_code}ï¼Œåœæ­¢è¯¥é¡µ")
                    break

                user_ids = crawler.extract_users_from_page(response.text, seen_users)
                if not user_ids:
                    print("  æœ¬é¡µæœªæå–åˆ°æ–°ç”¨æˆ·ï¼Œç»“æŸè¯¥å…³é”®è¯çš„ç¬¬ä¸€é˜¶æ®µ")
                    no_more_users = True
                    break

                for user_id in user_ids:
                    if should_exit or reach_target:
                        break
                    if user_id in processed_users:
                        continue

                    # å…ˆè·å–Açš„ç²‰ä¸æ•°ï¼Œè¶…è¿‡1500ç›´æ¥è·³è¿‡
                    fans_count = crawler.check_user_fans_count(user_id)
                    if fans_count is None:
                        fans_count = 0
                    if fans_count > A_FANS_THRESHOLD_SKIP:
                        print(f"  è·³è¿‡Aç±»ç”¨æˆ· {user_id}ï¼ˆç²‰ä¸ {fans_count} > {A_FANS_THRESHOLD_SKIP}ï¼‰")
                        continue

                    # è®°å½•Aï¼ˆå…¨å±€ï¼‰
                    node_categories["A"].add(user_id)
                    users_data[user_id] = {
                        'screen_name': f'ç”¨æˆ·{user_id}',
                        'fans_count': fans_count,
                        'category': 'A'
                    }
                    processed_users.add(user_id)
                    a_processed_for_topic += 1
                    print(f"  [A {a_processed_for_topic}] å¤„ç†Aç±»ç”¨æˆ· {user_id} | Aç²‰ä¸æ•° {fans_count}ï¼ˆè¯¥å…³é”®è¯ç´¯è®¡ {len(topic_bucket)} / {target_size} | å…¨å±€ {get_current_network_size()}ï¼‰")

                    # è®°å½•è¿‘10æ¡å½±å“åŠ›ï¼ˆä¿ç•™ï¼‰
                    if user_id not in popularity_data:
                        popularity_data[user_id] = crawler.calculate_user_popularity(user_id)

                    # å°†Aè®¡å…¥è¯¥å…³é”®è¯æ¡¶
                    topic_bucket.add(user_id)

                    # çˆ¬Açš„ç²‰ä¸ï¼ˆweibo.cnï¼‰ï¼Œå¹¶ç”¨ fans_count_cn åšBé˜ˆå€¼è¿‡æ»¤
                    fans_users = crawler.crawl_user_fans_cn(user_id)
                    new_b = 0
                    new_edges = 0
                    skipped_high = 0
                    skipped_format = 0

                    for fan in fans_users:
                        fan_id = str(fan.get('id'))
                        fan_screen_name = fan.get('screen_name', '')
                        cn_count = fan.get('fans_count_cn', None)

                        # è‹¥ç²‰ä¸å·²æ˜¯Aï¼šå…è®¸æ·»åŠ è¾¹ï¼ˆä¸é€‚ç”¨Bé˜ˆå€¼ï¼‰ï¼Œå¹¶è®¡å…¥æ¡¶
                        if fan_id in node_categories["A"]:
                            edge = (user_id, fan_id)
                            if edge not in edges_set:
                                edges_data.append(edge)
                                edges_set.add(edge)
                                new_edges += 1
                            topic_bucket.add(fan_id)
                            continue

                        # å·²åœ¨Bï¼šç›´æ¥è¡¥è¾¹ï¼Œä¸é‡å¤é˜ˆå€¼åˆ¤æ–­
                        if fan_id in node_categories["B"]:
                            edge = (user_id, fan_id)
                            if edge not in edges_set:
                                edges_data.append(edge)
                                edges_set.add(edge)
                                new_edges += 1
                            topic_bucket.add(fan_id)
                            continue

                        # æ‹Ÿå…¥Bï¼šå¿…é¡»æœ‰å¯è§£æçš„ç²‰ä¸æ•°ï¼Œä¸” <=1000ï¼›å¦åˆ™è·³è¿‡
                        if cn_count is None:
                            skipped_format += 1
                            continue
                        if cn_count > B_FANS_THRESHOLD_SKIP:
                            skipped_high += 1
                            print(f"    -> è·³è¿‡ç²‰ä¸ {fan_id}ï¼ˆç²‰ä¸ {cn_count} > {B_FANS_THRESHOLD_SKIP}ï¼‰")
                            continue

                        # åˆ°è¿™é‡Œï¼šåŠ å…¥ç½‘ç»œï¼Œå†™å…¥è¾¹ä¸Bç±»
                        edge = (user_id, fan_id)
                        if edge not in edges_set:
                            edges_data.append(edge)
                            edges_set.add(edge)
                            new_edges += 1

                        node_categories["B"].add(fan_id)
                        if fan_id not in users_data:
                            users_data[fan_id] = {
                                'screen_name': fan_screen_name,
                                'fans_count': int(cn_count),
                                'category': 'B'
                            }
                        else:
                            users_data[fan_id]['fans_count'] = int(cn_count)

                        topic_bucket.add(fan_id)
                        new_b += 1

                    print(f"    -> æœ¬Aæ–°å¢Bç±» {new_b} ä¸ªï¼Œè·³è¿‡è¶…æ ‡ {skipped_high} ä¸ªï¼Œè·³è¿‡æ ¼å¼ä¸æ˜ {skipped_format} ä¸ªï¼Œæ–°å¢è¾¹ {new_edges} æ¡ | å½“å‰å…¨å±€ï¼šç”¨æˆ· {len(users_data)}ï¼Œè¾¹ {len(edges_data)}")

                    # æ›´æ–°å¹¶ä¿å­˜è¯¥å…³é”®è¯è®¡æ•°ï¼ˆä¸æ¯æ¬¡éƒ½è½ç›˜ï¼Œåªæ›´æ–°å†…å­˜ï¼‰
                    update_topic_counts(topic)

                    # è¾¾æ ‡åˆ¤å®š
                    if len(topic_bucket) >= target_size:
                        reach_target = True
                        update_topic_counts(topic, finished_flag=True)
                        print(f"  âœ… å…³é”®è¯ #{topic}# å·²è¾¾ç›®æ ‡äººæ•° {target_size}ï¼ˆè¯¥å…³é”®è¯ç´¯è®¡ï¼‰ï¼Œç»“æŸè¯¥å…³é”®è¯çš„ç¬¬ä¸€é˜¶æ®µ")
                        break

                    if len(processed_users) % 10 == 0:
                        save_progress(output_dir)

                    time.sleep(random.uniform(1.0, 2.0))

                seen_users.update(user_ids)
                page += 1

            except Exception as e:
                print(f"  ç¬¬ä¸€é˜¶æ®µè¯·æ±‚å¼‚å¸¸: {e}")
                break

        # è‹¥å› æ— æ–°ç”¨æˆ·ç»“æŸï¼Œä¹Ÿæ ‡è®°å®Œæˆï¼ˆé¿å…é‡å¤æ‰«æï¼‰
        if not reach_target and no_more_users:
            update_topic_counts(topic, finished_flag=True)
        else:
            update_topic_counts(topic)

        entry = get_topic_entry(topic)
        print(f"å…³é”®è¯ #{topic}# ç¬¬ä¸€é˜¶æ®µå®ŒæˆçŠ¶æ€: {'å·²å®Œæˆ' if entry.get('finished_first_phase') else 'æœªå®Œæˆ'} | "
              f"è¯¥å…³é”®è¯ç´¯è®¡: {entry.get('count_total', 0)} | å…¨å±€è§„æ¨¡: {get_current_network_size()}ï¼ˆA: {len(node_categories['A'])}, B: {len(node_categories['B'])}ï¼‰")

    except KeyboardInterrupt:
        should_exit = True
        save_progress(output_dir)
        print("âœ… å·²ä¿å­˜è¿›åº¦ï¼ˆç”¨æˆ·ä¸­æ–­ï¼‰")
    except Exception as e:
        print(f"ç¨‹åºå¼‚å¸¸: {e}")
        save_progress(output_dir)

def run_second_phase_global():
    """å…¨å±€ç¬¬äºŒé˜¶æ®µï¼šå®Œå–„Bç±»ç”¨æˆ·çš„è¾¹ï¼ˆåªåœ¨A/Bä¹‹é—´åŠ è¾¹ï¼‰"""
    global should_exit

    print("\n=== å…¨å±€ç¬¬äºŒé˜¶æ®µï¼šå®Œå–„Bç±»ç”¨æˆ·çš„è¾¹ï¼ˆæ— éœ€å†åˆ¤æ–­é˜ˆå€¼ï¼‰ ===")
    b_users_to_process = [u for u in node_categories["B"] if u not in processed_users]
    total_b = len(b_users_to_process)
    print(f"éœ€è¦å¤„ç†çš„Bç±»ç”¨æˆ·: {total_b} ä¸ª")

    for i, user_id in enumerate(b_users_to_process, start=1):
        if should_exit:
            break

        print(f"[B {i}/{total_b}] æ­£åœ¨å¤„ç†Bç±»ç”¨æˆ· {user_id} ...")

        # æ ‡è®°å¤„ç†å¹¶å¯é€‰åˆ·æ–°ç²‰ä¸æ•°ï¼ˆä¸ç”¨äºé˜ˆå€¼åˆ¤æ–­ï¼‰
        b_fans_cnt = crawler.check_user_fans_count(user_id)
        if user_id not in users_data:
            users_data[user_id] = {
                'screen_name': f'ç”¨æˆ·{user_id}',
                'fans_count': b_fans_cnt,
                'category': 'B'
            }
        else:
            users_data[user_id]['fans_count'] = b_fans_cnt

        processed_users.add(user_id)
        if user_id not in popularity_data:
            popularity_data[user_id] = crawler.calculate_user_popularity(user_id)

        fans_users = crawler.crawl_user_fans_cn(user_id)
        valid_edge_count = 0
        for fan in fans_users:
            fan_id = str(fan.get('id'))
            if fan_id in node_categories["A"] or fan_id in node_categories["B"]:
                edge = (user_id, fan_id)
                if edge not in edges_set:
                    edges_data.append(edge)
                    edges_set.add(edge)
                    valid_edge_count += 1

        print(f"  -> æœ¬Bæ–°å¢æœ‰æ•ˆè¾¹ {valid_edge_count} æ¡ | å½“å‰å…¨å±€ï¼šç”¨æˆ· {len(users_data)}ï¼Œè¾¹ {len(edges_data)}")

        if (i % 10) == 0:
            save_progress(output_dir)
        time.sleep(random.uniform(0.5, 1.0))

def main():
    global crawler, should_exit, output_dir, topic_plan, topic_nodes_map, topics_processed

    signal.signal(signal.SIGINT, signal_handler)

    print("å¾®åšè¯é¢˜ç½‘ç»œçˆ¬å–å™¨ï¼ˆåˆå¹¶å¤šå…³é”®è¯ä¸ºä¸€ä¸ªå¤§ç½‘ç»œï¼‰")
    print("- æ”¯æŒæœ€å¤š5ä¸ªå…³é”®è¯ï¼Œæ¯ä¸ªå…³é”®è¯è®¾å®šç‹¬ç«‹ç›®æ ‡äººæ•°ï¼ˆä»…ç”¨äºè¯¥å…³é”®è¯è®¡æ•°ï¼‰")
    print(f"- Aç±»è·³è¿‡é˜ˆå€¼: ç²‰ä¸æ•° > {A_FANS_THRESHOLD_SKIP}")
    print(f"- Bç±»è·³è¿‡é˜ˆå€¼: ç²‰ä¸æ•° > {B_FANS_THRESHOLD_SKIP}ï¼ˆä»…ç¬¬ä¸€é˜¶æ®µè¿‡æ»¤æ‹Ÿå…¥Bï¼›ç¬¬äºŒé˜¶æ®µä¸å†åˆ¤æ–­ï¼‰")
    print("æŒ‰Ctrl+Cå¯éšæ—¶å®‰å…¨ä¸­æ–­")

    crawler = TopicNetworkCrawler()
    if not crawler.setup_drivers():
        return
    if not crawler.load_cookies_cn():
        crawler.cleanup()
        return

    try:
        # åˆå¹¶ç½‘ç»œè¾“å‡ºç›®å½•ï¼ˆç»Ÿä¸€ä¿å­˜/ç»­è·‘ï¼‰
        output_dir = f'{BASE_OUTPUT_DIR}/topic_combined'
        os.makedirs(output_dir, exist_ok=True)
        has_prev = load_progress(output_dir)

        # è‹¥å­˜åœ¨æ—§è¿›åº¦ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
        items = []
        if has_prev and topic_plan:
            print("\næ£€æµ‹åˆ°å·²æœ‰çš„å…³é”®è¯è¿›åº¦ï¼š")
            for idx, entry in enumerate(topic_plan, 1):
                print(f"  {idx}. #{entry.get('topic')}# | target={entry.get('target')} | "
                      f"å®Œæˆ: {entry.get('finished_first_phase')} | è®¡æ•°: A={entry.get('count_A')}, B={entry.get('count_B')}, æ€»={entry.get('count_total')}")
            choice = input("\næ˜¯å¦åœ¨ä¸Šè¿°è¿›åº¦ä¸Šç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
            if choice == 'y':
                # ä½¿ç”¨å·²ä¿å­˜çš„topic_planï¼ˆä¸å†è¯¢é—®ï¼‰
                items = [(e["topic"], e["target"]) for e in topic_plan]
            else:
                # ä¸ç»§ç»­ï¼Œåˆ™é‡æ–°è¾“å…¥å…³é”®è¯ä¸ç›®æ ‡ï¼Œå¹¶é‡ç½®ä¸è¦†ç›–topicç›¸å…³çš„æŒä¹…æ•°æ®
                items = prompt_keyword_targets(max_items=5)
                if not items:
                    print("æœªè¾“å…¥ä»»ä½•å…³é”®è¯ï¼Œç¨‹åºé€€å‡ºã€‚")
                    return
                # åˆå§‹åŒ–æ–°çš„topic_planä¸bucketé›†åˆï¼ˆä¿ç•™å·²æœ‰ç½‘ç»œæ•°æ®ä¸æ¸…ç©ºï¼Œä»¥é˜²ç”¨æˆ·å¸Œæœ›è¿½åŠ ï¼‰
                initialize_topic_plan_from_items(items)
                save_progress(output_dir)
        else:
            # æ— è¿›åº¦åˆ™æ­£å¸¸è¾“å…¥
            items = prompt_keyword_targets(max_items=5)
            if not items:
                print("æœªè¾“å…¥ä»»ä½•å…³é”®è¯ï¼Œç¨‹åºé€€å‡ºã€‚")
                return
            initialize_topic_plan_from_items(items)
            save_progress(output_dir)

        start_time = datetime.now()

        # ç¬¬ä¸€é˜¶æ®µï¼šä»…å¤„ç†æœªå®Œæˆçš„å…³é”®è¯
        unfinished = [e for e in topic_plan if not e.get("finished_first_phase", False)]
        if unfinished:
            print(f"\nå…±æœ‰ {len(unfinished)} ä¸ªå…³é”®è¯æœªå®Œæˆç¬¬ä¸€é˜¶æ®µï¼Œå°†ç»§ç»­å¤„ç†ï¼š")
            for e in unfinished:
                print(f"  - #{e['topic']}# (target={e['target']}, å½“å‰æ€»={e.get('count_total',0)})")
            for entry in unfinished:
                if should_exit:
                    break
                topic = entry["topic"]
                target = entry["target"]
                print(f"\n[ç»­è·‘] å¤„ç†å…³é”®è¯: {topic}ï¼Œç›®æ ‡äººæ•°: {target}")
                run_first_phase_for_topic(topic, target)
                if topic not in topics_processed:
                    topics_processed.append(topic)
                save_progress(output_dir)
        else:
            print("\næ‰€æœ‰å…³é”®è¯çš„ç¬¬ä¸€é˜¶æ®µå‡å·²å®Œæˆï¼Œå°†ç›´æ¥è¿›å…¥ç¬¬äºŒé˜¶æ®µã€‚")

        if should_exit:
            print("\nâš ï¸ ä¸­æ–­äºç¬¬ä¸€é˜¶æ®µï¼Œå·²ä¿å­˜åˆå¹¶è¿›åº¦ã€‚")
        else:
            # å…¨å±€ç¬¬äºŒé˜¶æ®µï¼ˆç»Ÿä¸€è¡¥è¾¹ï¼‰
            run_second_phase_global()
            save_final_data(output_dir, topic_label="combined_" + "_".join([e["topic"] for e in topic_plan]))
            print("\n=== å…¨éƒ¨å…³é”®è¯å¤„ç†å®Œæˆï¼Œå·²ç»Ÿä¸€è¡¥è¾¹å¹¶ä¿å­˜åˆå¹¶ç½‘ç»œ ===")
            print(f"æœ€ç»ˆç»Ÿè®¡: A={len(node_categories['A'])}, B={len(node_categories['B'])}, æ€»ç”¨æˆ·={get_current_network_size()}, æ€»è¾¹={len(edges_data)}")
            print(f"åˆå¹¶ç½‘ç»œç›®å½•: {output_dir}")

        end_time = datetime.now()
        print(f"\næ€»è€—æ—¶: {end_time - start_time}")

    except KeyboardInterrupt:
        should_exit = True
        if output_dir:
            save_progress(output_dir)
        print("âœ… å·²ä¿å­˜å½“å‰è¿›åº¦ï¼ˆç”¨æˆ·ä¸­æ–­ï¼‰")
    finally:
        if crawler:
            crawler.cleanup()

if __name__ == "__main__":
    main()